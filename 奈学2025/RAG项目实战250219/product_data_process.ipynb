{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3511f38a-da5e-4734-8bcd-2fa8c89515ca",
   "metadata": {},
   "source": [
    "# <center>基于问答数据的特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84296644-4ec2-4b12-b2e5-8cbda5053762",
   "metadata": {},
   "source": [
    "- **Step 1. 确定推荐系统的需求**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626d106-c607-4906-9355-2ede833afb0e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;开发一个推荐系统的功能链路，通过分析用户的历史对话记录，智能推荐最适合用户需求的课程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b583d1-2182-4930-b313-3781890b6301",
   "metadata": {},
   "source": [
    "- **Step 2. 数据说明**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333aa4e-8376-4590-8e66-b91a53e5b12f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们公司的课程分为四大类别，每类课程都通过PDF课件提供详细的教学内容，其中涵盖的技术领域：\n",
    "\n",
    "- 机器学习：涵盖从基础理论到高级应用的各个方面。\n",
    "- 深度学习：介绍深度学习的基础架构、算法及其在各行业中的应用。\n",
    "- 大数据架构：通过大数据平台架构设计从而解决复杂大数据场景。\n",
    "- 大模型工程落地学习计划：基于大模型进行应用开发。\n",
    "\n",
    "&emsp;&emsp;每个PDF课件包含多个一级和二级标题，这些标题明确区分不同的知识点，便于学习和复习：\n",
    "\n",
    "- 一级标题：代表主要的知识模块或课程大纲的核心部分。\n",
    "- 二级标题：在每个主要知识模块下细分更具体的话题或概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f426c95-ee47-4ec0-9de5-19c46c6668f2",
   "metadata": {},
   "source": [
    "- **Step 3. 数据处理**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e389ee-6b9a-4889-9313-08018a63bfea",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当大模型介入到数据处理流程的时候，我们需要考虑的就如RAG的Indexing过程一样：如何找到一种最合适的文本读取方式，更好的区分各个模块、内容，以保证收集到的数据都是相对完整且独立的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6821cd3-1fc1-4bac-9af2-3dfcc9dc5117",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里我们推荐使用的是：Markdown 格式。 Markdown是一种轻量级标记语言，允许用户使用简单的语法格式化纯文本。它广泛用于创建结构化文档，特别是在 GitHub、Jupyter Notebook 和各种内容管理系统等平台上。当将数据输入 LLM 或 RAG 系统时，使用 Markdown 格式有几个好处："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69299c78-8c6e-47f2-92a7-4efaf3aecf84",
   "metadata": {},
   "source": [
    "1. 结构化内容：Markdown 会将内容组织为标题、列表、表格和其他结构化元素。这种结构有助于更好地理解和上下文保存。\n",
    "2. 富文本：Markdown 支持基本格式，例如粗体、斜体、链接和代码块。在输入数据中包含富文本可以增强语言模型的上下文。\n",
    "3. Markdown 允许嵌入超链接、脚注和参考。在构建元数据过程中，对于引用外部资源或提供额外的上下文至关重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda7fbd-98e8-484b-9d9f-81e88a01e179",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里，我们给大家推荐一个PDF --> Markdown的转换工具：PyMuPDF。官方：https://github.com/pymupdf/RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec437aa-57ab-44c7-be66-d90743fba03a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;PyMuPDF能够从 PDF 页面中提取文本、图像、矢量图形，以及自 2023 年 8 月起还能够提取表格。每种对象类型都有自己的提取方法：一种用于文本，另一种用于表格、图像和矢量图形，同时合并了这些不同的提取，以生成一个通用的、统一的 Markdown 字符串，该字符串一致地表示整个页面的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a4fe31-981f-4a5e-9210-dde0f9a3c21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf4llm\n",
      "  Downloading pymupdf4llm-0.0.14-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pymupdf>=1.24.3 (from pymupdf4llm)\n",
      "  Downloading PyMuPDF-1.24.10-cp38-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.10 (from pymupdf>=1.24.3->pymupdf4llm)\n",
      "  Using cached PyMuPDFb-1.24.10-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
      "Downloading pymupdf4llm-0.0.14-py3-none-any.whl (24 kB)\n",
      "Downloading PyMuPDF-1.24.10-cp38-none-manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m118.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyMuPDFb-1.24.10-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
      "Installing collected packages: PyMuPDFb, pymupdf, pymupdf4llm\n",
      "Successfully installed PyMuPDFb-1.24.10 pymupdf-1.24.10 pymupdf4llm-0.0.14\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    " ! pip install -U pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6520eaac-215c-4f35-b2a9-625a15eb146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/Ch 1 开源大模型本地部署硬件指南.pdf...\n",
      "[                                        ] (0/2[=                                       ] ( 1/2=[===                                     ] ( 2/2=[=====                                   ] ( 3/2=[=======                                 ] ( 4/2=[=========                               ] ( 5/2[==========                              ] ( 6/22=[============                            ] ( 7/22=[==============                          ] ( 8/22=[================                        ] ( 9/22=[==================                      ] (10/22[====================                    ] (11/22=[=====================                   ] (12/2=[=======================                 ] (13/2=[=========================               ] (14/2=[===========================             ] (15/2=[=============================           ] (16/2[==============================          ] (17/22=[================================        ] (18/22=[==================================      ] (19/22=[====================================    ] (20/22=[======================================  ] (21/22[========================================] (22/22]\n"
     ]
    }
   ],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "doc = pymupdf4llm.to_markdown(\"data/Ch 1 开源大模型本地部署硬件指南.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8311022-98f7-401f-ad31-0bba10d04b82",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 本地部署开源大模型\\n\\n## Ch.1 如何选择合适的硬件配置\\n\\n为了在本地有效部署和使用开源大模型，深入理解硬件与软件的需求至关重要。在硬件需求方面，关键\\n\\n是配置一台或多台高性能的个人计算机系统或租用配备了先进GPU的在线服务器，确保有足够的内存和存储\\n\\n空间来处理大数据和复杂模型。至于软件需求，推荐使用Ubuntu操作系统，因其在机器学习领域的支持和\\n\\n兼容性优于Windows。编程语言建议以Python为主，结合TensorFlow或PyTorch等流行机器学习框架，并\\n\\n利用DeepSpeed等优化工具来提升大模型的运行效率和性能。\\n\\n所以在本系列课程中，我们将从硬件选择入手，逐步引导大家理解并掌握如何为大模型部署选择合适的\\n\\n硬件，以及如何高效地配置和运行这些模型，从零到一实现大模型的本地部署和应用。首先来看硬件方面，\\n\\n提前规划计算资源是必要的。目前，我们主要考虑以下两种途径：\\n\\n1. 配置个人计算机或服务器，组建一个适合大模型使用需求的计算机系统。\\n\\n2. 租用在线GPU服务，通过云计算平台获取大模型所需的计算能力。\\n\\n# 一、大模型应用需求分析\\n\\n大模型的本地部署主要应用于三个方面：训练（train）、高效微调（fine-tune）和推理\\n\\n**（inference）。这些过程在算力消耗上有显著差异：**\\n\\n**训练：算力最密集，通常消耗的算力是推理过程的至少三个数量级以上。**\\n\\n**微调：微调是在预训练模型的基础上对其进行进一步调整以适应特定任务的过程，其算力需求低于训**\\n\\n练，但高于推理。\\n\\n**推理：推理指的是使用训练好的模型来进行预测或分析，是算力消耗最低的阶段。**\\n\\n总的来说，在算力消耗上，训练 > 微调 > 推理。\\n\\n从头训练一个大模型并非易事，这不仅对个人用户，对于许多企业而言也同样困难。因此，如果个人使\\n\\n用，关注点应该放在推理和微调的性能上。在这两种应用需求下，对硬件的核心要求体现在GPU的选择上，\\n\\n**对CPU和内存的要求并不高。无论是选择租用在线算力还是配置本地计算机，如果想在本地运行大模型，我**\\n\\n们可以拆分成两个关注点：\\n\\n模型：选择什么基座模型或微调模型，这可以直接下载至本地。\\n\\n硬件：希望在什么硬件平台上来执行，可以分为 CPU 和 GPU 两大类。\\n\\n⼤部分开源⼤模型⽀持在 CPU 和 Mac M系列芯片上运⾏，但较为繁琐且占⽤内存⾄少 32G 以上，因此\\n\\n更推荐在 GPU 上运⾏。针对本地部署大模型，在选择GPU时，可以遵循的简单策略是：在满足具体的大模\\n\\n**型的官方配置要求下，选择性价比最高的GPU。**\\n\\nGPU的性能主要由以下三个核心参数决定：\\n\\n1. 计算能力：这是最关注的指标，尤其是32位浮点计算能力。随着技术发展，16位浮点训练也日渐普\\n\\n及。对于仅进行预测的任务，INT 8 量化版本也足够；\\n\\n2. 显存大小：大模型的规模和训练批量大小直接影响对显存的需求。更大的模型或更大的批量处理需要更\\n\\n多的显存；\\n\\n\\n-----\\n\\n处理大量数据时的性能通常也越好；\\n\\n注：显存带宽相对固定，选择空间较小。\\n\\n# 二、硬件配置的选择标准\\n\\n无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\\n\\n（如微调或推理）都需要相应的硬件配置方案来支持。所以在选择硬件配置时应根据具体的模型需求和预期\\n\\n**用途来确定。**\\n\\n因此，我们的建议是：根据部署的大模型配置需求，先选择出最合适的 GPU，然后再根据所选 GPU 的\\n\\n**特性，进一步搭配计算机的其他组件，如CPU、内存和存储等，以确保整体系统的协调性和高效性能。最简**\\n\\n**单的匹配GPU的标准是显存大小和性价比。因为训练不纯粹看一个显存容量大小，而是和芯片的算力高度相**\\n\\n关的。因为实际训练的过程当中，将海量的数据切块成不同的batch size，然后送入显卡进行训练。显存\\n\\n大，意味着一次可以送进更大的数据块。但是芯片算力如果不足，单个数据块就需要更长的等待时间显存和\\n\\n算力，必须要相辅相成。\\n\\n简单来说，在深度学习的训练和推理中，GPU的显存主要用于以下几个方面：\\n\\n1. 权重存储：模型的参数，包括权重和偏置，都需要在显存中存储。这些参数是模型进行预测或分类所必\\n\\n需的。\\n\\n2. 中间过程数据存储：在模型计算的前向传播和反向传播过程中，会产生并且需要暂时存储大量的中间计\\n\\n算结果。这些数据同样存储在显存中。\\n\\n3. 计算过程：GPU专门为并行处理大量的矩阵和向量运算而设计，这正是深度学习中常见的计算类型。这\\n\\n些计算直接在显存中进行，以利用GPU的高速运算能力。\\n\\n显存的大小和速度直接影响到模型的处理速度和能处理的模型大小。显存越大，意味着可以处理更大的\\n\\n模型和更复杂的计算任务，但同时也需要更多的能源和可能导致更高的成本。\\n\\n\"芯片\"通常指的是集成电路，它们被集成到各种电脑硬件组件中，如CPU、GPU和主板等。CPU本身就\\n\\n是一种芯片。它是计算机的大脑，负责执行程序和处理数据。显卡上的核心组件是图形处理器\\n\\n（GPU），它也是一种芯片。GPU负责处理图形和视频渲染。\\n\\n**所谓的\"算力\"大小，通常指的是整个计算系统的处理能力，尽管在特定上下文中，它有时特指GPU的处**\\n\\n**理能力。**\\n\\n我们以ChatGLM-6B模型为例，官方给出的硬件配置说明如下：\\n\\n模型量化是一种用于优化模型的技术，特别是在推理时。它通过减少模型中使用的数值精度来减小模\\n\\n型的大小，加快推理速度，并降低内存和能源消耗。模型量化常用于将模型部署到资源受限的设备\\n\\n上，如手机或嵌入式系统，量化程度越高，对硬件的要求就会越低。 单精度通常指的是32位浮点数\\n\\n（FP32），使用32位表示，包括1位符号位、8位指数位和23位尾数位。FP32是标准的训练和推理格\\n\\n式，但由于半精度（FP16）提供了相似的结果且计算速度更快，更节省内存，因此在资源受限或需要\\n\\n\\n-----\\n\\n少，它的计算量就会越小，对应的输出结果的精度也就会越差。\\n\\n## 2.1 选择满足显存需求的 GPU\\n\\n关于如何选择GPU，当前市场 NVIDIA 和 AMD 是两大主要显卡生产商。但在人工智能、大数据、深度\\n\\n学习领域，NVIDIA（通常被称为N卡）几乎独占鳌头。主要原因还是NVIDIA在很早期就开始专注于AI和深度\\n\\n学习市场，开发了强大的软件工具和库，例如cuDNN、TensorRT，这些都是专门为深度学习优化的，与流\\n\\n行的深度学习框架（如TensorFlow、PyTorch等）紧密集成，同时NVIDIA的CUDA（Compute Unified\\n\\nDevice Architecture）作为独特的平行计算平台和编程模型，它允许开发者利用NVIDIA的GPU进行高效的通\\n\\n用计算。这一点对于深度学习和大数据分析等需要大量并行处理的应用来说至关重要。\\n\\n**英伟达是一家什么公司？**\\n\\n这时候可能有小伙伴说了，英伟达是一家卖游戏显卡的，这个说法呢，对，但也不对，从财报来看，英\\n\\n伟达目前主要有四块业务，分别是游戏GPU，数据中心产品，自动驾驶芯片和其他业务。占比分别为\\n\\n33.6%，55.%，3.3%和7.4%。游戏GPU，数据中心产品，自动驾驶芯片其实都可以归类为计算芯片这个门\\n\\n类下面，换句话说，如果从财报公布的业务情况来分析的话，英伟达确实就是一家卖计算芯片的公司。但如\\n\\n果真的把英伟达当做一家卖芯片的公司，那就大错特错了。英伟达的确是靠着游戏显卡起家，并且在人工智\\n\\n能爆发的现在靠着一手AI计算芯片市值突破了万亿美元，但其实它并不是一家卖芯片的公司，我对英伟达的\\n\\n定位是，它是一家卖人工智能系统的公司。这就有两个核心的概念，一个是英伟达的计算芯片，一个是英伟\\n\\n达针对自家芯片做的计算架构CUDA，二者缺一不可。这种定位就像智能手机时代的苹果公司，苹果依靠着A\\n\\n系列芯片和ios操作系统收割了智能手机行业超过80%的利润。人工智能大发展的时代，英伟达就依靠着GPU\\n\\n和计算芯片与CUDA计算架构，共同组成的AI生态系统赢得了市场青睐，根据相关机构的统计数据，在独立\\n\\n显卡领域，英伟达的市占率高达85%，在AI算力芯片领域，在未来可能达到90%，现在做深度学习，英伟达\\n\\n的卡就是刚需，没有其他的选择。\\n\\n因此，我们建议还是选择 NVIDIA 的显卡。如果对应的ChatGLM-6B模型的硬件配置说明，我们就可以\\n\\n这样选择GPU。理论上，在进行少量对话时:\\n\\n在选择显卡时，必须遵循的首要准则是：显卡的显存容量一定要高于大模型官方要求的最低显存配置。\\n\\n这是确保模型能够有效运行的基本要求。显存容量越大，其推理或微调的能力就会越强。当然，随着显存容\\n\\n量的增加，显卡的价格也相应提高。以下是目前最主流的几款大模型的显卡型号及其显存容量：\\n\\n|显卡型号|显存容量|\\n|---|---|\\n|H100|80 GB|\\n|A100|80/40 GB|\\n|H800|80 GB|\\n\\n\\n\\nA800 80 GB\\n\\n\\n-----\\n\\n|显卡型号|显存容量|\\n|---|---|\\n|4090|24 GB|\\n|3090|24 GB|\\n\\n\\n其组合形式可以分为以下四类：\\n\\n1. 纯CPU：基于不同架构的CPU配置，适用于不需要或不能使用GPU加速的场景。（不推荐）\\n\\nx86 (如Intel或AMD)\\n\\nARM (如Apple、Qualcomm、MTK)\\n\\n2. 单机单卡：使用一块GPU进行计算，适用于大多数个人使用和一些中等计算负载的场景。（典型配置）\\n\\nNvidia系列GPU\\n\\nAMD系列GPU\\n\\nApple系列GPU\\n\\nApple Neural Engine（较少见，支持有限）\\n\\n3. 单机多卡：在一台机器上使用多张GPU卡，适用于高计算负载的场景，如模型分割处理。（典型配置）\\n\\n4. 多机配置：使用多台计算机进行集群计算，通常超出个人使用范围，主要用于从头预训练基座模型等高\\n\\n负载任务。\\n\\n所以，在单个显卡的显存容量不足以满足需求时，也可以采用多显卡配置来增加整体的显存容量。只要\\n\\n总显存超过官方推荐的配置要求就可以。此外，在选择显卡时，除了考虑整体显存容量，还要根据不同显卡\\n\\n的性能和成本进行权衡。根据具体需求和预算，决定是选择单张高性能显卡，还是部署多张成本效益更高的\\n\\n低版本显卡。实现最优的性价比。比如在理论上，在进行多轮对话时或需要微调时，采用单机多卡：\\n\\n## 2.2 主流显卡性能分析\\n\\n对于 NVIDIA的显卡（N卡）卡来说，我们可以按照以下几个维度来划分：\\n\\n按照产品线划分：\\n\\n|系列|特点|主要应用领域|\\n|---|---|---|\\n\\n\\nGeForce\\n\\n系列（G\\n\\n\\n消费级GPU产品线，注重提供高性能的图形处理能力和游戏\\n\\n特性 性价比高 适合游戏和深度学习推理 训练\\n\\n\\n主要面向游戏玩家和普\\n\\n通用户\\n\\n\\n-----\\n\\n|系列|特点|主要应用领域|\\n|---|---|---|\\n|Quadro 系列（P 系列）|专业级GPU产品线，针对商业和专业应用领域进行了优化， 适用于设计、建筑等专业图像处理。|设计、建筑、专业图像 处理等领域。|\\n|Tesla系 列（T系 列）|主要用于高性能计算和机器学习任务，集成深度学习加速 器，提供快速的矩阵运算和神经网络推理。|高性能计算、机器学习 任务等领域。|\\n|Tegra系 列|移动处理器产品线，用于嵌入式系统、智能手机、平板电 脑、汽车电子等领域，具备高性能的图形和计算能力，低功 耗。|嵌入式系统、智能手 机、平板电脑、汽车电 子等领域。|\\n|Jetson系 列|面向边缘计算和人工智能应用的嵌入式开发平台，具备强大 的计算和推理能力，适用于智能摄像头、机器人、自动驾驶 系统等。|边缘计算、人工智能、 机器人等领域。|\\n|DGX系列|面向深度学习和人工智能研究的高性能计算服务器，集成多 个GPU和专用硬件，支持大规模深度学习模型的训练和推 理。|深度学习、人工智能研 究和开发等领域。|\\n\\n\\n按照架构划分：\\n\\n|架构|年份|芯 片 代 号|特点|代表产品|\\n|---|---|---|---|---|\\n|Tesla|2006|GT|第一个通用并行计算架构，主要用于科学计算和 高性能计算。|Tesla C870/GeForce 8800 GTX|\\n|Fermi|2010|GF|引入CUDA架构、ECC内存等，用于科学计算、图 形处理和高性能计算。|Tesla C2050/GeForce GTX 480|\\n|Kepler|2012|GK|功耗效率和性能改进，引入GPU Boost技术，适 用于科学计算、深度学习和游戏。|Tesla K40/GeForce GTX 680|\\n|Maxwell|2014|GM|提高功耗效率，引入新技术如多层次内存系统， 应用于游戏、深度学习和移动设备。|Tesla M40/GeForce GTX 980|\\n|Pascal|2016|GP|16nm FinFET制程技术，加强深度学习和AI计算 支持，引入Tensor Cores，应用于深度学习和高 性能计算。|Tesla P100/GeForce GTX 1080|\\n\\n\\n-----\\n\\n**架构** **年份**\\n\\n|Col1|Col2|号|Col4|Col5|\\n|---|---|---|---|---|\\n|Volta|2017|GV|深度学习优化特性，如Tensor Cores，主要用于 深度学习、科学计算和高性能计算。|Tesla V100|\\n|Turing|2018|TU|实时光线追踪技术、深度学习技术，适用于游 戏、深度学习和专业可视化。|Tesla T4/GeForce RTX 2080 Ti|\\n|Ampere|2020|GA|第二代深度学习架构，更多Tensor Cores、改进 的Ray Tracing技术，应用于深度学习、科学计算 和高性能计算。|Telsa A100/GeForce RTX 3090|\\n|Ada Lovelace|2022|AD|专为光线追踪和基于AI的神经图形设计，第四代 Tensor Core，第三代RT Core，提高GPU性能。|GeForce RTX 4090|\\n|Hopper|2022|GH|下一代加速计算平台，支持PCIe 5.0，专用的 Transformer引擎，适用于大型语言模型和对话 AI，提供企业级AI支持。|Telsa H100|\\n\\n\\n\\n按照应用领域划分：\\n\\n\\n**特点** **代表产品**\\n\\n|类型|系列|描述|应用领域|代表产品|\\n|---|---|---|---|---|\\n|游戏娱 乐|GeForce RTX™系 列|面向大众消费级游戏和创作者用户的图形 加速卡。在性能、功耗和成本之间达到最 佳平衡点,提供极致的游戏和创作体验。|游戏、娱乐、 内容创作|如RTX 3090、RTX 4090等|\\n|专业设 计和虚 拟化|NVIDIA RTX™系 列|面向专业可视化和创意工作负载的高性能 GPU,提供强大的计算性能、大容量视频 内存等。服务于工业设计、建筑设计、影 视特效渲染等专业用户。|工业设计、建 筑设计、影视 特效渲染|高端专业可视 化工作站级显 卡|\\n|深度学 习、人 工智能 和高性 能计算|A系列、 H系列、 L系列、 V系列、 T系列|不同系列针对不同AI和计算需求。A系列 是AI计算加速器; H系列是AI超算; L系列 是边缘AI推理; V系列是虚拟工作站; T系 列是AI推理优化解决方案。|数据中心AI训 练和推理、边 缘AI、虚拟桌 面、AI推理加 速|A100、 A30、A40、 H100、 L40、 DeepStream 加速器等|\\n\\n\\n像大模型领域这种生成式人工智能，需要强大的算力来生成文本、图像、视频等内容。在这个背景下，\\n\\nNVIDIA先后推出V100、A100和H100等多款用于AI训练的芯片，其中 A100 是 H100 的上一代产品，于2020\\n\\n年发布，使用7纳米工艺，支持AI推理和训练。而H100，该显卡是2022年3月发布，可谓是核弹级性能显\\n\\n卡，采用了台机电4纳米工艺，具备800亿个晶体管，采用最新 Neda Hopper架构，同时显存还支持\\n\\nhbm3，最高带宽可达 3TB每秒。第四代MNLINK的带宽，900G每秒。是PCIE5.0的7倍，比上一代的A100显\\n\\n\\n-----\\n\\n飞跃，各项基础性能是A100的三倍之多，H100的单片显卡售价24万元左右。\\n\\n但在2022年10月，漂亮国政府为了限制我国的人工智能发展，发布禁令：禁止NVIDIA向中国出售A100\\n\\n和H100显卡。数据显示，2022年中国市场的人工智能芯片规模高达70亿美元，而这70亿的市场，被NVIDIA\\n\\n垄断了90%，虽然NVIDIA的A100，H100这样的顶级芯片不能卖给中国，但NVIDIA作为商业公司，也是要做\\n\\n生意的，于是为了合规，NVIDIA针对传输速率进行了限制，提出了中国大陆特供版的A800和H800，即：\\n\\nH100、A100的阉割版。\\n\\n也就是说，由于漂亮国的禁令，我们现在使用的GPU都是中国特供版的，说白了就是阉割版的，像\\n\\nA100，到国内就成了A800，H100到国内就成了H800，那么 A ~ H的差距在哪里呢？\\n\\n直接用 SXM 版本的H800进行对比，只能说这个参数对比，对于不了解的人来说，还是比较出人意料\\n\\n的，除了 FP64 和 NVLink传输速率上的明显削弱，其他参数和H100都是一模一样的。FP64上的削弱主要影\\n\\n响是H800在科学计算、流体计算、有限元分析等超算领域的应用，受到影响最大的还是NVLINK上的削减，\\n\\n但因为架构上的升级，虽然比不上同为 Hoper架构的H100，但是比AMPERRE架构的A100还是要强上不少，\\n\\n说白了，老黄想要抓住国内市场，就算是阉割，也不会阉割的特别过分，漂亮国政府想限制国内的超算，那\\n\\n就算把超算性能砍掉，传输速率减小，换个名字，GPU照卖。只要保证H800在大部分场景下的性能不受影\\n\\n响，能满足大部分人的使用需求就足够了。毕竟也不会有人跟钱过不去，所以 其实H800和 H 100的性能差\\n\\n距并没有想象的那么夸张，就算是砍掉了FP64和 NVLINK的传输速率，性能依旧够用。最关键的是，它合法\\n\\n呀。所以如果不是追求极致性能的话，也没必要冒着风险去选择H100。\\n\\n而就在今年的10月份，漂亮国又玩起了变卦，10月份刚升级了芯片禁令，开启了新一轮的出口管制，先\\n\\n预留了30天的窗口期，随后又要求立即生效，连30天都没了，也就是说，从10月份开始，中国将无法再获得\\n\\nNVIDIA5类的GPU显卡 （A800、A800、H100、A100，L40S），其实早在8月份的时候，BAT的一些大厂不\\n\\n知道是收到风声还是控制风险，就向NVIDIA提前订购了10万个A800芯片，结果这次也是彻底泡汤。其实从\\n\\n\\n-----\\n\\n的尖端AI芯片了，漂亮国就是亮牌，高端AI芯片，必禁无疑。所以对于目前的 A100系列和H100系列，因为\\n\\n是漂亮国断供之前出的芯片，所以现在国内还有货，只不过市场渠道比较乱，需要甄别。\\n\\n同时需要说明的是，GeForce 系列显卡虽被官方定位为面向消费级市场，适合游戏爱好者。但这类显卡\\n\\n在深度学习领域同样展现出了出色的性能，很多人用来做推理、训练，单张卡的性能跟深度学习专业卡Tesla\\n\\n系列比起来其实差不太多，但是性价比却高很多。对于大模型来说，同样可以使用GeForce 系列显卡。\\n\\n那么个人使用或者实验室针对大模型的推理和微调需求配置服务器，高端显卡目前我们可选的就是\\n\\nA100、A800、H100和4090等，应该如何选呢？\\n\\n## 2.3 单卡4090 vs A100系列\\n\\n先说结论：没有双精度需求，追求性价比，选4090。有双精度需求，选A100，没有A100选A800。如果\\n\\n**是做大模型的训练，GeForce RTX 4090 是不行的。但在执行推理（inference/serving）任务时，使用**\\n\\n**RTX 4090 不仅可行，而且在性价比方面甚至略优于 A100。同时如果做微调，也勉强是可以的，但建议多**\\n\\n**卡。**\\n\\n|GPU 型号|Tensor FP16 算力|Tensor FP32 算力|内存 容量|内存 带宽|通信 带宽|通信 时延|售价（美元）|\\n|---|---|---|---|---|---|---|---|\\n|H100|989 Tflops|495 Tflops|80 GB|3.35 TB/s|900 GB/s|~1 us|30000~40000|\\n|A100|312 Tflops|156 Tflops|80 GB|2 TB/s|900 GB/s|~1 us|15000|\\n|4090|330 Tflops|83 Tflops|24 GB|1 TB/s|64 GB/s|~10 us|1600|\\n\\n\\n\\n**推理**\\n\\n从数据对比来看，A100 和 GeForce RTX 4090 显卡在通信能力和内存容量方面存在显著差异，但在算力\\n\\n上差距并不大。在 FP16 算力方面，两者几乎相当，4090 甚至略有优势。相较于 A100，其较高的性价比主\\n\\n要源于推理过程通常涉及单一模型，在这种场景下，显卡的算力才是关键因素，而 4090 在这方面表现出\\n\\n色。虽然内存带宽同样重要，但在推理任务中，4090 的内存带宽通常足以应对需求，不会成为显著的制约\\n\\n因素。\\n\\n[LambdaLabs 有个很好的 GPU 单机训练性能和成本对比：https://lambdalabs.com/gpu-benchmarks](https://lambdalabs.com/gpu-benchmarks)\\n\\n， 我们来看：\\n\\n\\n-----\\n\\n高的。\\n\\n**微调**\\n\\n反观训练需求下，4090在训练的时候表现不佳的原因主要是其有限的通信能力和内存容量。比如训练\\n\\nLLaMA-2 70B 时需要2400块 A100 ，同时据说训练ChatGPT用了上万块 A100，主要还是因为训练过程除了\\n\\n存储模型参数外，还需要处理大量数据以及各层之间的中间数据和参数。因此，大容量内存和高通信带宽会\\n\\n比较关键，以便高效地处理和协调这些信息。首先就是把n个T的数据，分发到不同的GPU上去，然后训练，\\n\\n这叫数据并行。第二个并行就是会把这个模型的数据在一块GPU里可能放不下，所以要按照每一层，把某几\\n\\n层放在不同的GPU上面，进行串联。这就叫流水线并行。第三个就是Tensor张量并行。主要是我们目前训练\\n\\n的Transform模型都是多头的，每一个头都是可以按照张量来进行并行训练。所以整个 LLaMA-2 70B他会通\\n\\n过张量，流水线、数据三种并行方式，从模型内层，到模型层之间，和训练数据三个维度进行计算空间的划\\n\\n分。\\n\\n2400块GPU之间要进行大量的协调和通讯计算，这种复杂的并行结构需要 GPU 之间进行大量的协调和\\n\\n通信。4090 的通信带宽仅为 64 GB/s，与 A100 的 900 GB/s 相比差距过大，导致在这类大规模训练任务中\\n\\n通信成为瓶颈，进而影响整体性价比。因此，尽管 4090 在某些方面表现优秀，但在大模型训练中的局限性\\n\\n仍然明显。尽管微调过程对硬件的要求相较于训练相对较低，但这个过程仍然需要足够的内存以存储模型参\\n\\n数，以及有效的通信带宽来处理数据和模型层之间的交互。所以对于需要高通信带宽和大内存容量的大模型\\n\\n微调任务，A100等高端GPU可能是更合适的选择。\\n\\n我们拿 GPT 3 来说，GPT 3的参数将近700亿，假设每个参数使用4字节（通常使用float 32）进行存\\n\\n储，训练运算储备需求是 4200 GB，完成一次GPT 3训练的总算力是：3.15 * 10 ^23 Flops,仅考虑算力的情\\n\\n况下，单块 A100 需要45741天，几乎是128年（假设有效算力是78Tflpos），单块4090 需要91146天，几\\n\\n乎是250年，（假设有效算力是40 Tflpos）。任何一张单卡训练一次都需要超过100年，对于参数量达到10\\n\\n亿级别的大模型，参数本身就大，而且大模型通常需要更高的显存来来存储参数、中间计算结果和梯度等，\\n\\n既然要多卡运行，数据的同步效率就会显得非常重要，那么内存带宽、通信带宽、通信延时等性能将非常非\\n\\n常重要。，4090 24g的显存小，而且内存带块、通信贷款、和通信延时都相对较弱，这就有点像短板理论。\\n\\n最弱的那一项就决定了显卡的能力。综上，4090在较大的大模型没有什么发挥的余地，但随着现在的大模型\\n\\n越来越小，对显存和算力需求相对较小的大模型，再加上推理的算力需求更低，如LLama 7B 13B模型，单\\n\\n卡的4090 都可以运行，至少对于学习和研究大模型的个人或实验室来说，4090还真是不错的选择。\\n\\n⼤模型的⼯业级实践要求，⼤模型全量微调需要⾄少4张A100 80G显卡；（ChatGLM6B模型 全量微调\\n\\n差不多也是需要这个配置）\\n\\n## 2.4 单卡4090 vs 双卡3090\\n\\n如果预算差不多的情况下，对于两张3090与一张4090的选择，推荐使用两张3090显卡。虽然从算力角\\n\\n度看，两张3090与一张4090大致持平，但两张3090显卡提供的总显存会更多，这对于处理大型模型尤为重\\n\\n要。目前，大多数深度学习计算框架都支持各种并行计算技术，如流水线并行、张量并行和CPU卸载。这些\\n\\n技术使得即使是显存较小的显卡也能处理大型模型。在这种情况下，双3090配置可以更有效地利用流水线并\\n\\n行，同时，与单4090配置相比，CPU卸载的需求会降低。此外，企业环境一般都是多卡并行，使用双3090配\\n\\n置还可以练习如何写多卡代码。现有技术如串行反向传播，进一步增强了多卡系统的效率，使其成为一个经\\n\\n济高效的选择，尤其是在需要处理大量数据和复杂模型的情况下。因此，从目前的技术和应用需求来看，选\\n\\n择两张3090显卡无疑是更优的选择。\\n\\n## 2.5 风扇卡与涡轮卡如何选择\\n\\n\\n-----\\n\\n风扇卡与涡轮卡的供电接口位置不同，涡轮卡的供电接口位置在接口尾部，供电线比风扇卡的线\\n\\n更短，这样是方便安装和理线，而风扇卡供电接口一般在显卡顶部，接线后线缆会高于机箱最高\\n\\n面，在服务器中使用风扇卡，服务器盖板盖不上。\\n\\n在散热方向上面，涡轮卡散热方向是朝尾部散热，并于服务器风向是一致的，而风扇卡的散热是朝四面\\n\\n八方来散热的，平常的PC机箱放一张是可以适应的，但用作服务器上（很多时候是多卡）就不适合了，\\n\\n很容易因为温度过热出现宕机。\\n\\n风扇卡与涡轮卡的尺寸大小不同\\n\\n涡轮卡与风扇卡的尺寸大小也是不一样的，风扇卡的尺寸一般是2.5-3倍宽设计，而涡轮卡的尺寸\\n\\n大小是双宽设计，因为涡轮卡为了方便放入服务器里，所以涡轮卡的尺寸和高度都远远低于风扇\\n\\n卡，从而服务器可以支持4卡或者8卡，如果用风扇卡代替涡轮卡装在服务器里，那位置够不够还\\n\\n是一回事儿呢。\\n\\n面对市场不同\\n\\n风扇卡无论是公版显卡还是非公版显卡，风扇卡都是面向个人的，是应用在个人游戏行业的，\\n\\n4090风扇卡的特点就是外观炫酷，而个人游戏行业就是为了风扇卡的外观和玩游戏的性能。而\\n\\n4090涡轮卡是定制版，是面向AI科技产业，因为做工精巧、支持多卡安装、性价比高等一系列优\\n\\n点，4090涡轮卡深受广大AI深度学习用户的喜爱。\\n\\n## 2.6 整机参考配置\\n\\n确定GPU后，根据GPU搭配合适的计算机组件，具体来说，计算机八大件：CPU、散热器、主板、内\\n\\n存、硬盘、显卡、电源、风扇、机箱。个人使用的计算机，典型的配置是单GPU或双GPU，一般不超过四个\\n\\nGPU，否则常规的机箱放不下，且运行时噪声很大，而且容易跳闸。\\n\\n目前国内实验室主流的还是4090和3090,10万+的预算配置4张4090是没问题的，20~30万的预算则可以\\n\\n考虑8张4090，或者两张A100 80G，如果预算不限，A100 8卡服务器一定是最佳选择。\\n\\n这里给出一个本地部署ChatGLM-6B，同时也适用于大多数消费级实验环境的配置：\\n\\nGPU：3090双卡，涡轮版；总共48G显存，能够适⽤于⼤多数试验和复现性质深度学习任务；同时双卡\\n\\n也便于模拟多卡运⾏的⼯业级环境；\\n\\nCPU：AMD 5900X；12核24线程，模拟普通服务器多线程设置；\\n\\n存储：64G内存+2T SSD数据盘；内存主要考虑机器学习任务需求；\\n\\n电源：1600W单电源；双卡GPU的电源在1200W-1600W均可；\\n\\n主板：华硕ROG X570-E；服务器级PCE，⽀持双卡PCIE；\\n\\n机箱：ROG太阳神601；atx全塔式⼤机箱，便于⾼功耗下散热；\\n\\nA800 工作站的典型配置信\\n\\n|配置项|规格|\\n|---|---|\\n|CPU|Intel 8358P 2.6G 11.2UFI 48M 32C 240W *2|\\n|内存|DDR4 3200 64G *32|\\n\\n\\n\\n数据盘 960G 2.5 SATA 6Gb R SSD *2\\n\\n\\n-----\\n\\n|配置项|规格|\\n|---|---|\\n|硬盘|3.84T 2.5-E4x4R SSD *2|\\n|网络|双口10G光纤网卡（含模块）*1|\\n||双口25G SFP28无模块光纤网卡（MCX512A-ADAT ）*1|\\n|GPU|HV HGX A800 8-GPU 8OGB *1|\\n|电源|3500W电源模块*4|\\n|其他|25G SFP28多模光模块 *2|\\n||单端口200G HDR HCA卡(型号:MCX653105A-HDAT) *4|\\n||2GB SAS 12Gb 8口 RAID卡 *1|\\n||16A电源线缆国标1.8m *4|\\n||托轨 *1|\\n||主板预留PCIE4.0x16接口 *4|\\n||支持2个M.2 *1|\\n|原厂质保|3年 *1|\\n\\n\\n总的来说：\\n\\n3090⽐4090综合性价⽐更⾼，不过4090计算速度⼏乎是3090的两倍，有需求亦可考虑升级，不过\\n\\n4090需要的机箱空间更⼤、电源配置也要求更⾼；\\n\\n双卡GPU升级路线：3090—>4090—>A100 40G （2.5w左右）—>A100 80G（6~7w左右）；\\n\\n⼤模型的⼯业级实践要求，⼤模型全量微调需要⾄少4张A100 80G显卡；（ChatGLM6B模型 全量微调\\n\\n差不多也是需要这个配置）\\n\\n## 2.7 显卡博弈的形式分析\\n\\n除此之外，在2023年11月13日老黄又放出来两个核弹。第一个核弹就是它推出了全新的超算GPU\\n\\nH200，直接说是当世最强，听起来很嚣张，但其实一点没有吹牛。在AI超算领域，对手只有看NVIDIA车尾\\n\\n灯的份。从数据层面看，H200强在大模型推理上，以700亿参数的Llama2 二代大模型为例，h200推理速度\\n\\n几乎比前代的h100快了一倍。而且能耗还降低了一半。显存从h100的80GB，直接拉到了141gb，带宽也从\\n\\n3.35TB/s，提升到了4.8TB/s，最新的GPU H200，跟前一代H100相比，最大的提升就是它的内存，达到了\\n\\n惊人的1.15TB/s，相当于在1s内传输了 230步FHD的高清电影。如果每一部的容量按5G来算的话。这个跟我\\n\\n们以前的计算机里的内存条就不一样了，它采用的最新技术是HBM3e，HBM就是高带宽内存，这个实现是\\n\\n把DRAM内存用3D封装的技术叠了起来，然后把它和GPU芯片放在同一个GPU的底板上，它们之间的通信就\\n\\n通过这块晶元直接做了，这样内存的容量就大大提高了，同时他们和GPU的通信速度也有显著的增长，。达\\n\\n到了每秒钟4.8个TB。然后又把所有的软件做了优化，这样就使得ChatGPT这样 大模型的推理速度大大的提\\n\\n升，跟A100相比提高了 18倍。第二个核弹就是CPU和GPU的合体，GH200， 就是把ARM的CPU和它的GPU\\n\\n封装在了同一块GPU晶圆板上，这样CPU和GPU之间的传输速度就非常快，而且可以共享内存。内存也达到\\n\\n\\n-----\\n\\n有1/2。\\n\\n炸一听好像是王炸升级，刚装满h100的企业要哭晕在厕所了。但实际上，它可能只是h100的一个中期\\n\\n改款，单论峰值算力，H100和H200其实是一模一样的。，真正提升的是显存和带宽，然而对于AI芯片的性\\n\\n能，讨论最多的是训练能力。，在GPT 3 175B大模型的训练中，H200相较于H100，只强了10%，提升并不\\n\\n明显，这操作，大概率是老黄有意为之，以前为了打造大模型，对GPU的首要要求是训练，但是到了现在，\\n\\n随着各种AI大预言模型的落地，大家开始卷的是推理速度。于是H200的升级，就忽略了算力升级，转向推理\\n\\n方面的发力，老黄的刀法依旧精准。哪怕只是小提升，依然当得起最强的称号。谁让NVIDIA的显卡，在AI芯\\n\\n片这块，这就是遥遥领先。\\n\\n但这因为是断供后的新卡，国内现在基本买不到。\\n\\n在H200没出现以前，H100是地表最强GPU，NVIDIA每一个层级的性能基本都是翻倍的，H100，其中\\n\\n微软采购了 15w片，mate 采购了 15w片，谷歌、亚马逊、甲骨文、腾讯都是5w片，那么谷歌的gemini发布\\n\\n晚，原来是因为缺少GPU哈。一共是 48w片，和外界传的 一年H100的产量50w基本吻合。在2024年预计出\\n\\n货量在200万张。中国采购的用户 H800要比H100量大，而且H800的售价比H100还要高，为什么性能不行\\n\\n价格还是高呢，主要原因还是有一份建议国企采购产品中的文件，里面只有A800和H800，没有A100和\\n\\nH100，这就导致国企采购更愿意采购A800的原因，\\n\\n同时需要说的是，在今年的10月份，漂亮国再次禁用 H800、A800芯片后，NVIDIA计算再次推出中国特\\n\\n供AI芯片，初步计划是3款，分别是h20，L20和 L2，这三款基于H100进行阉割。使以性能符合禁令的要\\n\\n求。其中最强的是H20，但与H100相比，性能被封印了80%，只有H100的20%左右的性能，对于NVIDIA而\\n\\n言，中国这笔70亿美元的大市场肯定不能丢，必须推出AI芯片来抢占，不过，近日有消息传出，这三款特供\\n\\n版芯片要跳票了，只有L20可能会按期推出，H20和L2都可能延期。特别是 H20这个最强的，什么时候推\\n\\n出，会不会推出都是未知数，最重要的原因，还是目前中国的市场已经发生了变化。没有NVIDIA想像的那么\\n\\n美好了。\\n\\n有一些朋友可能还不知道这回事，而知道的朋友有些也不清楚，为什么偏偏是显卡会成为国际博弈的工\\n\\n具，这些卡一张卖 十几 甚至几十万元，这么赚钱的生意，怎么就不让做了。在1999年之前的人类文明早\\n\\n期，世界上是没有显卡的，但已经有了电子游戏了，那时候的游戏画面是由CPU生成的，游戏玩家说，需要\\n\\n有高画质，于是就有了显卡。1999年，NVIDIA声称自己发明了GPU，也就是 GFFORCE 256，所谓的GPU，\\n\\n就是图形计算单元，他是显卡最最核心的部件，再给他配上其他一系列零件，就成为了一张显卡。GPU跟显\\n\\n卡，严格来说不是一个概念，只是大家平时很少特意区分。这玩意为啥能提升游戏画质？因为渲染游戏画面\\n\\n这件事，难就难在计算量太大了，比如游戏中任何一个3D物体，它的位置、方向、大小、光源、物体表面等\\n\\n变化，都需要电脑来计算。\\n\\n渲染画面这件事，就像再做10000道加减乘除，CPU的核心很强，但数量少。每个核心就像出于一个智\\n\\n力巅峰的高三学生，他能熟练的解出模拟卷上的最后一道大题，但让他算 1000道，他得累死。而显卡上面\\n\\n密密麻麻的分布着几千个小核心，每个核心都像是一个小学生，高考题肯定是不会，但他们能同时并排启\\n\\n动，在10秒只能，把10000道题做完。所以渲染特别快。显卡能提升画质，就是因为他有这种强大的并行计\\n\\n算能力。而显卡从此脱离游戏领域，被别的领域盯上，乃至成为国际博弈的筹码。也是因为这种强大的并行\\n\\n计算能力，。发布了没几年，斯坦福大学实验室就盯上了显卡，它们想要显卡解决其他领域的问题，但是并\\n\\n不简单，显卡算力虽强，但是它们都是小学生，需要合适的软件能驾驭才行。沿用刚才的比方，GPU就是一\\n\\n万个小学生在同时工作，计算能力很强，但前提是你得能把一道难解的大题，分解成无数个小学生能解决的\\n\\n简单问题才行。否则显卡再强又有什么用呢？转换到现实中，就是得让开发者能方便的写出代码。利用上显\\n\\n卡的并行计算能力才行。所以在2006年，带领团队出现了至今仍然在不断更新的 CUDA，CUDA就是更方便\\n\\n的让开发人员能够面向 编程 如果绝大多数 模型的训练 背后都离 开 的支持 除了\\n\\n\\n-----\\n\\n并行计算能力，在软件层面，配套的编程平台也成熟了，这就意味着，GPU可以完全离开游戏领域，走向更\\n\\n大的世界了。\\n\\n第一次感受到GPU，就是挖矿，也就是挖比特币，挖矿其实就是用计算机 来解决数学问题，比如任何数\\n\\n据，都可以通过哈希算法生成一串哈希值，原始数据不管发生多小的变化，最后生成的哈希值都完全不同。\\n\\n我们有时候下载大文件时，也会利用哈希算法的这种特性，来做一次校验。。看看下载的文件是否完整。很\\n\\n多挖矿，就是要生成一个符合要求的哈希值，这就需要计算机去反复的尝试，所以挖矿就跟游戏画面一样，\\n\\n属于那种不难，但是计算量非常大的事情。恰好能够利用显卡的算力。于是在加密价格持续攀升的日子里，\\n\\n显卡涨价，缺货。一路推动NVIDIA的市值从140亿美元暴涨到了 1750亿美元。但显卡跟加密货币之间只是一\\n\\n段露水情缘，随着专用矿机的出现，虚拟比特币的价格跳水，以太坊等调整了挖矿规则等原因，显卡跟虚拟\\n\\n货币的关系已经大不如前了。但显卡就跟上了更大、更革命的科技浪潮，就是AI。现在所有人都知道，AI是\\n\\n可能引起新一轮科技革命的巨大产业，而几乎所有的AI模型训练，都需要显卡。\\n\\n就拿现在正火的ChatGPT来说，它的模型训练中涉及到大量的矩阵运算，这些矩阵运算本身不难，但是\\n\\n量很大很大，所以就需要GPU来并行处理。AI是可能改变世界的，而AI的基础是 算法、算力和数据。而提到\\n\\n的A100和H100，售价高到十几万、甚至几十万的专业显卡，还供不应求。有报道说，训练ChatGPT需要相\\n\\n当于300块A100显卡的算力，光这一个项目就需要花几十个亿来购买显卡，这也是为什么 从2022年10月开\\n\\n始，NVIDIA的市值在半年时间内就飙升了34倍。\\n\\n## 2.8 国产AI超算芯片期待\\n\\n这么着急赶尽杀绝，不惜拉上自己企业垫背。答案就是我国在半导体自主研发上，已经触碰到了他们的\\n\\n痛处。很多人总以为，我们依赖国外的AI芯片，是自身技术上不过关，其实真相是我们的芯片都没有真正的\\n\\n上过牌桌，为什么？ AI芯片只有在实际应用中才能够发现问题，加快迭代，而我们的国产芯片，起步晚、性\\n\\n能差，所以国内的厂商大多不愿意使用，这也就造成了国产芯片无法获得正向反馈，发展速度只会越来越\\n\\n慢，这是一个矛盾的循环。在还有选择的时候，考虑到性能也好，成本也好，中国企业往往愿意选择像\\n\\nNVIDIA的芯片，所以在很长一段时间，美国对中国的高端芯片的制裁，也不是彻底封死，因为国产的AI芯片\\n\\n不是它的对手，都还不够好用，但现在，局面彻底改变了。出口禁令全面升级，中国企业没有了其他的任何\\n\\n选择，下一步只能规模化的采购国产芯片，并且培育出本土的产业链，逐步实现真正意义上的国产替代。那\\n\\n可能有人说，这件事这么简单，能实现岂不是早就实现了。不太可能。其实还真不一定，放眼全球算力芯片\\n\\n市场，不可否认，NVIDIA占据了九成多的份额，出于高度垄断的地位。但是，目前国产AI芯片的可替代方\\n\\n案，也不少。\\n\\n如果单看并行计算这个领域，有两家国产GPU公司值得关注：分别是摩尔线程和壁任科技。\\n\\n摩尔线程2020年10月成立，在2023年10月17日，第一款产品摩尔芯用了7纳米工艺，支持CUDA平台和\\n\\n算法模型，性能超过每秒20万亿次浮点计算，仅成立三年就上了白宫严选名单。成为老美的制裁对象之一。\\n\\n是现在唯一可能买到的实际产品，并且一直在更新驱动的公司。壁任科技一款产品 壁任一号，7纳米工艺，\\n\\n支持CUDA平台和算法模型，性能超过每秒30万亿次浮点计算。去年发布了一个GPU 叫BR100，性能就直逼\\n\\n英伟达的H100，但是这个芯片并没有使用，原因还是台积电不给生产，准确的说是美国政府不让台积电给我\\n\\n们生产，这就是一个不公平的竞争，华为的遭遇大家就更熟悉了，19年以后 芯片的生产、制造全都被摁的死\\n\\n死的，但是麒麟芯片出来了，说明我们大概率已经突破了先进芯片制造的生产流程了，顶多就是成本高一\\n\\n点。\\n\\n这些当中除了华为之外都面临相同的问题，那就是没有针对芯片专门优化的计算架构，换句话说，这些\\n\\n公司不具备与CUDA抗衡的能力，如果能成为芯片供应商，去识别其他的计算架构，理论上也是可以的。但\\n\\n是对于使用者来说这样做效率就太低了。一旦训练十几天，一旦出现BUG，不就前功尽弃了吗。所以这事还\\n\\n是得看华为 华为就厉害了 在人工智能领域的布局包括但不限于昇腾系列计算芯片 CANN异构计算架\\n\\n\\n-----\\n\\n可能布局出全栈式人工智能的公司，也是唯一一个全栈式国产化公司，困扰华为的最大问题还是美国的制\\n\\n裁，芯片没法生产，再怎么布局也是白搭，一旦能解决芯片问题，\\n\\n有句话说的好，天下苦英伟达久矣。有时候看似把我们逼入绝境，但其实是给我了我们绝地求生的机\\n\\n会，就看我们怎么把握了。尽管中国企业会迎来一段痛苦的过渡期，但从长远来看，这是一条不得不走的\\n\\n路。而面对当下的巨大差距，最关键的是终于有了攻坚克难的凝聚力。相信度过这段阵痛期，有的人，有的\\n\\n国家，只会后悔的拍大腿，眼睁睁的看着被逆袭、被超越，毕竟每一次我们都是这样过来的。\\n\\n# 三、组装计算机硬件选型策略\\n\\n计算机八大件：CPU、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于一套\\n\\n需要部署大模型的个人计算机，如何搭配。\\n\\n## 3.1 GPU选型策略\\n\\n1. 选择厂商\\n\\n目前独立显卡主要有AMD和NVIDIA两家厂商。其中NVIDIA在深度学习布局较早，对深度学习框架支持\\n\\n更好。建议选择NVIDIA的GPU。\\n\\n[桌面显卡性能天梯图：https://www.mydrivers.com/zhuanti/tianti/gpu/index.html](https://www.mydrivers.com/zhuanti/tianti/gpu/index.html)\\n\\n2. 选择系列及品牌\\n\\n**对个人用户来说，就是从NVIDIA的RTX系列中，选择出合适的GPU。就部署大模型的需求来说，只需**\\n\\n考虑计算能力和显存大小就可以了。显存带宽通常相对固定，选择空间较小。目前各级别显卡的均价如下：\\n\\n\\n-----\\n\\n|品牌|华硕|微星|技嘉|\\n|---|---|---|---|\\n|顶级旗舰||||\\n|旗舰|ROG猛禽|超龙X|大雕|\\n|次旗舰|TUF|魔龙|超级雕/小雕|\\n|中端|巨齿鲨|/|雪鹰/魔鹰|\\n|丐版|DUAL|万图师|猎鹰|\\n\\n\\n华硕显卡品控优做工好，高品牌信仰足，但溢价严重，这个牌子最会宣传，卡不错但性价比不是很高。\\n\\n高端优先推荐华硕ROG猛禽，当然缺点就是：贵，另外主流用户个人更推荐TUF，更低端的巨齿鲨和\\n\\nDUAL不太推荐\\n\\n微星显卡30系列之前更推荐魔龙，30系列更推荐超龙\\n\\n**准一线**\\n\\n|品牌|七彩虹|\\n|---|---|\\n|顶级旗舰|九段|\\n|旗舰|火神/水神|\\n|次旗舰|adoc|\\n|中端|ultra|\\n|丐版|战斧|\\n\\n\\n\\n推荐七彩虹，用料良心，保修出名的好，深受矿老板们的喜爱，支持个人送保，ultra，三风扇，散热性\\n\\n能极好，噪音小，白色颜值高，不带rgb灯效，喜欢rgb灯效的可以选择adoc。\\n\\n**二线**\\n\\n|品牌|影驰|索泰|映众|耕升|铭瑄|\\n|---|---|---|---|---|---|\\n|顶级旗舰||||||\\n|旗舰|名人堂|PGF/AMP|冰龙寒霜||MGG 大玩家|\\n|此旗舰|GAMER/星耀|天启|超级冰龙||电竞之心|\\n|中端|金属大师|/|冰龙|炫光/星极||\\n|丐版|黑将/大将|X-gaming|黑金至尊|追风|turbo/终结者|\\n\\n\\n\\n二线品牌中，影驰和索泰属于二线中实力比较强的两个，可以说是强二线，索泰重点推荐PGF（排骨\\n\\n\\n-----\\n\\n级别的产品，颜值高，性能强，次旗舰GAMER和星耀一个主打DIY一个主打RGB，都是非常有特点的产品\\n\\n企业级显卡\\n\\n参考第二部分的GPU推荐。\\n\\n服务器推断卡\\n\\n除了用于训练，还有一类卡是用于推断的（只预测，不训练），如：\\n\\n这些卡全部都是不带风扇的，但它们也需要散热，需要借助服务器强大的风扇被动散热，所以只能在专\\n\\n门设计的服务器上运行，性价比首选 Tesla T4，但是发挥全部性能需要使用 TensorRT 深度优化，目前仍然\\n\\n存在许多坑，比如当你的网络使用了不支持的运算符时，需要自己实现。\\n\\n**避免踩坑**\\n\\n如果选择配置单机多卡，采购显卡的时候，一定要注意买涡轮版的，不要买两个或者三个风扇的版本，\\n\\n除非只打算买一张卡。因为涡轮风扇的热是往外机箱外部吹的，可以很好地带走热量，如果买三个风扇的版\\n\\n本，插多卡的时候，上面的卡会把热量吹向第二张卡，导致第二张卡温度过高，影响性能。\\n\\n## 3.2 CPU选型策略\\n\\nCPU在大模型使用中起到什么作用？当在GPU上运行大模型时，CPU几乎不会进行任何计算。最有用的\\n\\n应用是数据预处理。CPU负责将数据从系统内存传输到GPU的显存中，同时也处理GPU完成计算后的数据。\\n\\n有两种不同的通用数据处理策略，具有不同的CPU需求。\\n\\n训练时处理数据：高性能的多核CPU能显著提高效率。建议每个GPU至少有4个线程，即为每个GPU分\\n\\n配两个CPU核心。每为GPU增加一个核心 ，应该获得大约0-5％的额外性能提升。\\n\\n训练前处理数据：不需要非常好的CPU。建议每个GPU至少有2个线程，即为每个GPU分配一个CPU核\\n\\n心。用这种策略，更多内核也不会让性能显著提升。\\n\\n在这种情况下，GPU通常承担大部分计算负担，CPU的作用更多是管理和协调，因此需要高核心数，同\\n\\n时也需要快速的数据预处理，同样需要高频率，所以高核心 + 高频率，虽然不是必须，但推我们推荐还是能\\n\\n**高即高，标准是：要与选择的GPU和CPU的性能水平相匹配，避免将一款高端显卡与低端CPU或一款高性能**\\n\\nCPU与低端显卡匹配，因为这可能导致性能瓶颈。比如：\\n\\nNVIDIA GeForce RTX 3090 * 2 搭配 Intel Core i7-13700K/KF 或 AMD 5900X CPU；\\n\\nNVIDIA GeForce RTX 3080 搭配 Intel Core i9-11900K CPU。\\n\\n但相对来说，瓶颈没有那么大，一般以一个GPU对应 2~4 个CPU核数就满足基本需求，比如单卡机器买\\n\\n四核CPU，四卡机器买十核CPU。在训练的时候，只要数据生成器（DataLoader）的产出速度比 GPU 的消\\n\\n耗速度快，那么 CPU 就不会成为瓶颈，也就不会拖慢训练速度。\\n\\n\\n-----\\n\\n去很长的一段时间了里，英特尔一直是绝对霸主的存在，代表最先进的生产力，时至今日，AMD在产品性能\\n\\n层面已经完全可以和Intel正面硬刚了。\\n\\n[CPU性能天梯图：https://www.mydrivers.com/zhuanti/tianti/cpu/index.html](https://www.mydrivers.com/zhuanti/tianti/cpu/index.html)\\n\\n**Intel 系列命名规范**\\n\\n可以通过CPU名称得到一些信息，如i7-10700K，代表产品型号是i7，后面的10代表是第10代，然后700\\n\\n代表性能等级高低，K代表这个CPU可以超频，当然后缀字母还有T、X、F等，X后缀代表高性能处理器，而T\\n\\n代表超低电压，/F代表无CPU无内置显卡版本。\\n\\n1. 系列：由低到高 Celeron（赛扬） / Pentium（奔腾） /酷睿系列的i3 / i5 / i7 / i9\\n\\n2. 世代：第1组数字代表是第几代\\n\\n例如这三个CPU：I7-8700、I7-9700，i7-10700第1个是第八代，第2个是第九代、第3个是第十代，还\\n\\n是比较容易理解的。\\n\\n3. 性能：第2组(3个数)是表示性能等级\\n\\n例如：I5-12400、I5-12500，数字越大表示越好。\\n\\n4. 后缀：K→可超频，F→没有核显\\n\\n可超频K版CPU要搭配可超频的Z系列主板才行，才可以超频，搭载不能超频的主板也可以用，只是不能\\n\\n超频了而已。\\n\\n没有核显的F版CPU要搭配独立显卡才能开机点亮屏幕\\n\\n超频简单理解就是可以提升处理器性能，核显就是把一张入门级的显卡塞进处理器里。\\n\\n**i3是家用级别，i5是游戏级别，i7是生产力和游戏发烧友级别，i9是最顶级的。后缀带K可以超频，带F**\\n\\n**表示没有核显。**\\n\\n**AMD系列命名规范**\\n\\n和Intel类似：\\n\\n1. 系列：由低到高 APU / Althlon（速龙） / Ryzen（锐龙）系列 R3 / R5 / R7 / R9\\n\\n2. 世代：第1个数字代表第几代\\n\\n3. 例如这两个CPU：R7-2700X、R7-3700X，第1个是第二代，第2个是第三代。\\n\\n4. 性能：第2组数字（3个数字）表示性能等级\\n\\n数字越大性能越好，例如 R7 3800X的性能大于R7 3700X。\\n\\n5 后缀：字母G表示有核显 字母X没有明确意思 一般性能强一点 如R5 3600X比R5 3600性能高一\\n\\n\\n-----\\n\\n**要选Intel还是AMD，其实都可以。如果追求性价比，AMD性价比高一些，如果主要玩游戏，且对价格**\\n\\n不敏感，建议选择英特尔Intel，英特尔Intel一般主频较高，一些游戏主要依赖主频，所以高主频的Intel玩游\\n\\n戏更推荐一些。除了品牌维度的分析，目前主流的大模型训练硬件通常采用 Intel + NVIDIA GPU。但具体\\n\\n情况具体分析，只能简单说：一分钱一分货，一般来说贵的好。\\n\\n**选购CPU误区**\\n\\n电子产品有一个说法是，“买新不买旧”，一般新产品，会用更新的工艺架构，性能更强，功耗更低，比\\n\\n较值得购买。当然有些时候，老一代的性价比很高，也可以考虑，但如果是好几代前的老产品，就不要考虑\\n\\n了，有些商家会卖几年前的i7电脑主机，它的性能可能还不如最新的i3，主要是忽悠小白的，要注意辨别。\\n\\n目前消费级市场，我们最常听到的i3 i5 i7 等，是英特尔的酷睿系列产品，主要面向一般消费市场，数字\\n\\n大的性能更强，（注意这里只在同代产品中成立）。AMD与之对应的是R3 R5 R7。这里值得注意是，同代产\\n\\n品i7比i5强，如果拿老一代的i7和新一代的i5比，就未必成立，部分商家经常会营销i5免费升级i7，其实是把\\n\\n最新一代的i5换成立了老一代的i7，性能方面可能还不如没升级呢？比如i5-8400的性能就高于i7-7700.\\n\\n## 3.3 散热选型策略\\n\\nCPU 不断地更新换代和性能提升，其功耗和发热量也越来越大，如果温度过高，就会出现自动关机或者\\n\\n是蓝屏死机等情况，所以需要单独的散热器来压制，目前CPU散热器分两种：水冷和风冷。\\n\\n风冷和水冷系统都是用于GPU的散热解决方案。它们各有优势和不足：通常，水冷系统在散热效率方面\\n\\n**优于风冷系统。以Intel的i9-13900KF为例，这款CPU性能目前位于CPU性能天梯榜第二位，很多用户认为使**\\n\\n用水冷系统是必要的。但如果这款CPU没有超频需求，使用高质量的风冷系统其实也能够有效地散热。只有\\n\\n在超频的情况下，水冷系统由于其更出色的冷却效果，才成为更佳的选择。\\n\\n但需要注意，风冷和水冷与GPU无关。在计算机硬件中，CPU和GPU（显卡）的散热策略和要求各有不\\n\\n同。CPU通常需要配单独的散热器，我们可以根据需要选择并购买不同类型的散热器，例如水冷或风冷系\\n\\n统，并且可以根据性能要求进行升级。由于CPU的高主频和较少的核心数（通常是几个到二十几个核心），\\n\\n高性能的CPU在运行时会产生较多热量，因此需要更有效的散热解决方案来维持合适的运行温度。与此相\\n\\n对，显卡通常采用一体化的散热设计，其散热器是显卡的重要组成部分。显卡散热器的设计已经由各厂商经\\n\\n过测试和优化，因此用户一般不需要担心显卡的散热问题。显卡采用的是多核心、低频率的策略，即使是高\\n\\n端显卡如Nvidia的4090，其频率也相对较低，通常在3000MHz左右，而同代的高端CPU（如Intel i9）的频\\n\\n率可能是其两倍。显卡的散热器可以直接接触GPU核心和显存，从而高效散热。因此，在正常满载情况下，\\n\\n显卡的温度达到70多或80多度是正常现象，通常不会成为性能瓶颈。\\n\\n对于大模型部署来说，首要原则还是CPU的等级要和GPU相匹配。对于中低端处理器，如Intel的i5系\\n\\n列，以及AMD的R5和R7系列，一般推荐使用风冷系统。这些处理器的热设计功耗（TDP）通常较低，风冷系\\n\\n统足以提供有效的散热。而对于更高性能的处理器，如Intel的i7 13700KF及更高级别的i7和i9系列，建议至\\n\\n少使用240mm规格的水冷系统。考虑到这些处理器较高的性能和热输出，水冷系统能提供更为高效和稳定\\n\\n的冷却效果。因此，尽管风冷在某些情况下依然可行，但为了确保最佳性能和稳定性，对于高性能处理器，\\n\\n\\n-----\\n\\n在构建大模型的系统时，低端主板通常不适用。根据所选的CPU和GPU规格，应从中端或高端主板中选\\n\\n择出合适的。\\n\\n|制造 商|系列|定 位|支持CPU超 频|支持内存超 频|适用用户|\\n|---|---|---|---|---|---|\\n|Intel|Z系 列|高 端|是|是|追求高性能和定制化设置的用户|\\n|Intel|B系 列|中 端|否|是|需要一定性能但预算有限的用户|\\n|Intel|H系 列|低 端|否|否|预算有限或对性能要求不高的基本用 途|\\n|AMD|X系 列|高 端|是|是|追求最高性能和高度定制化的用户|\\n|AMD|B系 列|中 端|否|是|寻求性价比的用户|\\n|AMD|A系 列|低 端|否|否|有限预算或基本计算需求的用户|\\n\\n\\n\\n选择主板时，核心因素是确保它与CPU的性能和超频能力相匹配。以Intel处理器为例：对于中高端CPU\\n\\n（如i5系列及以上），更适合选择B660到Z690系列的主板。对于如13600KF这样的高性能CPU，至少应选择\\n\\nB660系列的主板作为起点。需要考虑的是CPU是否支持超频（如带有“K”后缀）。可超频的CPU更适合搭配\\n\\n支持超频的高端主板，如Z系列\\n\\n其次，需要检查CPU和主板型号是否匹配及合理。\\n\\n通常情况下，每一种型号的CPU都需要搭配对应型号的主板，每代CPU和主板都有自己的针角及接口类\\n\\n型，Intel cpu不能用于AMD系列主板，某些主板可能会通用几代cpu，但有的主板只能兼容某一代，例如\\n\\nintel 十代 的i510400f，不能用于早期的四代 b85系列主板，而是否匹配，指的是高性能CPU搭配低性能主\\n\\n板，h610是入门主板，虽然可以点亮，但是低端主板因为供电有限，无法发挥出cpu的全部性能，以及无法\\n\\n超频，这样就失去了cpu本身的性能和意义。\\n\\n最后，考虑PCIe通道。\\n\\nPCIe通道是一种高速接口，用于将GPU连接到计算机的主板。通过这些通道，GPU可以与CPU以及系统\\n\\n内存快速交换数据。每个PCIe通道（或称为“通道”）都提供一定的数据传输带宽。更多的通道意味着更高的\\n\\n总体带宽。例如，PCIe 3.0 x16接口意味着有16个通道，每个通道的速度是PCIe 3.0标准的速度。\\n\\nGPU的性能部分取决于它与主板之间的通信速度，这是由PCIe通道的数量和版本（如PCIe 3.0、4.0或\\n\\n5.0）决定的。更高版本的PCIe提供更高的传输速率，从而可能提高GPU的性能。以下是需要考虑的几个关键\\n\\n点：\\n\\n1. PCIe版本：\\n\\n\\n-----\\n\\n4.0和5.0）提供更高的数据传输速率，这对于高性能GPU和其他高速设备非常重要。\\n\\n2. PCIe槽数量和布局：\\n\\n主板上的PCIe槽数量决定了可以安装多少个扩展卡。如果计划安装多个GPU或其他PCIe设备，需\\n\\n要确保主板有足够的槽位。\\n\\n槽位布局也很重要，尤其是在安装大型GPU时，需要确保它们之间有足够的空间，避免过热或物\\n\\n理干扰。\\n\\n3. PCIe通道分配：\\n\\n主板上的PCIe通道是从CPU和芯片组分配的。不同的主板可能有不同的通道分配方式，这可能会\\n\\n影响到扩展卡的性能，特别是在多GPU配置中。\\n\\n确认主板是否支持您所需的PCIe配置，例如双向或四向GPU设置。\\n\\n4. 与GPU的兼容性：\\n\\n虽然大多数现代GPU兼容大多数主板的PCIe槽，但是为了最佳性能，最好确认GPU与主板的PCIe\\n\\n版本相匹配。\\n\\n综上所述，因为需要通过PCIe通道连接和使用GPU，因此在选择主板时考虑PCIe通道的版本、数量、布\\n\\n局和通道分配非常重要。\\n\\n## 3.5 硬盘选型策略\\n\\n**首先考虑接口类型。主流固态硬盘主要有两种接口：SATA和M.2。**\\n\\n**SATA接口的固态硬盘体积较大，形状类似于传统的机械硬盘，主要用于升级老式电脑，因为这些电脑**\\n\\n通常不具备M.2接口。SATA接口硬盘的最高速度为600MB/s。\\n\\n**M.2接口的硬盘则较小，可以直接安装在主板上的专用接口。它们采用新的硬盘协议，速度上可以达到**\\n\\n4GB/s。\\n\\n**推荐选择 M.2接口的硬盘。**\\n\\n**然后考虑协议。M.2接口的固态硬盘分为SATA协议和NVMe协议两种。**\\n\\nM.2接口的SATA协议硬盘速度较慢，实际上就是标准SATA硬盘的形状变化，速度仍然是最高\\n\\n600MB/s，这类硬盘多用于旧电脑。\\n\\n**NVMe协议硬盘则速度更快，适合对速度有较高要求的应用。**\\n\\n在面对大量小文件的时候，使用 NVMe 硬盘可以一分钟扫完 1000万文件，如果使用普通硬盘，那么就\\n\\n需要一天时间。推荐选择 NVME协议的M.2接口的硬盘。\\n\\n**最后考虑 PCIe等级。当前市面上最新的是PCIe 5.0，但更常见的是PCIe 3.0和PCIe 4.0。PCIe等级越**\\n\\n高，硬盘的速度潜力越大。但重要的是检查主板是否支持相应的PCIe等级。例如，一些主板可能最高只支持\\n\\n到PCIe 4.0。一般来说，选择PCIe 4.0的即可。\\n\\n硬盘不会限制深度学习任务的运行，但如果小看了硬盘的作用，可能会让你追、悔、莫、及。想象一\\n\\n下，如果你从硬盘中读取的数据的速度只有100MB/s，那么加载一个32张ImageNet图片构成的mini\\n\\n-----\\n\\n**建议内存容量应大于GPU的显存。例如，对于搭载单卡GPU的系统，建议配置至少16GB内存。如果是**\\n\\n四卡GPU系统，则建议至少配置64GB内存。由于数据生成器（DataLoader）的存在，数据不需要全部加载\\n\\n到内存中，因此内存通常不会成为性能瓶颈。\\n\\n内存不用太纠结，是GPU显存的一到两倍。目前，128G 就可以，64G 也凑合。而且内存没那么贵，可\\n\\n以配满。\\n\\n内存大小不会影响深度学习性能，但是它可能会影响你执行GPU代码的效率。内存容量大一点，CPU就\\n\\n可以不通过磁盘，直接和GPU交换数据。所以应该配备与GPU显存匹配的内存容量。\\n\\n在选择的时候，注意检查主板是否支持内存的数量及型号。目前常见的 ddr3 ~ 5，每一代内存都需要对\\n\\n应主板的插槽类，ddr 5代内存 是无法混插在 ddr 4代内存上的。另外需要确定主板的内存插槽数量，如果只\\n\\n有两个插槽，买了四个，那么根本插不进去。\\n\\n其次检查cpu主板是否支持内存频率。内存条的频率上限 受到 cpu 和主板的控制， 例如 i5的 10400f +\\n\\nb460主板 = 2666，如果你买的内存是 3600频率的，无疑发挥不出内存本身的优势。\\n\\n## 3.7 电源选型策略\\n\\n在选择电脑电源时，需要检查电源的瓦数是否足以支持整机的功耗。并非越高瓦数越好，但瓦数过低可\\n\\n能会导致关机或黑屏等问题。在确定合适的电源瓦数之前，应综合评估整机硬件的功耗，尤其要考虑CPU和\\n\\n显卡这两个功耗大户。通常，将CPU和显卡的TDP功耗相加后乘以2可以得到一个合适的电源瓦数估计。例\\n\\n如，对于一个65W的CPU和125W的显卡，合适的电源瓦数应该在400W或450W左右。\\n\\n双卡最好1000W以上，四卡最好买1600W的电源\\n\\n## 3.8 机箱选型策略\\n\\n最后，选择完所有上述所有配件之后，选择机箱就相对简单，只需要确保这个机箱足够宽敞，能够容纳\\n\\n所选的所有配件。我们需要检查以下几项内容：\\n\\n1. 核对主板与机箱尺寸匹配性：\\n\\n确保所选主板的大小与机箱兼容。例如，ITX主板应与ITX机箱相匹配。这就像选择合适大小的鞋子\\n\\n一样重要。\\n\\n2. 确认机箱支持显卡尺寸：\\n\\n对比显卡的长度与机箱对显卡长度的限制。建议选择机箱的显卡限长至少比显卡长度长出30毫米\\n\\n以上，以确保有足够空间进行安装和通风。\\n\\n3. 检查散热器与机箱的兼容性：\\n\\n非常重要的一点是比较散热器的尺寸与机箱的散热限高。如果散热器太高，可能无法正常安装侧\\n\\n盖。\\n\\n考虑到许多内存条配备较高的散热马甲，需要确认散热风扇是否会受到内存条的干扰。\\n\\n如果选择水冷散热系统，确保机箱的水冷位能够容纳水冷冷排的尺寸。例如，360mm的水冷冷排\\n\\n无法安装在仅适用于240mm的位置上。\\n\\n\\n-----\\n\\n常见的电源类型包括SFX、ATX和TFX。由于不同规格的电源在形状和大小上有所不同，必须确认\\n\\n机箱的电源仓是否适合所选电源的尺寸。\\n\\n\\n-----\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c06f1d-1632-4e08-ae9b-0bb1509f134c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们定义一个用于从Markdown格式文本中提取目录（Table of Contents，TOC）的Python函数。它使用正则表达式来识别Markdown中的标题并将其分类。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb15ba8-fbab-468f-a2d1-b8a62fc31fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_toc(markdown):\n",
    "    # 初始化一个列表用于存储目录项，每项为标题级别和标题文本的元组\n",
    "    toc = []\n",
    "    \n",
    "    # 正则表达式模式，用于匹配Markdown标题，这包括'#'字符和后面的文本\n",
    "    # '^' 表示行的开始\n",
    "    # '(#+)' 捕获一个或多个'#'字符\n",
    "    # '\\s+' 匹配一个或多个空格\n",
    "    # '(.*)' 捕获标题文本，直到行尾\n",
    "    pattern = r'^(#+)\\s+(.*)$'\n",
    "    \n",
    "    # 将输入的Markdown文本按行分割\n",
    "    lines = markdown.split('\\n')\n",
    "    \n",
    "    # 遍历每一行\n",
    "    for line in lines:\n",
    "        # 使用正则表达式匹配当前行\n",
    "        match = re.match(pattern, line)\n",
    "        \n",
    "        # 如果当前行匹配Markdown标题格式\n",
    "        if match:\n",
    "            # 计算标题的级别，基于'#'的数量减去1（即，'#'对应0级，'##'对应1级，以此类推）\n",
    "            level = len(match.group(1)) - 1\n",
    "            \n",
    "            # 获取标题文本\n",
    "            title = match.group(2)\n",
    "            \n",
    "            # 将当前标题的级别和文本作为元组添加到目录列表中\n",
    "            toc.append((level, title))\n",
    "    \n",
    "    # 返回整理好的目录列表\n",
    "    return toc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd1018-82d1-4496-ab5b-02ab28edac87",
   "metadata": {},
   "source": [
    "&emsp;&emsp;执行调用测试函数功能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da88e17a-09ff-466b-8e64-ef26d648ecef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '本地部署开源大模型'),\n",
       " (1, 'Ch.1 如何选择合适的硬件配置'),\n",
       " (0, '一、大模型应用需求分析'),\n",
       " (0, '二、硬件配置的选择标准'),\n",
       " (1, '2.1 选择满足显存需求的 GPU'),\n",
       " (1, '2.2 主流显卡性能分析'),\n",
       " (1, '2.3 单卡4090 vs A100系列'),\n",
       " (1, '2.4 单卡4090 vs 双卡3090'),\n",
       " (1, '2.5 风扇卡与涡轮卡如何选择'),\n",
       " (1, '2.6 整机参考配置'),\n",
       " (1, '2.7 显卡博弈的形式分析'),\n",
       " (1, '2.8 国产AI超算芯片期待'),\n",
       " (0, '三、组装计算机硬件选型策略'),\n",
       " (1, '3.1 GPU选型策略'),\n",
       " (1, '3.2 CPU选型策略'),\n",
       " (1, '3.3 散热选型策略'),\n",
       " (1, '3.5 硬盘选型策略'),\n",
       " (1, '3.7 电源选型策略'),\n",
       " (1, '3.8 机箱选型策略')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc=extract_toc(doc)\n",
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ce163d5-9c44-4b23-abe6-6b3b8edaf7bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本地部署开源大模型\n",
      " Ch.1 如何选择合适的硬件配置\n",
      "一、大模型应用需求分析\n",
      "二、硬件配置的选择标准\n",
      " 2.1 选择满足显存需求的 GPU\n",
      " 2.2 主流显卡性能分析\n",
      " 2.3 单卡4090 vs A100系列\n",
      " 2.4 单卡4090 vs 双卡3090\n",
      " 2.5 风扇卡与涡轮卡如何选择\n",
      " 2.6 整机参考配置\n",
      " 2.7 显卡博弈的形式分析\n",
      " 2.8 国产AI超算芯片期待\n",
      "三、组装计算机硬件选型策略\n",
      " 3.1 GPU选型策略\n",
      " 3.2 CPU选型策略\n",
      " 3.3 散热选型策略\n",
      " 3.5 硬盘选型策略\n",
      " 3.7 电源选型策略\n",
      " 3.8 机箱选型策略\n"
     ]
    }
   ],
   "source": [
    "toc = extract_toc(doc)\n",
    "\n",
    "for level, title in toc:\n",
    "    print(' ' * level + title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13057b6d-6249-4f2a-920c-c0e6301424de",
   "metadata": {},
   "source": [
    "&emsp;&emsp;进一步，从Markdown格式的文本中根据一级标题提取并组织各个部分的内容。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d108abef-ee7c-4301-bd93-bc8aa1b141a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_by_sections(markdown):\n",
    "    # 初始化一个字典来存储每个一级标题及其对应的内容\n",
    "    sections = {}\n",
    "    # 用于追踪当前处理的一级标题\n",
    "    current_section = None\n",
    "    # 内容累积器，用于收集当前一级标题下的所有行\n",
    "    content_accumulator = []\n",
    "    \n",
    "    # 将输入的Markdown文本按行分割\n",
    "    lines = markdown.split('\\n')\n",
    "    # 正则表达式模式，用于匹配Markdown标题\n",
    "    pattern = r'^(#+)\\s+(.*)$'\n",
    "    \n",
    "    # 遍历每一行\n",
    "    for line in lines:\n",
    "        # 使用正则表达式匹配当前行\n",
    "        match = re.match(pattern, line)\n",
    "        if match:\n",
    "            # 计算标题的级别，基于'#'的数量减去1\n",
    "            level = len(match.group(1)) - 1\n",
    "            # 获取标题文本\n",
    "            title = match.group(2)\n",
    "            # 如果是一级标题\n",
    "            if level == 0:\n",
    "                # 如果不是处理的第一个一级标题，存储之前标题下的所有累积内容\n",
    "                if current_section is not None:\n",
    "                    sections[current_section] = \"\\n\".join(content_accumulator).strip()\n",
    "                # 更新当前处理的一级标题并重置内容累积器\n",
    "                current_section = title\n",
    "                content_accumulator = []\n",
    "            elif current_section is not None:\n",
    "                # 如果是子标题或内容，并且已经有一级标题被处理，继续累积内容\n",
    "                content_accumulator.append(line)\n",
    "        elif current_section is not None:\n",
    "            # 如果当前行不是标题，但已经存在一级标题，继续累积内容\n",
    "            content_accumulator.append(line)\n",
    "    \n",
    "    # 处理完成所有行后，确保最后一个一级标题下的内容也被存储\n",
    "    if current_section is not None:\n",
    "        sections[current_section] = \"\\n\".join(content_accumulator).strip()\n",
    "    \n",
    "    # 返回存储了各个一级标题和对应内容的字典\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715d3e6-d941-4063-bb55-ac5d32474c48",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这个函数通过Markdown中的标题级别来组织内容，仅收集一级标题下的内容。它忽略不属于任何一级标题下的内容，并将每个一级标题下的内容组织为连续的文本块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd7d4f61-588b-43c5-b674-260b9545021a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section: 本地部署开源大模型\n",
      "## Ch.1 如何选择合适的硬件配置\n",
      "\n",
      "为了在本地有效部署和使用开源大模型，深入理解硬件与软件的需求至关重要。在硬件需求方面，关键\n",
      "\n",
      "是配置一台或多台高性能的个人计算机系统或租用配备了先进GPU的在线服务器，确保有足够的内存和存储\n",
      "\n",
      "空间来处理大数据和复杂模型。至于软件需求，推荐使用Ubuntu操作系统，因其在机器学习领域的支持和\n",
      "\n",
      "兼容性优于Windows。编程语言建议以Python为主，结合TensorFlow或PyTorch等流行机器学习框架，并\n",
      "\n",
      "利用DeepSpeed等优化工具来提升大模型的运行效率和性能。\n",
      "\n",
      "所以在本系列课程中，我们将从硬件选择入手，逐步引导大家理解并掌握如何为大模型部署选择合适的\n",
      "\n",
      "硬件，以及如何高效地配置和运行这些模型，从零到一实现大模型的本地部署和应用。首先来看硬件方面，\n",
      "\n",
      "提前规划计算资源是必要的。目前，我们主要考虑以下两种途径：\n",
      "\n",
      "1. 配置个人计算机或服务器，组建一个适合大模型使用需求的计算机系统。\n",
      "\n",
      "2. 租用在线GPU服务，通过云计算平台获取大模型所需的计算能力。\n",
      "\n",
      "---\n",
      "\n",
      "Section: 一、大模型应用需求分析\n",
      "大模型的本地部署主要应用于三个方面：训练（train）、高效微调（fine-tune）和推理\n",
      "\n",
      "**（inference）。这些过程在算力消耗上有显著差异：**\n",
      "\n",
      "**训练：算力最密集，通常消耗的算力是推理过程的至少三个数量级以上。**\n",
      "\n",
      "**微调：微调是在预训练模型的基础上对其进行进一步调整以适应特定任务的过程，其算力需求低于训**\n",
      "\n",
      "练，但高于推理。\n",
      "\n",
      "**推理：推理指的是使用训练好的模型来进行预测或分析，是算力消耗最低的阶段。**\n",
      "\n",
      "总的来说，在算力消耗上，训练 > 微调 > 推理。\n",
      "\n",
      "从头训练一个大模型并非易事，这不仅对个人用户，对于许多企业而言也同样困难。因此，如果个人使\n",
      "\n",
      "用，关注点应该放在推理和微调的性能上。在这两种应用需求下，对硬件的核心要求体现在GPU的选择上，\n",
      "\n",
      "**对CPU和内存的要求并不高。无论是选择租用在线算力还是配置本地计算机，如果想在本地运行大模型，我**\n",
      "\n",
      "们可以拆分成两个关注点：\n",
      "\n",
      "模型：选择什么基座模型或微调模型，这可以直接下载至本地。\n",
      "\n",
      "硬件：希望在什么硬件平台上来执行，可以分为 CPU 和 GPU 两大类。\n",
      "\n",
      "⼤部分开源⼤模型⽀持在 CPU 和 Mac M系列芯片上运⾏，但较为繁琐且占⽤内存⾄少 32G 以上，因此\n",
      "\n",
      "更推荐在 GPU 上运⾏。针对本地部署大模型，在选择GPU时，可以遵循的简单策略是：在满足具体的大模\n",
      "\n",
      "**型的官方配置要求下，选择性价比最高的GPU。**\n",
      "\n",
      "GPU的性能主要由以下三个核心参数决定：\n",
      "\n",
      "1. 计算能力：这是最关注的指标，尤其是32位浮点计算能力。随着技术发展，16位浮点训练也日渐普\n",
      "\n",
      "及。对于仅进行预测的任务，INT 8 量化版本也足够；\n",
      "\n",
      "2. 显存大小：大模型的规模和训练批量大小直接影响对显存的需求。更大的模型或更大的批量处理需要更\n",
      "\n",
      "多的显存；\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "处理大量数据时的性能通常也越好；\n",
      "\n",
      "注：显存带宽相对固定，选择空间较小。\n",
      "\n",
      "---\n",
      "\n",
      "Section: 二、硬件配置的选择标准\n",
      "无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\n",
      "\n",
      "（如微调或推理）都需要相应的硬件配置方案来支持。所以在选择硬件配置时应根据具体的模型需求和预期\n",
      "\n",
      "**用途来确定。**\n",
      "\n",
      "因此，我们的建议是：根据部署的大模型配置需求，先选择出最合适的 GPU，然后再根据所选 GPU 的\n",
      "\n",
      "**特性，进一步搭配计算机的其他组件，如CPU、内存和存储等，以确保整体系统的协调性和高效性能。最简**\n",
      "\n",
      "**单的匹配GPU的标准是显存大小和性价比。因为训练不纯粹看一个显存容量大小，而是和芯片的算力高度相**\n",
      "\n",
      "关的。因为实际训练的过程当中，将海量的数据切块成不同的batch size，然后送入显卡进行训练。显存\n",
      "\n",
      "大，意味着一次可以送进更大的数据块。但是芯片算力如果不足，单个数据块就需要更长的等待时间显存和\n",
      "\n",
      "算力，必须要相辅相成。\n",
      "\n",
      "简单来说，在深度学习的训练和推理中，GPU的显存主要用于以下几个方面：\n",
      "\n",
      "1. 权重存储：模型的参数，包括权重和偏置，都需要在显存中存储。这些参数是模型进行预测或分类所必\n",
      "\n",
      "需的。\n",
      "\n",
      "2. 中间过程数据存储：在模型计算的前向传播和反向传播过程中，会产生并且需要暂时存储大量的中间计\n",
      "\n",
      "算结果。这些数据同样存储在显存中。\n",
      "\n",
      "3. 计算过程：GPU专门为并行处理大量的矩阵和向量运算而设计，这正是深度学习中常见的计算类型。这\n",
      "\n",
      "些计算直接在显存中进行，以利用GPU的高速运算能力。\n",
      "\n",
      "显存的大小和速度直接影响到模型的处理速度和能处理的模型大小。显存越大，意味着可以处理更大的\n",
      "\n",
      "模型和更复杂的计算任务，但同时也需要更多的能源和可能导致更高的成本。\n",
      "\n",
      "\"芯片\"通常指的是集成电路，它们被集成到各种电脑硬件组件中，如CPU、GPU和主板等。CPU本身就\n",
      "\n",
      "是一种芯片。它是计算机的大脑，负责执行程序和处理数据。显卡上的核心组件是图形处理器\n",
      "\n",
      "（GPU），它也是一种芯片。GPU负责处理图形和视频渲染。\n",
      "\n",
      "**所谓的\"算力\"大小，通常指的是整个计算系统的处理能力，尽管在特定上下文中，它有时特指GPU的处**\n",
      "\n",
      "**理能力。**\n",
      "\n",
      "我们以ChatGLM-6B模型为例，官方给出的硬件配置说明如下：\n",
      "\n",
      "模型量化是一种用于优化模型的技术，特别是在推理时。它通过减少模型中使用的数值精度来减小模\n",
      "\n",
      "型的大小，加快推理速度，并降低内存和能源消耗。模型量化常用于将模型部署到资源受限的设备\n",
      "\n",
      "上，如手机或嵌入式系统，量化程度越高，对硬件的要求就会越低。 单精度通常指的是32位浮点数\n",
      "\n",
      "（FP32），使用32位表示，包括1位符号位、8位指数位和23位尾数位。FP32是标准的训练和推理格\n",
      "\n",
      "式，但由于半精度（FP16）提供了相似的结果且计算速度更快，更节省内存，因此在资源受限或需要\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "少，它的计算量就会越小，对应的输出结果的精度也就会越差。\n",
      "\n",
      "## 2.1 选择满足显存需求的 GPU\n",
      "\n",
      "关于如何选择GPU，当前市场 NVIDIA 和 AMD 是两大主要显卡生产商。但在人工智能、大数据、深度\n",
      "\n",
      "学习领域，NVIDIA（通常被称为N卡）几乎独占鳌头。主要原因还是NVIDIA在很早期就开始专注于AI和深度\n",
      "\n",
      "学习市场，开发了强大的软件工具和库，例如cuDNN、TensorRT，这些都是专门为深度学习优化的，与流\n",
      "\n",
      "行的深度学习框架（如TensorFlow、PyTorch等）紧密集成，同时NVIDIA的CUDA（Compute Unified\n",
      "\n",
      "Device Architecture）作为独特的平行计算平台和编程模型，它允许开发者利用NVIDIA的GPU进行高效的通\n",
      "\n",
      "用计算。这一点对于深度学习和大数据分析等需要大量并行处理的应用来说至关重要。\n",
      "\n",
      "**英伟达是一家什么公司？**\n",
      "\n",
      "这时候可能有小伙伴说了，英伟达是一家卖游戏显卡的，这个说法呢，对，但也不对，从财报来看，英\n",
      "\n",
      "伟达目前主要有四块业务，分别是游戏GPU，数据中心产品，自动驾驶芯片和其他业务。占比分别为\n",
      "\n",
      "33.6%，55.%，3.3%和7.4%。游戏GPU，数据中心产品，自动驾驶芯片其实都可以归类为计算芯片这个门\n",
      "\n",
      "类下面，换句话说，如果从财报公布的业务情况来分析的话，英伟达确实就是一家卖计算芯片的公司。但如\n",
      "\n",
      "果真的把英伟达当做一家卖芯片的公司，那就大错特错了。英伟达的确是靠着游戏显卡起家，并且在人工智\n",
      "\n",
      "能爆发的现在靠着一手AI计算芯片市值突破了万亿美元，但其实它并不是一家卖芯片的公司，我对英伟达的\n",
      "\n",
      "定位是，它是一家卖人工智能系统的公司。这就有两个核心的概念，一个是英伟达的计算芯片，一个是英伟\n",
      "\n",
      "达针对自家芯片做的计算架构CUDA，二者缺一不可。这种定位就像智能手机时代的苹果公司，苹果依靠着A\n",
      "\n",
      "系列芯片和ios操作系统收割了智能手机行业超过80%的利润。人工智能大发展的时代，英伟达就依靠着GPU\n",
      "\n",
      "和计算芯片与CUDA计算架构，共同组成的AI生态系统赢得了市场青睐，根据相关机构的统计数据，在独立\n",
      "\n",
      "显卡领域，英伟达的市占率高达85%，在AI算力芯片领域，在未来可能达到90%，现在做深度学习，英伟达\n",
      "\n",
      "的卡就是刚需，没有其他的选择。\n",
      "\n",
      "因此，我们建议还是选择 NVIDIA 的显卡。如果对应的ChatGLM-6B模型的硬件配置说明，我们就可以\n",
      "\n",
      "这样选择GPU。理论上，在进行少量对话时:\n",
      "\n",
      "在选择显卡时，必须遵循的首要准则是：显卡的显存容量一定要高于大模型官方要求的最低显存配置。\n",
      "\n",
      "这是确保模型能够有效运行的基本要求。显存容量越大，其推理或微调的能力就会越强。当然，随着显存容\n",
      "\n",
      "量的增加，显卡的价格也相应提高。以下是目前最主流的几款大模型的显卡型号及其显存容量：\n",
      "\n",
      "|显卡型号|显存容量|\n",
      "|---|---|\n",
      "|H100|80 GB|\n",
      "|A100|80/40 GB|\n",
      "|H800|80 GB|\n",
      "\n",
      "\n",
      "\n",
      "A800 80 GB\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "|显卡型号|显存容量|\n",
      "|---|---|\n",
      "|4090|24 GB|\n",
      "|3090|24 GB|\n",
      "\n",
      "\n",
      "其组合形式可以分为以下四类：\n",
      "\n",
      "1. 纯CPU：基于不同架构的CPU配置，适用于不需要或不能使用GPU加速的场景。（不推荐）\n",
      "\n",
      "x86 (如Intel或AMD)\n",
      "\n",
      "ARM (如Apple、Qualcomm、MTK)\n",
      "\n",
      "2. 单机单卡：使用一块GPU进行计算，适用于大多数个人使用和一些中等计算负载的场景。（典型配置）\n",
      "\n",
      "Nvidia系列GPU\n",
      "\n",
      "AMD系列GPU\n",
      "\n",
      "Apple系列GPU\n",
      "\n",
      "Apple Neural Engine（较少见，支持有限）\n",
      "\n",
      "3. 单机多卡：在一台机器上使用多张GPU卡，适用于高计算负载的场景，如模型分割处理。（典型配置）\n",
      "\n",
      "4. 多机配置：使用多台计算机进行集群计算，通常超出个人使用范围，主要用于从头预训练基座模型等高\n",
      "\n",
      "负载任务。\n",
      "\n",
      "所以，在单个显卡的显存容量不足以满足需求时，也可以采用多显卡配置来增加整体的显存容量。只要\n",
      "\n",
      "总显存超过官方推荐的配置要求就可以。此外，在选择显卡时，除了考虑整体显存容量，还要根据不同显卡\n",
      "\n",
      "的性能和成本进行权衡。根据具体需求和预算，决定是选择单张高性能显卡，还是部署多张成本效益更高的\n",
      "\n",
      "低版本显卡。实现最优的性价比。比如在理论上，在进行多轮对话时或需要微调时，采用单机多卡：\n",
      "\n",
      "## 2.2 主流显卡性能分析\n",
      "\n",
      "对于 NVIDIA的显卡（N卡）卡来说，我们可以按照以下几个维度来划分：\n",
      "\n",
      "按照产品线划分：\n",
      "\n",
      "|系列|特点|主要应用领域|\n",
      "|---|---|---|\n",
      "\n",
      "\n",
      "GeForce\n",
      "\n",
      "系列（G\n",
      "\n",
      "\n",
      "消费级GPU产品线，注重提供高性能的图形处理能力和游戏\n",
      "\n",
      "特性 性价比高 适合游戏和深度学习推理 训练\n",
      "\n",
      "\n",
      "主要面向游戏玩家和普\n",
      "\n",
      "通用户\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "|系列|特点|主要应用领域|\n",
      "|---|---|---|\n",
      "|Quadro 系列（P 系列）|专业级GPU产品线，针对商业和专业应用领域进行了优化， 适用于设计、建筑等专业图像处理。|设计、建筑、专业图像 处理等领域。|\n",
      "|Tesla系 列（T系 列）|主要用于高性能计算和机器学习任务，集成深度学习加速 器，提供快速的矩阵运算和神经网络推理。|高性能计算、机器学习 任务等领域。|\n",
      "|Tegra系 列|移动处理器产品线，用于嵌入式系统、智能手机、平板电 脑、汽车电子等领域，具备高性能的图形和计算能力，低功 耗。|嵌入式系统、智能手 机、平板电脑、汽车电 子等领域。|\n",
      "|Jetson系 列|面向边缘计算和人工智能应用的嵌入式开发平台，具备强大 的计算和推理能力，适用于智能摄像头、机器人、自动驾驶 系统等。|边缘计算、人工智能、 机器人等领域。|\n",
      "|DGX系列|面向深度学习和人工智能研究的高性能计算服务器，集成多 个GPU和专用硬件，支持大规模深度学习模型的训练和推 理。|深度学习、人工智能研 究和开发等领域。|\n",
      "\n",
      "\n",
      "按照架构划分：\n",
      "\n",
      "|架构|年份|芯 片 代 号|特点|代表产品|\n",
      "|---|---|---|---|---|\n",
      "|Tesla|2006|GT|第一个通用并行计算架构，主要用于科学计算和 高性能计算。|Tesla C870/GeForce 8800 GTX|\n",
      "|Fermi|2010|GF|引入CUDA架构、ECC内存等，用于科学计算、图 形处理和高性能计算。|Tesla C2050/GeForce GTX 480|\n",
      "|Kepler|2012|GK|功耗效率和性能改进，引入GPU Boost技术，适 用于科学计算、深度学习和游戏。|Tesla K40/GeForce GTX 680|\n",
      "|Maxwell|2014|GM|提高功耗效率，引入新技术如多层次内存系统， 应用于游戏、深度学习和移动设备。|Tesla M40/GeForce GTX 980|\n",
      "|Pascal|2016|GP|16nm FinFET制程技术，加强深度学习和AI计算 支持，引入Tensor Cores，应用于深度学习和高 性能计算。|Tesla P100/GeForce GTX 1080|\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "**架构** **年份**\n",
      "\n",
      "|Col1|Col2|号|Col4|Col5|\n",
      "|---|---|---|---|---|\n",
      "|Volta|2017|GV|深度学习优化特性，如Tensor Cores，主要用于 深度学习、科学计算和高性能计算。|Tesla V100|\n",
      "|Turing|2018|TU|实时光线追踪技术、深度学习技术，适用于游 戏、深度学习和专业可视化。|Tesla T4/GeForce RTX 2080 Ti|\n",
      "|Ampere|2020|GA|第二代深度学习架构，更多Tensor Cores、改进 的Ray Tracing技术，应用于深度学习、科学计算 和高性能计算。|Telsa A100/GeForce RTX 3090|\n",
      "|Ada Lovelace|2022|AD|专为光线追踪和基于AI的神经图形设计，第四代 Tensor Core，第三代RT Core，提高GPU性能。|GeForce RTX 4090|\n",
      "|Hopper|2022|GH|下一代加速计算平台，支持PCIe 5.0，专用的 Transformer引擎，适用于大型语言模型和对话 AI，提供企业级AI支持。|Telsa H100|\n",
      "\n",
      "\n",
      "\n",
      "按照应用领域划分：\n",
      "\n",
      "\n",
      "**特点** **代表产品**\n",
      "\n",
      "|类型|系列|描述|应用领域|代表产品|\n",
      "|---|---|---|---|---|\n",
      "|游戏娱 乐|GeForce RTX™系 列|面向大众消费级游戏和创作者用户的图形 加速卡。在性能、功耗和成本之间达到最 佳平衡点,提供极致的游戏和创作体验。|游戏、娱乐、 内容创作|如RTX 3090、RTX 4090等|\n",
      "|专业设 计和虚 拟化|NVIDIA RTX™系 列|面向专业可视化和创意工作负载的高性能 GPU,提供强大的计算性能、大容量视频 内存等。服务于工业设计、建筑设计、影 视特效渲染等专业用户。|工业设计、建 筑设计、影视 特效渲染|高端专业可视 化工作站级显 卡|\n",
      "|深度学 习、人 工智能 和高性 能计算|A系列、 H系列、 L系列、 V系列、 T系列|不同系列针对不同AI和计算需求。A系列 是AI计算加速器; H系列是AI超算; L系列 是边缘AI推理; V系列是虚拟工作站; T系 列是AI推理优化解决方案。|数据中心AI训 练和推理、边 缘AI、虚拟桌 面、AI推理加 速|A100、 A30、A40、 H100、 L40、 DeepStream 加速器等|\n",
      "\n",
      "\n",
      "像大模型领域这种生成式人工智能，需要强大的算力来生成文本、图像、视频等内容。在这个背景下，\n",
      "\n",
      "NVIDIA先后推出V100、A100和H100等多款用于AI训练的芯片，其中 A100 是 H100 的上一代产品，于2020\n",
      "\n",
      "年发布，使用7纳米工艺，支持AI推理和训练。而H100，该显卡是2022年3月发布，可谓是核弹级性能显\n",
      "\n",
      "卡，采用了台机电4纳米工艺，具备800亿个晶体管，采用最新 Neda Hopper架构，同时显存还支持\n",
      "\n",
      "hbm3，最高带宽可达 3TB每秒。第四代MNLINK的带宽，900G每秒。是PCIE5.0的7倍，比上一代的A100显\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "飞跃，各项基础性能是A100的三倍之多，H100的单片显卡售价24万元左右。\n",
      "\n",
      "但在2022年10月，漂亮国政府为了限制我国的人工智能发展，发布禁令：禁止NVIDIA向中国出售A100\n",
      "\n",
      "和H100显卡。数据显示，2022年中国市场的人工智能芯片规模高达70亿美元，而这70亿的市场，被NVIDIA\n",
      "\n",
      "垄断了90%，虽然NVIDIA的A100，H100这样的顶级芯片不能卖给中国，但NVIDIA作为商业公司，也是要做\n",
      "\n",
      "生意的，于是为了合规，NVIDIA针对传输速率进行了限制，提出了中国大陆特供版的A800和H800，即：\n",
      "\n",
      "H100、A100的阉割版。\n",
      "\n",
      "也就是说，由于漂亮国的禁令，我们现在使用的GPU都是中国特供版的，说白了就是阉割版的，像\n",
      "\n",
      "A100，到国内就成了A800，H100到国内就成了H800，那么 A ~ H的差距在哪里呢？\n",
      "\n",
      "直接用 SXM 版本的H800进行对比，只能说这个参数对比，对于不了解的人来说，还是比较出人意料\n",
      "\n",
      "的，除了 FP64 和 NVLink传输速率上的明显削弱，其他参数和H100都是一模一样的。FP64上的削弱主要影\n",
      "\n",
      "响是H800在科学计算、流体计算、有限元分析等超算领域的应用，受到影响最大的还是NVLINK上的削减，\n",
      "\n",
      "但因为架构上的升级，虽然比不上同为 Hoper架构的H100，但是比AMPERRE架构的A100还是要强上不少，\n",
      "\n",
      "说白了，老黄想要抓住国内市场，就算是阉割，也不会阉割的特别过分，漂亮国政府想限制国内的超算，那\n",
      "\n",
      "就算把超算性能砍掉，传输速率减小，换个名字，GPU照卖。只要保证H800在大部分场景下的性能不受影\n",
      "\n",
      "响，能满足大部分人的使用需求就足够了。毕竟也不会有人跟钱过不去，所以 其实H800和 H 100的性能差\n",
      "\n",
      "距并没有想象的那么夸张，就算是砍掉了FP64和 NVLINK的传输速率，性能依旧够用。最关键的是，它合法\n",
      "\n",
      "呀。所以如果不是追求极致性能的话，也没必要冒着风险去选择H100。\n",
      "\n",
      "而就在今年的10月份，漂亮国又玩起了变卦，10月份刚升级了芯片禁令，开启了新一轮的出口管制，先\n",
      "\n",
      "预留了30天的窗口期，随后又要求立即生效，连30天都没了，也就是说，从10月份开始，中国将无法再获得\n",
      "\n",
      "NVIDIA5类的GPU显卡 （A800、A800、H100、A100，L40S），其实早在8月份的时候，BAT的一些大厂不\n",
      "\n",
      "知道是收到风声还是控制风险，就向NVIDIA提前订购了10万个A800芯片，结果这次也是彻底泡汤。其实从\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "的尖端AI芯片了，漂亮国就是亮牌，高端AI芯片，必禁无疑。所以对于目前的 A100系列和H100系列，因为\n",
      "\n",
      "是漂亮国断供之前出的芯片，所以现在国内还有货，只不过市场渠道比较乱，需要甄别。\n",
      "\n",
      "同时需要说明的是，GeForce 系列显卡虽被官方定位为面向消费级市场，适合游戏爱好者。但这类显卡\n",
      "\n",
      "在深度学习领域同样展现出了出色的性能，很多人用来做推理、训练，单张卡的性能跟深度学习专业卡Tesla\n",
      "\n",
      "系列比起来其实差不太多，但是性价比却高很多。对于大模型来说，同样可以使用GeForce 系列显卡。\n",
      "\n",
      "那么个人使用或者实验室针对大模型的推理和微调需求配置服务器，高端显卡目前我们可选的就是\n",
      "\n",
      "A100、A800、H100和4090等，应该如何选呢？\n",
      "\n",
      "## 2.3 单卡4090 vs A100系列\n",
      "\n",
      "先说结论：没有双精度需求，追求性价比，选4090。有双精度需求，选A100，没有A100选A800。如果\n",
      "\n",
      "**是做大模型的训练，GeForce RTX 4090 是不行的。但在执行推理（inference/serving）任务时，使用**\n",
      "\n",
      "**RTX 4090 不仅可行，而且在性价比方面甚至略优于 A100。同时如果做微调，也勉强是可以的，但建议多**\n",
      "\n",
      "**卡。**\n",
      "\n",
      "|GPU 型号|Tensor FP16 算力|Tensor FP32 算力|内存 容量|内存 带宽|通信 带宽|通信 时延|售价（美元）|\n",
      "|---|---|---|---|---|---|---|---|\n",
      "|H100|989 Tflops|495 Tflops|80 GB|3.35 TB/s|900 GB/s|~1 us|30000~40000|\n",
      "|A100|312 Tflops|156 Tflops|80 GB|2 TB/s|900 GB/s|~1 us|15000|\n",
      "|4090|330 Tflops|83 Tflops|24 GB|1 TB/s|64 GB/s|~10 us|1600|\n",
      "\n",
      "\n",
      "\n",
      "**推理**\n",
      "\n",
      "从数据对比来看，A100 和 GeForce RTX 4090 显卡在通信能力和内存容量方面存在显著差异，但在算力\n",
      "\n",
      "上差距并不大。在 FP16 算力方面，两者几乎相当，4090 甚至略有优势。相较于 A100，其较高的性价比主\n",
      "\n",
      "要源于推理过程通常涉及单一模型，在这种场景下，显卡的算力才是关键因素，而 4090 在这方面表现出\n",
      "\n",
      "色。虽然内存带宽同样重要，但在推理任务中，4090 的内存带宽通常足以应对需求，不会成为显著的制约\n",
      "\n",
      "因素。\n",
      "\n",
      "[LambdaLabs 有个很好的 GPU 单机训练性能和成本对比：https://lambdalabs.com/gpu-benchmarks](https://lambdalabs.com/gpu-benchmarks)\n",
      "\n",
      "， 我们来看：\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "高的。\n",
      "\n",
      "**微调**\n",
      "\n",
      "反观训练需求下，4090在训练的时候表现不佳的原因主要是其有限的通信能力和内存容量。比如训练\n",
      "\n",
      "LLaMA-2 70B 时需要2400块 A100 ，同时据说训练ChatGPT用了上万块 A100，主要还是因为训练过程除了\n",
      "\n",
      "存储模型参数外，还需要处理大量数据以及各层之间的中间数据和参数。因此，大容量内存和高通信带宽会\n",
      "\n",
      "比较关键，以便高效地处理和协调这些信息。首先就是把n个T的数据，分发到不同的GPU上去，然后训练，\n",
      "\n",
      "这叫数据并行。第二个并行就是会把这个模型的数据在一块GPU里可能放不下，所以要按照每一层，把某几\n",
      "\n",
      "层放在不同的GPU上面，进行串联。这就叫流水线并行。第三个就是Tensor张量并行。主要是我们目前训练\n",
      "\n",
      "的Transform模型都是多头的，每一个头都是可以按照张量来进行并行训练。所以整个 LLaMA-2 70B他会通\n",
      "\n",
      "过张量，流水线、数据三种并行方式，从模型内层，到模型层之间，和训练数据三个维度进行计算空间的划\n",
      "\n",
      "分。\n",
      "\n",
      "2400块GPU之间要进行大量的协调和通讯计算，这种复杂的并行结构需要 GPU 之间进行大量的协调和\n",
      "\n",
      "通信。4090 的通信带宽仅为 64 GB/s，与 A100 的 900 GB/s 相比差距过大，导致在这类大规模训练任务中\n",
      "\n",
      "通信成为瓶颈，进而影响整体性价比。因此，尽管 4090 在某些方面表现优秀，但在大模型训练中的局限性\n",
      "\n",
      "仍然明显。尽管微调过程对硬件的要求相较于训练相对较低，但这个过程仍然需要足够的内存以存储模型参\n",
      "\n",
      "数，以及有效的通信带宽来处理数据和模型层之间的交互。所以对于需要高通信带宽和大内存容量的大模型\n",
      "\n",
      "微调任务，A100等高端GPU可能是更合适的选择。\n",
      "\n",
      "我们拿 GPT 3 来说，GPT 3的参数将近700亿，假设每个参数使用4字节（通常使用float 32）进行存\n",
      "\n",
      "储，训练运算储备需求是 4200 GB，完成一次GPT 3训练的总算力是：3.15 * 10 ^23 Flops,仅考虑算力的情\n",
      "\n",
      "况下，单块 A100 需要45741天，几乎是128年（假设有效算力是78Tflpos），单块4090 需要91146天，几\n",
      "\n",
      "乎是250年，（假设有效算力是40 Tflpos）。任何一张单卡训练一次都需要超过100年，对于参数量达到10\n",
      "\n",
      "亿级别的大模型，参数本身就大，而且大模型通常需要更高的显存来来存储参数、中间计算结果和梯度等，\n",
      "\n",
      "既然要多卡运行，数据的同步效率就会显得非常重要，那么内存带宽、通信带宽、通信延时等性能将非常非\n",
      "\n",
      "常重要。，4090 24g的显存小，而且内存带块、通信贷款、和通信延时都相对较弱，这就有点像短板理论。\n",
      "\n",
      "最弱的那一项就决定了显卡的能力。综上，4090在较大的大模型没有什么发挥的余地，但随着现在的大模型\n",
      "\n",
      "越来越小，对显存和算力需求相对较小的大模型，再加上推理的算力需求更低，如LLama 7B 13B模型，单\n",
      "\n",
      "卡的4090 都可以运行，至少对于学习和研究大模型的个人或实验室来说，4090还真是不错的选择。\n",
      "\n",
      "⼤模型的⼯业级实践要求，⼤模型全量微调需要⾄少4张A100 80G显卡；（ChatGLM6B模型 全量微调\n",
      "\n",
      "差不多也是需要这个配置）\n",
      "\n",
      "## 2.4 单卡4090 vs 双卡3090\n",
      "\n",
      "如果预算差不多的情况下，对于两张3090与一张4090的选择，推荐使用两张3090显卡。虽然从算力角\n",
      "\n",
      "度看，两张3090与一张4090大致持平，但两张3090显卡提供的总显存会更多，这对于处理大型模型尤为重\n",
      "\n",
      "要。目前，大多数深度学习计算框架都支持各种并行计算技术，如流水线并行、张量并行和CPU卸载。这些\n",
      "\n",
      "技术使得即使是显存较小的显卡也能处理大型模型。在这种情况下，双3090配置可以更有效地利用流水线并\n",
      "\n",
      "行，同时，与单4090配置相比，CPU卸载的需求会降低。此外，企业环境一般都是多卡并行，使用双3090配\n",
      "\n",
      "置还可以练习如何写多卡代码。现有技术如串行反向传播，进一步增强了多卡系统的效率，使其成为一个经\n",
      "\n",
      "济高效的选择，尤其是在需要处理大量数据和复杂模型的情况下。因此，从目前的技术和应用需求来看，选\n",
      "\n",
      "择两张3090显卡无疑是更优的选择。\n",
      "\n",
      "## 2.5 风扇卡与涡轮卡如何选择\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "风扇卡与涡轮卡的供电接口位置不同，涡轮卡的供电接口位置在接口尾部，供电线比风扇卡的线\n",
      "\n",
      "更短，这样是方便安装和理线，而风扇卡供电接口一般在显卡顶部，接线后线缆会高于机箱最高\n",
      "\n",
      "面，在服务器中使用风扇卡，服务器盖板盖不上。\n",
      "\n",
      "在散热方向上面，涡轮卡散热方向是朝尾部散热，并于服务器风向是一致的，而风扇卡的散热是朝四面\n",
      "\n",
      "八方来散热的，平常的PC机箱放一张是可以适应的，但用作服务器上（很多时候是多卡）就不适合了，\n",
      "\n",
      "很容易因为温度过热出现宕机。\n",
      "\n",
      "风扇卡与涡轮卡的尺寸大小不同\n",
      "\n",
      "涡轮卡与风扇卡的尺寸大小也是不一样的，风扇卡的尺寸一般是2.5-3倍宽设计，而涡轮卡的尺寸\n",
      "\n",
      "大小是双宽设计，因为涡轮卡为了方便放入服务器里，所以涡轮卡的尺寸和高度都远远低于风扇\n",
      "\n",
      "卡，从而服务器可以支持4卡或者8卡，如果用风扇卡代替涡轮卡装在服务器里，那位置够不够还\n",
      "\n",
      "是一回事儿呢。\n",
      "\n",
      "面对市场不同\n",
      "\n",
      "风扇卡无论是公版显卡还是非公版显卡，风扇卡都是面向个人的，是应用在个人游戏行业的，\n",
      "\n",
      "4090风扇卡的特点就是外观炫酷，而个人游戏行业就是为了风扇卡的外观和玩游戏的性能。而\n",
      "\n",
      "4090涡轮卡是定制版，是面向AI科技产业，因为做工精巧、支持多卡安装、性价比高等一系列优\n",
      "\n",
      "点，4090涡轮卡深受广大AI深度学习用户的喜爱。\n",
      "\n",
      "## 2.6 整机参考配置\n",
      "\n",
      "确定GPU后，根据GPU搭配合适的计算机组件，具体来说，计算机八大件：CPU、散热器、主板、内\n",
      "\n",
      "存、硬盘、显卡、电源、风扇、机箱。个人使用的计算机，典型的配置是单GPU或双GPU，一般不超过四个\n",
      "\n",
      "GPU，否则常规的机箱放不下，且运行时噪声很大，而且容易跳闸。\n",
      "\n",
      "目前国内实验室主流的还是4090和3090,10万+的预算配置4张4090是没问题的，20~30万的预算则可以\n",
      "\n",
      "考虑8张4090，或者两张A100 80G，如果预算不限，A100 8卡服务器一定是最佳选择。\n",
      "\n",
      "这里给出一个本地部署ChatGLM-6B，同时也适用于大多数消费级实验环境的配置：\n",
      "\n",
      "GPU：3090双卡，涡轮版；总共48G显存，能够适⽤于⼤多数试验和复现性质深度学习任务；同时双卡\n",
      "\n",
      "也便于模拟多卡运⾏的⼯业级环境；\n",
      "\n",
      "CPU：AMD 5900X；12核24线程，模拟普通服务器多线程设置；\n",
      "\n",
      "存储：64G内存+2T SSD数据盘；内存主要考虑机器学习任务需求；\n",
      "\n",
      "电源：1600W单电源；双卡GPU的电源在1200W-1600W均可；\n",
      "\n",
      "主板：华硕ROG X570-E；服务器级PCE，⽀持双卡PCIE；\n",
      "\n",
      "机箱：ROG太阳神601；atx全塔式⼤机箱，便于⾼功耗下散热；\n",
      "\n",
      "A800 工作站的典型配置信\n",
      "\n",
      "|配置项|规格|\n",
      "|---|---|\n",
      "|CPU|Intel 8358P 2.6G 11.2UFI 48M 32C 240W *2|\n",
      "|内存|DDR4 3200 64G *32|\n",
      "\n",
      "\n",
      "\n",
      "数据盘 960G 2.5 SATA 6Gb R SSD *2\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "|配置项|规格|\n",
      "|---|---|\n",
      "|硬盘|3.84T 2.5-E4x4R SSD *2|\n",
      "|网络|双口10G光纤网卡（含模块）*1|\n",
      "||双口25G SFP28无模块光纤网卡（MCX512A-ADAT ）*1|\n",
      "|GPU|HV HGX A800 8-GPU 8OGB *1|\n",
      "|电源|3500W电源模块*4|\n",
      "|其他|25G SFP28多模光模块 *2|\n",
      "||单端口200G HDR HCA卡(型号:MCX653105A-HDAT) *4|\n",
      "||2GB SAS 12Gb 8口 RAID卡 *1|\n",
      "||16A电源线缆国标1.8m *4|\n",
      "||托轨 *1|\n",
      "||主板预留PCIE4.0x16接口 *4|\n",
      "||支持2个M.2 *1|\n",
      "|原厂质保|3年 *1|\n",
      "\n",
      "\n",
      "总的来说：\n",
      "\n",
      "3090⽐4090综合性价⽐更⾼，不过4090计算速度⼏乎是3090的两倍，有需求亦可考虑升级，不过\n",
      "\n",
      "4090需要的机箱空间更⼤、电源配置也要求更⾼；\n",
      "\n",
      "双卡GPU升级路线：3090—>4090—>A100 40G （2.5w左右）—>A100 80G（6~7w左右）；\n",
      "\n",
      "⼤模型的⼯业级实践要求，⼤模型全量微调需要⾄少4张A100 80G显卡；（ChatGLM6B模型 全量微调\n",
      "\n",
      "差不多也是需要这个配置）\n",
      "\n",
      "## 2.7 显卡博弈的形式分析\n",
      "\n",
      "除此之外，在2023年11月13日老黄又放出来两个核弹。第一个核弹就是它推出了全新的超算GPU\n",
      "\n",
      "H200，直接说是当世最强，听起来很嚣张，但其实一点没有吹牛。在AI超算领域，对手只有看NVIDIA车尾\n",
      "\n",
      "灯的份。从数据层面看，H200强在大模型推理上，以700亿参数的Llama2 二代大模型为例，h200推理速度\n",
      "\n",
      "几乎比前代的h100快了一倍。而且能耗还降低了一半。显存从h100的80GB，直接拉到了141gb，带宽也从\n",
      "\n",
      "3.35TB/s，提升到了4.8TB/s，最新的GPU H200，跟前一代H100相比，最大的提升就是它的内存，达到了\n",
      "\n",
      "惊人的1.15TB/s，相当于在1s内传输了 230步FHD的高清电影。如果每一部的容量按5G来算的话。这个跟我\n",
      "\n",
      "们以前的计算机里的内存条就不一样了，它采用的最新技术是HBM3e，HBM就是高带宽内存，这个实现是\n",
      "\n",
      "把DRAM内存用3D封装的技术叠了起来，然后把它和GPU芯片放在同一个GPU的底板上，它们之间的通信就\n",
      "\n",
      "通过这块晶元直接做了，这样内存的容量就大大提高了，同时他们和GPU的通信速度也有显著的增长，。达\n",
      "\n",
      "到了每秒钟4.8个TB。然后又把所有的软件做了优化，这样就使得ChatGPT这样 大模型的推理速度大大的提\n",
      "\n",
      "升，跟A100相比提高了 18倍。第二个核弹就是CPU和GPU的合体，GH200， 就是把ARM的CPU和它的GPU\n",
      "\n",
      "封装在了同一块GPU晶圆板上，这样CPU和GPU之间的传输速度就非常快，而且可以共享内存。内存也达到\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "有1/2。\n",
      "\n",
      "炸一听好像是王炸升级，刚装满h100的企业要哭晕在厕所了。但实际上，它可能只是h100的一个中期\n",
      "\n",
      "改款，单论峰值算力，H100和H200其实是一模一样的。，真正提升的是显存和带宽，然而对于AI芯片的性\n",
      "\n",
      "能，讨论最多的是训练能力。，在GPT 3 175B大模型的训练中，H200相较于H100，只强了10%，提升并不\n",
      "\n",
      "明显，这操作，大概率是老黄有意为之，以前为了打造大模型，对GPU的首要要求是训练，但是到了现在，\n",
      "\n",
      "随着各种AI大预言模型的落地，大家开始卷的是推理速度。于是H200的升级，就忽略了算力升级，转向推理\n",
      "\n",
      "方面的发力，老黄的刀法依旧精准。哪怕只是小提升，依然当得起最强的称号。谁让NVIDIA的显卡，在AI芯\n",
      "\n",
      "片这块，这就是遥遥领先。\n",
      "\n",
      "但这因为是断供后的新卡，国内现在基本买不到。\n",
      "\n",
      "在H200没出现以前，H100是地表最强GPU，NVIDIA每一个层级的性能基本都是翻倍的，H100，其中\n",
      "\n",
      "微软采购了 15w片，mate 采购了 15w片，谷歌、亚马逊、甲骨文、腾讯都是5w片，那么谷歌的gemini发布\n",
      "\n",
      "晚，原来是因为缺少GPU哈。一共是 48w片，和外界传的 一年H100的产量50w基本吻合。在2024年预计出\n",
      "\n",
      "货量在200万张。中国采购的用户 H800要比H100量大，而且H800的售价比H100还要高，为什么性能不行\n",
      "\n",
      "价格还是高呢，主要原因还是有一份建议国企采购产品中的文件，里面只有A800和H800，没有A100和\n",
      "\n",
      "H100，这就导致国企采购更愿意采购A800的原因，\n",
      "\n",
      "同时需要说的是，在今年的10月份，漂亮国再次禁用 H800、A800芯片后，NVIDIA计算再次推出中国特\n",
      "\n",
      "供AI芯片，初步计划是3款，分别是h20，L20和 L2，这三款基于H100进行阉割。使以性能符合禁令的要\n",
      "\n",
      "求。其中最强的是H20，但与H100相比，性能被封印了80%，只有H100的20%左右的性能，对于NVIDIA而\n",
      "\n",
      "言，中国这笔70亿美元的大市场肯定不能丢，必须推出AI芯片来抢占，不过，近日有消息传出，这三款特供\n",
      "\n",
      "版芯片要跳票了，只有L20可能会按期推出，H20和L2都可能延期。特别是 H20这个最强的，什么时候推\n",
      "\n",
      "出，会不会推出都是未知数，最重要的原因，还是目前中国的市场已经发生了变化。没有NVIDIA想像的那么\n",
      "\n",
      "美好了。\n",
      "\n",
      "有一些朋友可能还不知道这回事，而知道的朋友有些也不清楚，为什么偏偏是显卡会成为国际博弈的工\n",
      "\n",
      "具，这些卡一张卖 十几 甚至几十万元，这么赚钱的生意，怎么就不让做了。在1999年之前的人类文明早\n",
      "\n",
      "期，世界上是没有显卡的，但已经有了电子游戏了，那时候的游戏画面是由CPU生成的，游戏玩家说，需要\n",
      "\n",
      "有高画质，于是就有了显卡。1999年，NVIDIA声称自己发明了GPU，也就是 GFFORCE 256，所谓的GPU，\n",
      "\n",
      "就是图形计算单元，他是显卡最最核心的部件，再给他配上其他一系列零件，就成为了一张显卡。GPU跟显\n",
      "\n",
      "卡，严格来说不是一个概念，只是大家平时很少特意区分。这玩意为啥能提升游戏画质？因为渲染游戏画面\n",
      "\n",
      "这件事，难就难在计算量太大了，比如游戏中任何一个3D物体，它的位置、方向、大小、光源、物体表面等\n",
      "\n",
      "变化，都需要电脑来计算。\n",
      "\n",
      "渲染画面这件事，就像再做10000道加减乘除，CPU的核心很强，但数量少。每个核心就像出于一个智\n",
      "\n",
      "力巅峰的高三学生，他能熟练的解出模拟卷上的最后一道大题，但让他算 1000道，他得累死。而显卡上面\n",
      "\n",
      "密密麻麻的分布着几千个小核心，每个核心都像是一个小学生，高考题肯定是不会，但他们能同时并排启\n",
      "\n",
      "动，在10秒只能，把10000道题做完。所以渲染特别快。显卡能提升画质，就是因为他有这种强大的并行计\n",
      "\n",
      "算能力。而显卡从此脱离游戏领域，被别的领域盯上，乃至成为国际博弈的筹码。也是因为这种强大的并行\n",
      "\n",
      "计算能力，。发布了没几年，斯坦福大学实验室就盯上了显卡，它们想要显卡解决其他领域的问题，但是并\n",
      "\n",
      "不简单，显卡算力虽强，但是它们都是小学生，需要合适的软件能驾驭才行。沿用刚才的比方，GPU就是一\n",
      "\n",
      "万个小学生在同时工作，计算能力很强，但前提是你得能把一道难解的大题，分解成无数个小学生能解决的\n",
      "\n",
      "简单问题才行。否则显卡再强又有什么用呢？转换到现实中，就是得让开发者能方便的写出代码。利用上显\n",
      "\n",
      "卡的并行计算能力才行。所以在2006年，带领团队出现了至今仍然在不断更新的 CUDA，CUDA就是更方便\n",
      "\n",
      "的让开发人员能够面向 编程 如果绝大多数 模型的训练 背后都离 开 的支持 除了\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "并行计算能力，在软件层面，配套的编程平台也成熟了，这就意味着，GPU可以完全离开游戏领域，走向更\n",
      "\n",
      "大的世界了。\n",
      "\n",
      "第一次感受到GPU，就是挖矿，也就是挖比特币，挖矿其实就是用计算机 来解决数学问题，比如任何数\n",
      "\n",
      "据，都可以通过哈希算法生成一串哈希值，原始数据不管发生多小的变化，最后生成的哈希值都完全不同。\n",
      "\n",
      "我们有时候下载大文件时，也会利用哈希算法的这种特性，来做一次校验。。看看下载的文件是否完整。很\n",
      "\n",
      "多挖矿，就是要生成一个符合要求的哈希值，这就需要计算机去反复的尝试，所以挖矿就跟游戏画面一样，\n",
      "\n",
      "属于那种不难，但是计算量非常大的事情。恰好能够利用显卡的算力。于是在加密价格持续攀升的日子里，\n",
      "\n",
      "显卡涨价，缺货。一路推动NVIDIA的市值从140亿美元暴涨到了 1750亿美元。但显卡跟加密货币之间只是一\n",
      "\n",
      "段露水情缘，随着专用矿机的出现，虚拟比特币的价格跳水，以太坊等调整了挖矿规则等原因，显卡跟虚拟\n",
      "\n",
      "货币的关系已经大不如前了。但显卡就跟上了更大、更革命的科技浪潮，就是AI。现在所有人都知道，AI是\n",
      "\n",
      "可能引起新一轮科技革命的巨大产业，而几乎所有的AI模型训练，都需要显卡。\n",
      "\n",
      "就拿现在正火的ChatGPT来说，它的模型训练中涉及到大量的矩阵运算，这些矩阵运算本身不难，但是\n",
      "\n",
      "量很大很大，所以就需要GPU来并行处理。AI是可能改变世界的，而AI的基础是 算法、算力和数据。而提到\n",
      "\n",
      "的A100和H100，售价高到十几万、甚至几十万的专业显卡，还供不应求。有报道说，训练ChatGPT需要相\n",
      "\n",
      "当于300块A100显卡的算力，光这一个项目就需要花几十个亿来购买显卡，这也是为什么 从2022年10月开\n",
      "\n",
      "始，NVIDIA的市值在半年时间内就飙升了34倍。\n",
      "\n",
      "## 2.8 国产AI超算芯片期待\n",
      "\n",
      "这么着急赶尽杀绝，不惜拉上自己企业垫背。答案就是我国在半导体自主研发上，已经触碰到了他们的\n",
      "\n",
      "痛处。很多人总以为，我们依赖国外的AI芯片，是自身技术上不过关，其实真相是我们的芯片都没有真正的\n",
      "\n",
      "上过牌桌，为什么？ AI芯片只有在实际应用中才能够发现问题，加快迭代，而我们的国产芯片，起步晚、性\n",
      "\n",
      "能差，所以国内的厂商大多不愿意使用，这也就造成了国产芯片无法获得正向反馈，发展速度只会越来越\n",
      "\n",
      "慢，这是一个矛盾的循环。在还有选择的时候，考虑到性能也好，成本也好，中国企业往往愿意选择像\n",
      "\n",
      "NVIDIA的芯片，所以在很长一段时间，美国对中国的高端芯片的制裁，也不是彻底封死，因为国产的AI芯片\n",
      "\n",
      "不是它的对手，都还不够好用，但现在，局面彻底改变了。出口禁令全面升级，中国企业没有了其他的任何\n",
      "\n",
      "选择，下一步只能规模化的采购国产芯片，并且培育出本土的产业链，逐步实现真正意义上的国产替代。那\n",
      "\n",
      "可能有人说，这件事这么简单，能实现岂不是早就实现了。不太可能。其实还真不一定，放眼全球算力芯片\n",
      "\n",
      "市场，不可否认，NVIDIA占据了九成多的份额，出于高度垄断的地位。但是，目前国产AI芯片的可替代方\n",
      "\n",
      "案，也不少。\n",
      "\n",
      "如果单看并行计算这个领域，有两家国产GPU公司值得关注：分别是摩尔线程和壁任科技。\n",
      "\n",
      "摩尔线程2020年10月成立，在2023年10月17日，第一款产品摩尔芯用了7纳米工艺，支持CUDA平台和\n",
      "\n",
      "算法模型，性能超过每秒20万亿次浮点计算，仅成立三年就上了白宫严选名单。成为老美的制裁对象之一。\n",
      "\n",
      "是现在唯一可能买到的实际产品，并且一直在更新驱动的公司。壁任科技一款产品 壁任一号，7纳米工艺，\n",
      "\n",
      "支持CUDA平台和算法模型，性能超过每秒30万亿次浮点计算。去年发布了一个GPU 叫BR100，性能就直逼\n",
      "\n",
      "英伟达的H100，但是这个芯片并没有使用，原因还是台积电不给生产，准确的说是美国政府不让台积电给我\n",
      "\n",
      "们生产，这就是一个不公平的竞争，华为的遭遇大家就更熟悉了，19年以后 芯片的生产、制造全都被摁的死\n",
      "\n",
      "死的，但是麒麟芯片出来了，说明我们大概率已经突破了先进芯片制造的生产流程了，顶多就是成本高一\n",
      "\n",
      "点。\n",
      "\n",
      "这些当中除了华为之外都面临相同的问题，那就是没有针对芯片专门优化的计算架构，换句话说，这些\n",
      "\n",
      "公司不具备与CUDA抗衡的能力，如果能成为芯片供应商，去识别其他的计算架构，理论上也是可以的。但\n",
      "\n",
      "是对于使用者来说这样做效率就太低了。一旦训练十几天，一旦出现BUG，不就前功尽弃了吗。所以这事还\n",
      "\n",
      "是得看华为 华为就厉害了 在人工智能领域的布局包括但不限于昇腾系列计算芯片 CANN异构计算架\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "可能布局出全栈式人工智能的公司，也是唯一一个全栈式国产化公司，困扰华为的最大问题还是美国的制\n",
      "\n",
      "裁，芯片没法生产，再怎么布局也是白搭，一旦能解决芯片问题，\n",
      "\n",
      "有句话说的好，天下苦英伟达久矣。有时候看似把我们逼入绝境，但其实是给我了我们绝地求生的机\n",
      "\n",
      "会，就看我们怎么把握了。尽管中国企业会迎来一段痛苦的过渡期，但从长远来看，这是一条不得不走的\n",
      "\n",
      "路。而面对当下的巨大差距，最关键的是终于有了攻坚克难的凝聚力。相信度过这段阵痛期，有的人，有的\n",
      "\n",
      "国家，只会后悔的拍大腿，眼睁睁的看着被逆袭、被超越，毕竟每一次我们都是这样过来的。\n",
      "\n",
      "---\n",
      "\n",
      "Section: 三、组装计算机硬件选型策略\n",
      "计算机八大件：CPU、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于一套\n",
      "\n",
      "需要部署大模型的个人计算机，如何搭配。\n",
      "\n",
      "## 3.1 GPU选型策略\n",
      "\n",
      "1. 选择厂商\n",
      "\n",
      "目前独立显卡主要有AMD和NVIDIA两家厂商。其中NVIDIA在深度学习布局较早，对深度学习框架支持\n",
      "\n",
      "更好。建议选择NVIDIA的GPU。\n",
      "\n",
      "[桌面显卡性能天梯图：https://www.mydrivers.com/zhuanti/tianti/gpu/index.html](https://www.mydrivers.com/zhuanti/tianti/gpu/index.html)\n",
      "\n",
      "2. 选择系列及品牌\n",
      "\n",
      "**对个人用户来说，就是从NVIDIA的RTX系列中，选择出合适的GPU。就部署大模型的需求来说，只需**\n",
      "\n",
      "考虑计算能力和显存大小就可以了。显存带宽通常相对固定，选择空间较小。目前各级别显卡的均价如下：\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "|品牌|华硕|微星|技嘉|\n",
      "|---|---|---|---|\n",
      "|顶级旗舰||||\n",
      "|旗舰|ROG猛禽|超龙X|大雕|\n",
      "|次旗舰|TUF|魔龙|超级雕/小雕|\n",
      "|中端|巨齿鲨|/|雪鹰/魔鹰|\n",
      "|丐版|DUAL|万图师|猎鹰|\n",
      "\n",
      "\n",
      "华硕显卡品控优做工好，高品牌信仰足，但溢价严重，这个牌子最会宣传，卡不错但性价比不是很高。\n",
      "\n",
      "高端优先推荐华硕ROG猛禽，当然缺点就是：贵，另外主流用户个人更推荐TUF，更低端的巨齿鲨和\n",
      "\n",
      "DUAL不太推荐\n",
      "\n",
      "微星显卡30系列之前更推荐魔龙，30系列更推荐超龙\n",
      "\n",
      "**准一线**\n",
      "\n",
      "|品牌|七彩虹|\n",
      "|---|---|\n",
      "|顶级旗舰|九段|\n",
      "|旗舰|火神/水神|\n",
      "|次旗舰|adoc|\n",
      "|中端|ultra|\n",
      "|丐版|战斧|\n",
      "\n",
      "\n",
      "\n",
      "推荐七彩虹，用料良心，保修出名的好，深受矿老板们的喜爱，支持个人送保，ultra，三风扇，散热性\n",
      "\n",
      "能极好，噪音小，白色颜值高，不带rgb灯效，喜欢rgb灯效的可以选择adoc。\n",
      "\n",
      "**二线**\n",
      "\n",
      "|品牌|影驰|索泰|映众|耕升|铭瑄|\n",
      "|---|---|---|---|---|---|\n",
      "|顶级旗舰||||||\n",
      "|旗舰|名人堂|PGF/AMP|冰龙寒霜||MGG 大玩家|\n",
      "|此旗舰|GAMER/星耀|天启|超级冰龙||电竞之心|\n",
      "|中端|金属大师|/|冰龙|炫光/星极||\n",
      "|丐版|黑将/大将|X-gaming|黑金至尊|追风|turbo/终结者|\n",
      "\n",
      "\n",
      "\n",
      "二线品牌中，影驰和索泰属于二线中实力比较强的两个，可以说是强二线，索泰重点推荐PGF（排骨\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "级别的产品，颜值高，性能强，次旗舰GAMER和星耀一个主打DIY一个主打RGB，都是非常有特点的产品\n",
      "\n",
      "企业级显卡\n",
      "\n",
      "参考第二部分的GPU推荐。\n",
      "\n",
      "服务器推断卡\n",
      "\n",
      "除了用于训练，还有一类卡是用于推断的（只预测，不训练），如：\n",
      "\n",
      "这些卡全部都是不带风扇的，但它们也需要散热，需要借助服务器强大的风扇被动散热，所以只能在专\n",
      "\n",
      "门设计的服务器上运行，性价比首选 Tesla T4，但是发挥全部性能需要使用 TensorRT 深度优化，目前仍然\n",
      "\n",
      "存在许多坑，比如当你的网络使用了不支持的运算符时，需要自己实现。\n",
      "\n",
      "**避免踩坑**\n",
      "\n",
      "如果选择配置单机多卡，采购显卡的时候，一定要注意买涡轮版的，不要买两个或者三个风扇的版本，\n",
      "\n",
      "除非只打算买一张卡。因为涡轮风扇的热是往外机箱外部吹的，可以很好地带走热量，如果买三个风扇的版\n",
      "\n",
      "本，插多卡的时候，上面的卡会把热量吹向第二张卡，导致第二张卡温度过高，影响性能。\n",
      "\n",
      "## 3.2 CPU选型策略\n",
      "\n",
      "CPU在大模型使用中起到什么作用？当在GPU上运行大模型时，CPU几乎不会进行任何计算。最有用的\n",
      "\n",
      "应用是数据预处理。CPU负责将数据从系统内存传输到GPU的显存中，同时也处理GPU完成计算后的数据。\n",
      "\n",
      "有两种不同的通用数据处理策略，具有不同的CPU需求。\n",
      "\n",
      "训练时处理数据：高性能的多核CPU能显著提高效率。建议每个GPU至少有4个线程，即为每个GPU分\n",
      "\n",
      "配两个CPU核心。每为GPU增加一个核心 ，应该获得大约0-5％的额外性能提升。\n",
      "\n",
      "训练前处理数据：不需要非常好的CPU。建议每个GPU至少有2个线程，即为每个GPU分配一个CPU核\n",
      "\n",
      "心。用这种策略，更多内核也不会让性能显著提升。\n",
      "\n",
      "在这种情况下，GPU通常承担大部分计算负担，CPU的作用更多是管理和协调，因此需要高核心数，同\n",
      "\n",
      "时也需要快速的数据预处理，同样需要高频率，所以高核心 + 高频率，虽然不是必须，但推我们推荐还是能\n",
      "\n",
      "**高即高，标准是：要与选择的GPU和CPU的性能水平相匹配，避免将一款高端显卡与低端CPU或一款高性能**\n",
      "\n",
      "CPU与低端显卡匹配，因为这可能导致性能瓶颈。比如：\n",
      "\n",
      "NVIDIA GeForce RTX 3090 * 2 搭配 Intel Core i7-13700K/KF 或 AMD 5900X CPU；\n",
      "\n",
      "NVIDIA GeForce RTX 3080 搭配 Intel Core i9-11900K CPU。\n",
      "\n",
      "但相对来说，瓶颈没有那么大，一般以一个GPU对应 2~4 个CPU核数就满足基本需求，比如单卡机器买\n",
      "\n",
      "四核CPU，四卡机器买十核CPU。在训练的时候，只要数据生成器（DataLoader）的产出速度比 GPU 的消\n",
      "\n",
      "耗速度快，那么 CPU 就不会成为瓶颈，也就不会拖慢训练速度。\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "去很长的一段时间了里，英特尔一直是绝对霸主的存在，代表最先进的生产力，时至今日，AMD在产品性能\n",
      "\n",
      "层面已经完全可以和Intel正面硬刚了。\n",
      "\n",
      "[CPU性能天梯图：https://www.mydrivers.com/zhuanti/tianti/cpu/index.html](https://www.mydrivers.com/zhuanti/tianti/cpu/index.html)\n",
      "\n",
      "**Intel 系列命名规范**\n",
      "\n",
      "可以通过CPU名称得到一些信息，如i7-10700K，代表产品型号是i7，后面的10代表是第10代，然后700\n",
      "\n",
      "代表性能等级高低，K代表这个CPU可以超频，当然后缀字母还有T、X、F等，X后缀代表高性能处理器，而T\n",
      "\n",
      "代表超低电压，/F代表无CPU无内置显卡版本。\n",
      "\n",
      "1. 系列：由低到高 Celeron（赛扬） / Pentium（奔腾） /酷睿系列的i3 / i5 / i7 / i9\n",
      "\n",
      "2. 世代：第1组数字代表是第几代\n",
      "\n",
      "例如这三个CPU：I7-8700、I7-9700，i7-10700第1个是第八代，第2个是第九代、第3个是第十代，还\n",
      "\n",
      "是比较容易理解的。\n",
      "\n",
      "3. 性能：第2组(3个数)是表示性能等级\n",
      "\n",
      "例如：I5-12400、I5-12500，数字越大表示越好。\n",
      "\n",
      "4. 后缀：K→可超频，F→没有核显\n",
      "\n",
      "可超频K版CPU要搭配可超频的Z系列主板才行，才可以超频，搭载不能超频的主板也可以用，只是不能\n",
      "\n",
      "超频了而已。\n",
      "\n",
      "没有核显的F版CPU要搭配独立显卡才能开机点亮屏幕\n",
      "\n",
      "超频简单理解就是可以提升处理器性能，核显就是把一张入门级的显卡塞进处理器里。\n",
      "\n",
      "**i3是家用级别，i5是游戏级别，i7是生产力和游戏发烧友级别，i9是最顶级的。后缀带K可以超频，带F**\n",
      "\n",
      "**表示没有核显。**\n",
      "\n",
      "**AMD系列命名规范**\n",
      "\n",
      "和Intel类似：\n",
      "\n",
      "1. 系列：由低到高 APU / Althlon（速龙） / Ryzen（锐龙）系列 R3 / R5 / R7 / R9\n",
      "\n",
      "2. 世代：第1个数字代表第几代\n",
      "\n",
      "3. 例如这两个CPU：R7-2700X、R7-3700X，第1个是第二代，第2个是第三代。\n",
      "\n",
      "4. 性能：第2组数字（3个数字）表示性能等级\n",
      "\n",
      "数字越大性能越好，例如 R7 3800X的性能大于R7 3700X。\n",
      "\n",
      "5 后缀：字母G表示有核显 字母X没有明确意思 一般性能强一点 如R5 3600X比R5 3600性能高一\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "**要选Intel还是AMD，其实都可以。如果追求性价比，AMD性价比高一些，如果主要玩游戏，且对价格**\n",
      "\n",
      "不敏感，建议选择英特尔Intel，英特尔Intel一般主频较高，一些游戏主要依赖主频，所以高主频的Intel玩游\n",
      "\n",
      "戏更推荐一些。除了品牌维度的分析，目前主流的大模型训练硬件通常采用 Intel + NVIDIA GPU。但具体\n",
      "\n",
      "情况具体分析，只能简单说：一分钱一分货，一般来说贵的好。\n",
      "\n",
      "**选购CPU误区**\n",
      "\n",
      "电子产品有一个说法是，“买新不买旧”，一般新产品，会用更新的工艺架构，性能更强，功耗更低，比\n",
      "\n",
      "较值得购买。当然有些时候，老一代的性价比很高，也可以考虑，但如果是好几代前的老产品，就不要考虑\n",
      "\n",
      "了，有些商家会卖几年前的i7电脑主机，它的性能可能还不如最新的i3，主要是忽悠小白的，要注意辨别。\n",
      "\n",
      "目前消费级市场，我们最常听到的i3 i5 i7 等，是英特尔的酷睿系列产品，主要面向一般消费市场，数字\n",
      "\n",
      "大的性能更强，（注意这里只在同代产品中成立）。AMD与之对应的是R3 R5 R7。这里值得注意是，同代产\n",
      "\n",
      "品i7比i5强，如果拿老一代的i7和新一代的i5比，就未必成立，部分商家经常会营销i5免费升级i7，其实是把\n",
      "\n",
      "最新一代的i5换成立了老一代的i7，性能方面可能还不如没升级呢？比如i5-8400的性能就高于i7-7700.\n",
      "\n",
      "## 3.3 散热选型策略\n",
      "\n",
      "CPU 不断地更新换代和性能提升，其功耗和发热量也越来越大，如果温度过高，就会出现自动关机或者\n",
      "\n",
      "是蓝屏死机等情况，所以需要单独的散热器来压制，目前CPU散热器分两种：水冷和风冷。\n",
      "\n",
      "风冷和水冷系统都是用于GPU的散热解决方案。它们各有优势和不足：通常，水冷系统在散热效率方面\n",
      "\n",
      "**优于风冷系统。以Intel的i9-13900KF为例，这款CPU性能目前位于CPU性能天梯榜第二位，很多用户认为使**\n",
      "\n",
      "用水冷系统是必要的。但如果这款CPU没有超频需求，使用高质量的风冷系统其实也能够有效地散热。只有\n",
      "\n",
      "在超频的情况下，水冷系统由于其更出色的冷却效果，才成为更佳的选择。\n",
      "\n",
      "但需要注意，风冷和水冷与GPU无关。在计算机硬件中，CPU和GPU（显卡）的散热策略和要求各有不\n",
      "\n",
      "同。CPU通常需要配单独的散热器，我们可以根据需要选择并购买不同类型的散热器，例如水冷或风冷系\n",
      "\n",
      "统，并且可以根据性能要求进行升级。由于CPU的高主频和较少的核心数（通常是几个到二十几个核心），\n",
      "\n",
      "高性能的CPU在运行时会产生较多热量，因此需要更有效的散热解决方案来维持合适的运行温度。与此相\n",
      "\n",
      "对，显卡通常采用一体化的散热设计，其散热器是显卡的重要组成部分。显卡散热器的设计已经由各厂商经\n",
      "\n",
      "过测试和优化，因此用户一般不需要担心显卡的散热问题。显卡采用的是多核心、低频率的策略，即使是高\n",
      "\n",
      "端显卡如Nvidia的4090，其频率也相对较低，通常在3000MHz左右，而同代的高端CPU（如Intel i9）的频\n",
      "\n",
      "率可能是其两倍。显卡的散热器可以直接接触GPU核心和显存，从而高效散热。因此，在正常满载情况下，\n",
      "\n",
      "显卡的温度达到70多或80多度是正常现象，通常不会成为性能瓶颈。\n",
      "\n",
      "对于大模型部署来说，首要原则还是CPU的等级要和GPU相匹配。对于中低端处理器，如Intel的i5系\n",
      "\n",
      "列，以及AMD的R5和R7系列，一般推荐使用风冷系统。这些处理器的热设计功耗（TDP）通常较低，风冷系\n",
      "\n",
      "统足以提供有效的散热。而对于更高性能的处理器，如Intel的i7 13700KF及更高级别的i7和i9系列，建议至\n",
      "\n",
      "少使用240mm规格的水冷系统。考虑到这些处理器较高的性能和热输出，水冷系统能提供更为高效和稳定\n",
      "\n",
      "的冷却效果。因此，尽管风冷在某些情况下依然可行，但为了确保最佳性能和稳定性，对于高性能处理器，\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "在构建大模型的系统时，低端主板通常不适用。根据所选的CPU和GPU规格，应从中端或高端主板中选\n",
      "\n",
      "择出合适的。\n",
      "\n",
      "|制造 商|系列|定 位|支持CPU超 频|支持内存超 频|适用用户|\n",
      "|---|---|---|---|---|---|\n",
      "|Intel|Z系 列|高 端|是|是|追求高性能和定制化设置的用户|\n",
      "|Intel|B系 列|中 端|否|是|需要一定性能但预算有限的用户|\n",
      "|Intel|H系 列|低 端|否|否|预算有限或对性能要求不高的基本用 途|\n",
      "|AMD|X系 列|高 端|是|是|追求最高性能和高度定制化的用户|\n",
      "|AMD|B系 列|中 端|否|是|寻求性价比的用户|\n",
      "|AMD|A系 列|低 端|否|否|有限预算或基本计算需求的用户|\n",
      "\n",
      "\n",
      "\n",
      "选择主板时，核心因素是确保它与CPU的性能和超频能力相匹配。以Intel处理器为例：对于中高端CPU\n",
      "\n",
      "（如i5系列及以上），更适合选择B660到Z690系列的主板。对于如13600KF这样的高性能CPU，至少应选择\n",
      "\n",
      "B660系列的主板作为起点。需要考虑的是CPU是否支持超频（如带有“K”后缀）。可超频的CPU更适合搭配\n",
      "\n",
      "支持超频的高端主板，如Z系列\n",
      "\n",
      "其次，需要检查CPU和主板型号是否匹配及合理。\n",
      "\n",
      "通常情况下，每一种型号的CPU都需要搭配对应型号的主板，每代CPU和主板都有自己的针角及接口类\n",
      "\n",
      "型，Intel cpu不能用于AMD系列主板，某些主板可能会通用几代cpu，但有的主板只能兼容某一代，例如\n",
      "\n",
      "intel 十代 的i510400f，不能用于早期的四代 b85系列主板，而是否匹配，指的是高性能CPU搭配低性能主\n",
      "\n",
      "板，h610是入门主板，虽然可以点亮，但是低端主板因为供电有限，无法发挥出cpu的全部性能，以及无法\n",
      "\n",
      "超频，这样就失去了cpu本身的性能和意义。\n",
      "\n",
      "最后，考虑PCIe通道。\n",
      "\n",
      "PCIe通道是一种高速接口，用于将GPU连接到计算机的主板。通过这些通道，GPU可以与CPU以及系统\n",
      "\n",
      "内存快速交换数据。每个PCIe通道（或称为“通道”）都提供一定的数据传输带宽。更多的通道意味着更高的\n",
      "\n",
      "总体带宽。例如，PCIe 3.0 x16接口意味着有16个通道，每个通道的速度是PCIe 3.0标准的速度。\n",
      "\n",
      "GPU的性能部分取决于它与主板之间的通信速度，这是由PCIe通道的数量和版本（如PCIe 3.0、4.0或\n",
      "\n",
      "5.0）决定的。更高版本的PCIe提供更高的传输速率，从而可能提高GPU的性能。以下是需要考虑的几个关键\n",
      "\n",
      "点：\n",
      "\n",
      "1. PCIe版本：\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "4.0和5.0）提供更高的数据传输速率，这对于高性能GPU和其他高速设备非常重要。\n",
      "\n",
      "2. PCIe槽数量和布局：\n",
      "\n",
      "主板上的PCIe槽数量决定了可以安装多少个扩展卡。如果计划安装多个GPU或其他PCIe设备，需\n",
      "\n",
      "要确保主板有足够的槽位。\n",
      "\n",
      "槽位布局也很重要，尤其是在安装大型GPU时，需要确保它们之间有足够的空间，避免过热或物\n",
      "\n",
      "理干扰。\n",
      "\n",
      "3. PCIe通道分配：\n",
      "\n",
      "主板上的PCIe通道是从CPU和芯片组分配的。不同的主板可能有不同的通道分配方式，这可能会\n",
      "\n",
      "影响到扩展卡的性能，特别是在多GPU配置中。\n",
      "\n",
      "确认主板是否支持您所需的PCIe配置，例如双向或四向GPU设置。\n",
      "\n",
      "4. 与GPU的兼容性：\n",
      "\n",
      "虽然大多数现代GPU兼容大多数主板的PCIe槽，但是为了最佳性能，最好确认GPU与主板的PCIe\n",
      "\n",
      "版本相匹配。\n",
      "\n",
      "综上所述，因为需要通过PCIe通道连接和使用GPU，因此在选择主板时考虑PCIe通道的版本、数量、布\n",
      "\n",
      "局和通道分配非常重要。\n",
      "\n",
      "## 3.5 硬盘选型策略\n",
      "\n",
      "**首先考虑接口类型。主流固态硬盘主要有两种接口：SATA和M.2。**\n",
      "\n",
      "**SATA接口的固态硬盘体积较大，形状类似于传统的机械硬盘，主要用于升级老式电脑，因为这些电脑**\n",
      "\n",
      "通常不具备M.2接口。SATA接口硬盘的最高速度为600MB/s。\n",
      "\n",
      "**M.2接口的硬盘则较小，可以直接安装在主板上的专用接口。它们采用新的硬盘协议，速度上可以达到**\n",
      "\n",
      "4GB/s。\n",
      "\n",
      "**推荐选择 M.2接口的硬盘。**\n",
      "\n",
      "**然后考虑协议。M.2接口的固态硬盘分为SATA协议和NVMe协议两种。**\n",
      "\n",
      "M.2接口的SATA协议硬盘速度较慢，实际上就是标准SATA硬盘的形状变化，速度仍然是最高\n",
      "\n",
      "600MB/s，这类硬盘多用于旧电脑。\n",
      "\n",
      "**NVMe协议硬盘则速度更快，适合对速度有较高要求的应用。**\n",
      "\n",
      "在面对大量小文件的时候，使用 NVMe 硬盘可以一分钟扫完 1000万文件，如果使用普通硬盘，那么就\n",
      "\n",
      "需要一天时间。推荐选择 NVME协议的M.2接口的硬盘。\n",
      "\n",
      "**最后考虑 PCIe等级。当前市面上最新的是PCIe 5.0，但更常见的是PCIe 3.0和PCIe 4.0。PCIe等级越**\n",
      "\n",
      "高，硬盘的速度潜力越大。但重要的是检查主板是否支持相应的PCIe等级。例如，一些主板可能最高只支持\n",
      "\n",
      "到PCIe 4.0。一般来说，选择PCIe 4.0的即可。\n",
      "\n",
      "硬盘不会限制深度学习任务的运行，但如果小看了硬盘的作用，可能会让你追、悔、莫、及。想象一\n",
      "\n",
      "下，如果你从硬盘中读取的数据的速度只有100MB/s，那么加载一个32张ImageNet图片构成的mini\n",
      "\n",
      "-----\n",
      "\n",
      "**建议内存容量应大于GPU的显存。例如，对于搭载单卡GPU的系统，建议配置至少16GB内存。如果是**\n",
      "\n",
      "四卡GPU系统，则建议至少配置64GB内存。由于数据生成器（DataLoader）的存在，数据不需要全部加载\n",
      "\n",
      "到内存中，因此内存通常不会成为性能瓶颈。\n",
      "\n",
      "内存不用太纠结，是GPU显存的一到两倍。目前，128G 就可以，64G 也凑合。而且内存没那么贵，可\n",
      "\n",
      "以配满。\n",
      "\n",
      "内存大小不会影响深度学习性能，但是它可能会影响你执行GPU代码的效率。内存容量大一点，CPU就\n",
      "\n",
      "可以不通过磁盘，直接和GPU交换数据。所以应该配备与GPU显存匹配的内存容量。\n",
      "\n",
      "在选择的时候，注意检查主板是否支持内存的数量及型号。目前常见的 ddr3 ~ 5，每一代内存都需要对\n",
      "\n",
      "应主板的插槽类，ddr 5代内存 是无法混插在 ddr 4代内存上的。另外需要确定主板的内存插槽数量，如果只\n",
      "\n",
      "有两个插槽，买了四个，那么根本插不进去。\n",
      "\n",
      "其次检查cpu主板是否支持内存频率。内存条的频率上限 受到 cpu 和主板的控制， 例如 i5的 10400f +\n",
      "\n",
      "b460主板 = 2666，如果你买的内存是 3600频率的，无疑发挥不出内存本身的优势。\n",
      "\n",
      "## 3.7 电源选型策略\n",
      "\n",
      "在选择电脑电源时，需要检查电源的瓦数是否足以支持整机的功耗。并非越高瓦数越好，但瓦数过低可\n",
      "\n",
      "能会导致关机或黑屏等问题。在确定合适的电源瓦数之前，应综合评估整机硬件的功耗，尤其要考虑CPU和\n",
      "\n",
      "显卡这两个功耗大户。通常，将CPU和显卡的TDP功耗相加后乘以2可以得到一个合适的电源瓦数估计。例\n",
      "\n",
      "如，对于一个65W的CPU和125W的显卡，合适的电源瓦数应该在400W或450W左右。\n",
      "\n",
      "双卡最好1000W以上，四卡最好买1600W的电源\n",
      "\n",
      "## 3.8 机箱选型策略\n",
      "\n",
      "最后，选择完所有上述所有配件之后，选择机箱就相对简单，只需要确保这个机箱足够宽敞，能够容纳\n",
      "\n",
      "所选的所有配件。我们需要检查以下几项内容：\n",
      "\n",
      "1. 核对主板与机箱尺寸匹配性：\n",
      "\n",
      "确保所选主板的大小与机箱兼容。例如，ITX主板应与ITX机箱相匹配。这就像选择合适大小的鞋子\n",
      "\n",
      "一样重要。\n",
      "\n",
      "2. 确认机箱支持显卡尺寸：\n",
      "\n",
      "对比显卡的长度与机箱对显卡长度的限制。建议选择机箱的显卡限长至少比显卡长度长出30毫米\n",
      "\n",
      "以上，以确保有足够空间进行安装和通风。\n",
      "\n",
      "3. 检查散热器与机箱的兼容性：\n",
      "\n",
      "非常重要的一点是比较散热器的尺寸与机箱的散热限高。如果散热器太高，可能无法正常安装侧\n",
      "\n",
      "盖。\n",
      "\n",
      "考虑到许多内存条配备较高的散热马甲，需要确认散热风扇是否会受到内存条的干扰。\n",
      "\n",
      "如果选择水冷散热系统，确保机箱的水冷位能够容纳水冷冷排的尺寸。例如，360mm的水冷冷排\n",
      "\n",
      "无法安装在仅适用于240mm的位置上。\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "常见的电源类型包括SFX、ATX和TFX。由于不同规格的电源在形状和大小上有所不同，必须确认\n",
      "\n",
      "机箱的电源仓是否适合所选电源的尺寸。\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 调用函数并提取内容\n",
    "sections = extract_content_by_sections(doc)\n",
    "\n",
    "# 打印每个一级标题下的内容\n",
    "for section, content in sections.items():\n",
    "    print(f\"Section: {section}\")\n",
    "    print(content)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc805b4-0e9b-4fff-b1ba-2e84ead8e7a8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来构建推荐系统的项目表示，针对我们的当前的设计，可以考虑将每个大课程分解为多个模块或课件，每个模块代表课程中的一个知识点或一组知识点。这样，每个模块都可以作为推荐系统中的一个独立单元。\n",
    "\n",
    "&emsp;&emsp;数据字段设计\n",
    "- ModuleID: 每个模块的唯一标识符。\n",
    "- Course: 对应的大课程名，如“机器学习”、“自然语言处理”等。\n",
    "- ModuleName: 模块名称，如“监督学习基础”、“卷积神经网络介绍”等。\n",
    "- Description: 模块的描述，简要概述模块内容。\n",
    "- URL: 指向模块资源的链接，如视频讲解、课件下载等。\n",
    "- Tags: 模块涉及的关键词或技术点，如“线性回归”，“BERT”，“图像分类”等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a03ce963-7d1b-4697-ac25-28d3a026f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch 1 开源大模型本地部署硬件指南.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def list_pdf_files(directory):\n",
    "    # 列出指定目录下的所有文件和文件夹\n",
    "    items = os.listdir(directory)\n",
    "    # 过滤出以 .pdf 结尾的文件\n",
    "    pdf_files = [item for item in items if item.endswith('.pdf')]\n",
    "    return pdf_files\n",
    "\n",
    "# 使用当前目录作为参数调用函数\n",
    "current_directory = 'data'\n",
    "pdf_files = list_pdf_files(current_directory)\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5959f363-3d5c-4616-8b4c-9516b5f4b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/Ch 1 开源大模型本地部署硬件指南.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_pdf_files_full_path(directory):\n",
    "    # 列出指定目录下的所有文件和文件夹\n",
    "    items = os.listdir(directory)\n",
    "    # 使用列表推导式，过滤出以 .pdf 结尾的文件，并为每个文件构造完整路径\n",
    "    pdf_files = [os.path.join(directory, item) for item in items if item.endswith('.pdf')]\n",
    "    return pdf_files\n",
    "\n",
    "# 使用当前目录作为参数调用函数\n",
    "current_directory = 'data'\n",
    "pdf_files_full_paths = list_pdf_files_full_path(current_directory)\n",
    "print(pdf_files_full_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79106a2f-923a-4e1d-8390-b5f537cc5f9a",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94569e2c-4032-49fe-a427-0cc7a4a7a0e9",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ca96a-9b20-4216-a4d0-5e6eea38edbe",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45f31a03-d502-456d-a3dd-3e27fc407719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import os\n",
    "\n",
    "data = []\n",
    "\n",
    "for module_name, content in sections.items():\n",
    "    tags = [line.strip() for line in content.split('\\n') if line.startswith('##')]\n",
    "    tags = [re.sub(r'^##\\s+', '', tag) for tag in tags]  # 清理 '##'\n",
    "\n",
    "    data.append({\n",
    "        'ModuleID': str(uuid.uuid4()),\n",
    "        'Course':pdf_files,\n",
    "        'URL': pdf_files_full_paths,\n",
    "        'ModuleName': module_name,\n",
    "        'Tags': \", \".join(tags),\n",
    "        'Content': content\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c6131b8-3a84-4b09-ae0e-98d010164be2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ModuleID': '3fb9c842-26df-4c66-a865-030ffbc05ba3',\n",
       "  'Course': ['Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'URL': ['data/Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'ModuleName': '本地部署开源大模型',\n",
       "  'Tags': 'Ch.1 如何选择合适的硬件配置',\n",
       "  'Content': '## Ch.1 如何选择合适的硬件配置\\n\\n为了在本地有效部署和使用开源大模型，深入理解硬件与软件的需求至关重要。在硬件需求方面，关键\\n\\n是配置一台或多台高性能的个人计算机系统或租用配备了先进GPU的在线服务器，确保有足够的内存和存储\\n\\n空间来处理大数据和复杂模型。至于软件需求，推荐使用Ubuntu操作系统，因其在机器学习领域的支持和\\n\\n兼容性优于Windows。编程语言建议以Python为主，结合TensorFlow或PyTorch等流行机器学习框架，并\\n\\n利用DeepSpeed等优化工具来提升大模型的运行效率和性能。\\n\\n所以在本系列课程中，我们将从硬件选择入手，逐步引导大家理解并掌握如何为大模型部署选择合适的\\n\\n硬件，以及如何高效地配置和运行这些模型，从零到一实现大模型的本地部署和应用。首先来看硬件方面，\\n\\n提前规划计算资源是必要的。目前，我们主要考虑以下两种途径：\\n\\n1. 配置个人计算机或服务器，组建一个适合大模型使用需求的计算机系统。\\n\\n2. 租用在线GPU服务，通过云计算平台获取大模型所需的计算能力。'},\n",
       " {'ModuleID': 'a4d419bf-68cd-47a7-9efb-bbdef5b8756e',\n",
       "  'Course': ['Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'URL': ['data/Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'ModuleName': '一、大模型应用需求分析',\n",
       "  'Tags': '',\n",
       "  'Content': '大模型的本地部署主要应用于三个方面：训练（train）、高效微调（fine-tune）和推理\\n\\n**（inference）。这些过程在算力消耗上有显著差异：**\\n\\n**训练：算力最密集，通常消耗的算力是推理过程的至少三个数量级以上。**\\n\\n**微调：微调是在预训练模型的基础上对其进行进一步调整以适应特定任务的过程，其算力需求低于训**\\n\\n练，但高于推理。\\n\\n**推理：推理指的是使用训练好的模型来进行预测或分析，是算力消耗最低的阶段。**\\n\\n总的来说，在算力消耗上，训练 > 微调 > 推理。\\n\\n从头训练一个大模型并非易事，这不仅对个人用户，对于许多企业而言也同样困难。因此，如果个人使\\n\\n用，关注点应该放在推理和微调的性能上。在这两种应用需求下，对硬件的核心要求体现在GPU的选择上，\\n\\n**对CPU和内存的要求并不高。无论是选择租用在线算力还是配置本地计算机，如果想在本地运行大模型，我**\\n\\n们可以拆分成两个关注点：\\n\\n模型：选择什么基座模型或微调模型，这可以直接下载至本地。\\n\\n硬件：希望在什么硬件平台上来执行，可以分为 CPU 和 GPU 两大类。\\n\\n⼤部分开源⼤模型⽀持在 CPU 和 Mac M系列芯片上运⾏，但较为繁琐且占⽤内存⾄少 32G 以上，因此\\n\\n更推荐在 GPU 上运⾏。针对本地部署大模型，在选择GPU时，可以遵循的简单策略是：在满足具体的大模\\n\\n**型的官方配置要求下，选择性价比最高的GPU。**\\n\\nGPU的性能主要由以下三个核心参数决定：\\n\\n1. 计算能力：这是最关注的指标，尤其是32位浮点计算能力。随着技术发展，16位浮点训练也日渐普\\n\\n及。对于仅进行预测的任务，INT 8 量化版本也足够；\\n\\n2. 显存大小：大模型的规模和训练批量大小直接影响对显存的需求。更大的模型或更大的批量处理需要更\\n\\n多的显存；\\n\\n\\n-----\\n\\n处理大量数据时的性能通常也越好；\\n\\n注：显存带宽相对固定，选择空间较小。'},\n",
       " {'ModuleID': 'ad5870ff-83e3-4858-b9c5-613e7f2ad695',\n",
       "  'Course': ['Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'URL': ['data/Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'ModuleName': '二、硬件配置的选择标准',\n",
       "  'Tags': '2.1 选择满足显存需求的 GPU, 2.2 主流显卡性能分析, 2.3 单卡4090 vs A100系列, 2.4 单卡4090 vs 双卡3090, 2.5 风扇卡与涡轮卡如何选择, 2.6 整机参考配置, 2.7 显卡博弈的形式分析, 2.8 国产AI超算芯片期待',\n",
       "  'Content': '无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\\n\\n（如微调或推理）都需要相应的硬件配置方案来支持。所以在选择硬件配置时应根据具体的模型需求和预期\\n\\n**用途来确定。**\\n\\n因此，我们的建议是：根据部署的大模型配置需求，先选择出最合适的 GPU，然后再根据所选 GPU 的\\n\\n**特性，进一步搭配计算机的其他组件，如CPU、内存和存储等，以确保整体系统的协调性和高效性能。最简**\\n\\n**单的匹配GPU的标准是显存大小和性价比。因为训练不纯粹看一个显存容量大小，而是和芯片的算力高度相**\\n\\n关的。因为实际训练的过程当中，将海量的数据切块成不同的batch size，然后送入显卡进行训练。显存\\n\\n大，意味着一次可以送进更大的数据块。但是芯片算力如果不足，单个数据块就需要更长的等待时间显存和\\n\\n算力，必须要相辅相成。\\n\\n简单来说，在深度学习的训练和推理中，GPU的显存主要用于以下几个方面：\\n\\n1. 权重存储：模型的参数，包括权重和偏置，都需要在显存中存储。这些参数是模型进行预测或分类所必\\n\\n需的。\\n\\n2. 中间过程数据存储：在模型计算的前向传播和反向传播过程中，会产生并且需要暂时存储大量的中间计\\n\\n算结果。这些数据同样存储在显存中。\\n\\n3. 计算过程：GPU专门为并行处理大量的矩阵和向量运算而设计，这正是深度学习中常见的计算类型。这\\n\\n些计算直接在显存中进行，以利用GPU的高速运算能力。\\n\\n显存的大小和速度直接影响到模型的处理速度和能处理的模型大小。显存越大，意味着可以处理更大的\\n\\n模型和更复杂的计算任务，但同时也需要更多的能源和可能导致更高的成本。\\n\\n\"芯片\"通常指的是集成电路，它们被集成到各种电脑硬件组件中，如CPU、GPU和主板等。CPU本身就\\n\\n是一种芯片。它是计算机的大脑，负责执行程序和处理数据。显卡上的核心组件是图形处理器\\n\\n（GPU），它也是一种芯片。GPU负责处理图形和视频渲染。\\n\\n**所谓的\"算力\"大小，通常指的是整个计算系统的处理能力，尽管在特定上下文中，它有时特指GPU的处**\\n\\n**理能力。**\\n\\n我们以ChatGLM-6B模型为例，官方给出的硬件配置说明如下：\\n\\n模型量化是一种用于优化模型的技术，特别是在推理时。它通过减少模型中使用的数值精度来减小模\\n\\n型的大小，加快推理速度，并降低内存和能源消耗。模型量化常用于将模型部署到资源受限的设备\\n\\n上，如手机或嵌入式系统，量化程度越高，对硬件的要求就会越低。 单精度通常指的是32位浮点数\\n\\n（FP32），使用32位表示，包括1位符号位、8位指数位和23位尾数位。FP32是标准的训练和推理格\\n\\n式，但由于半精度（FP16）提供了相似的结果且计算速度更快，更节省内存，因此在资源受限或需要\\n\\n\\n-----\\n\\n少，它的计算量就会越小，对应的输出结果的精度也就会越差。\\n\\n## 2.1 选择满足显存需求的 GPU\\n\\n关于如何选择GPU，当前市场 NVIDIA 和 AMD 是两大主要显卡生产商。但在人工智能、大数据、深度\\n\\n学习领域，NVIDIA（通常被称为N卡）几乎独占鳌头。主要原因还是NVIDIA在很早期就开始专注于AI和深度\\n\\n学习市场，开发了强大的软件工具和库，例如cuDNN、TensorRT，这些都是专门为深度学习优化的，与流\\n\\n行的深度学习框架（如TensorFlow、PyTorch等）紧密集成，同时NVIDIA的CUDA（Compute Unified\\n\\nDevice Architecture）作为独特的平行计算平台和编程模型，它允许开发者利用NVIDIA的GPU进行高效的通\\n\\n用计算。这一点对于深度学习和大数据分析等需要大量并行处理的应用来说至关重要。\\n\\n**英伟达是一家什么公司？**\\n\\n这时候可能有小伙伴说了，英伟达是一家卖游戏显卡的，这个说法呢，对，但也不对，从财报来看，英\\n\\n伟达目前主要有四块业务，分别是游戏GPU，数据中心产品，自动驾驶芯片和其他业务。占比分别为\\n\\n33.6%，55.%，3.3%和7.4%。游戏GPU，数据中心产品，自动驾驶芯片其实都可以归类为计算芯片这个门\\n\\n类下面，换句话说，如果从财报公布的业务情况来分析的话，英伟达确实就是一家卖计算芯片的公司。但如\\n\\n果真的把英伟达当做一家卖芯片的公司，那就大错特错了。英伟达的确是靠着游戏显卡起家，并且在人工智\\n\\n能爆发的现在靠着一手AI计算芯片市值突破了万亿美元，但其实它并不是一家卖芯片的公司，我对英伟达的\\n\\n定位是，它是一家卖人工智能系统的公司。这就有两个核心的概念，一个是英伟达的计算芯片，一个是英伟\\n\\n达针对自家芯片做的计算架构CUDA，二者缺一不可。这种定位就像智能手机时代的苹果公司，苹果依靠着A\\n\\n系列芯片和ios操作系统收割了智能手机行业超过80%的利润。人工智能大发展的时代，英伟达就依靠着GPU\\n\\n和计算芯片与CUDA计算架构，共同组成的AI生态系统赢得了市场青睐，根据相关机构的统计数据，在独立\\n\\n显卡领域，英伟达的市占率高达85%，在AI算力芯片领域，在未来可能达到90%，现在做深度学习，英伟达\\n\\n的卡就是刚需，没有其他的选择。\\n\\n因此，我们建议还是选择 NVIDIA 的显卡。如果对应的ChatGLM-6B模型的硬件配置说明，我们就可以\\n\\n这样选择GPU。理论上，在进行少量对话时:\\n\\n在选择显卡时，必须遵循的首要准则是：显卡的显存容量一定要高于大模型官方要求的最低显存配置。\\n\\n这是确保模型能够有效运行的基本要求。显存容量越大，其推理或微调的能力就会越强。当然，随着显存容\\n\\n量的增加，显卡的价格也相应提高。以下是目前最主流的几款大模型的显卡型号及其显存容量：\\n\\n|显卡型号|显存容量|\\n|---|---|\\n|H100|80 GB|\\n|A100|80/40 GB|\\n|H800|80 GB|\\n\\n\\n\\nA800 80 GB\\n\\n\\n-----\\n\\n|显卡型号|显存容量|\\n|---|---|\\n|4090|24 GB|\\n|3090|24 GB|\\n\\n\\n其组合形式可以分为以下四类：\\n\\n1. 纯CPU：基于不同架构的CPU配置，适用于不需要或不能使用GPU加速的场景。（不推荐）\\n\\nx86 (如Intel或AMD)\\n\\nARM (如Apple、Qualcomm、MTK)\\n\\n2. 单机单卡：使用一块GPU进行计算，适用于大多数个人使用和一些中等计算负载的场景。（典型配置）\\n\\nNvidia系列GPU\\n\\nAMD系列GPU\\n\\nApple系列GPU\\n\\nApple Neural Engine（较少见，支持有限）\\n\\n3. 单机多卡：在一台机器上使用多张GPU卡，适用于高计算负载的场景，如模型分割处理。（典型配置）\\n\\n4. 多机配置：使用多台计算机进行集群计算，通常超出个人使用范围，主要用于从头预训练基座模型等高\\n\\n负载任务。\\n\\n所以，在单个显卡的显存容量不足以满足需求时，也可以采用多显卡配置来增加整体的显存容量。只要\\n\\n总显存超过官方推荐的配置要求就可以。此外，在选择显卡时，除了考虑整体显存容量，还要根据不同显卡\\n\\n的性能和成本进行权衡。根据具体需求和预算，决定是选择单张高性能显卡，还是部署多张成本效益更高的\\n\\n低版本显卡。实现最优的性价比。比如在理论上，在进行多轮对话时或需要微调时，采用单机多卡：\\n\\n## 2.2 主流显卡性能分析\\n\\n对于 NVIDIA的显卡（N卡）卡来说，我们可以按照以下几个维度来划分：\\n\\n按照产品线划分：\\n\\n|系列|特点|主要应用领域|\\n|---|---|---|\\n\\n\\nGeForce\\n\\n系列（G\\n\\n\\n消费级GPU产品线，注重提供高性能的图形处理能力和游戏\\n\\n特性 性价比高 适合游戏和深度学习推理 训练\\n\\n\\n主要面向游戏玩家和普\\n\\n通用户\\n\\n\\n-----\\n\\n|系列|特点|主要应用领域|\\n|---|---|---|\\n|Quadro 系列（P 系列）|专业级GPU产品线，针对商业和专业应用领域进行了优化， 适用于设计、建筑等专业图像处理。|设计、建筑、专业图像 处理等领域。|\\n|Tesla系 列（T系 列）|主要用于高性能计算和机器学习任务，集成深度学习加速 器，提供快速的矩阵运算和神经网络推理。|高性能计算、机器学习 任务等领域。|\\n|Tegra系 列|移动处理器产品线，用于嵌入式系统、智能手机、平板电 脑、汽车电子等领域，具备高性能的图形和计算能力，低功 耗。|嵌入式系统、智能手 机、平板电脑、汽车电 子等领域。|\\n|Jetson系 列|面向边缘计算和人工智能应用的嵌入式开发平台，具备强大 的计算和推理能力，适用于智能摄像头、机器人、自动驾驶 系统等。|边缘计算、人工智能、 机器人等领域。|\\n|DGX系列|面向深度学习和人工智能研究的高性能计算服务器，集成多 个GPU和专用硬件，支持大规模深度学习模型的训练和推 理。|深度学习、人工智能研 究和开发等领域。|\\n\\n\\n按照架构划分：\\n\\n|架构|年份|芯 片 代 号|特点|代表产品|\\n|---|---|---|---|---|\\n|Tesla|2006|GT|第一个通用并行计算架构，主要用于科学计算和 高性能计算。|Tesla C870/GeForce 8800 GTX|\\n|Fermi|2010|GF|引入CUDA架构、ECC内存等，用于科学计算、图 形处理和高性能计算。|Tesla C2050/GeForce GTX 480|\\n|Kepler|2012|GK|功耗效率和性能改进，引入GPU Boost技术，适 用于科学计算、深度学习和游戏。|Tesla K40/GeForce GTX 680|\\n|Maxwell|2014|GM|提高功耗效率，引入新技术如多层次内存系统， 应用于游戏、深度学习和移动设备。|Tesla M40/GeForce GTX 980|\\n|Pascal|2016|GP|16nm FinFET制程技术，加强深度学习和AI计算 支持，引入Tensor Cores，应用于深度学习和高 性能计算。|Tesla P100/GeForce GTX 1080|\\n\\n\\n-----\\n\\n**架构** **年份**\\n\\n|Col1|Col2|号|Col4|Col5|\\n|---|---|---|---|---|\\n|Volta|2017|GV|深度学习优化特性，如Tensor Cores，主要用于 深度学习、科学计算和高性能计算。|Tesla V100|\\n|Turing|2018|TU|实时光线追踪技术、深度学习技术，适用于游 戏、深度学习和专业可视化。|Tesla T4/GeForce RTX 2080 Ti|\\n|Ampere|2020|GA|第二代深度学习架构，更多Tensor Cores、改进 的Ray Tracing技术，应用于深度学习、科学计算 和高性能计算。|Telsa A100/GeForce RTX 3090|\\n|Ada Lovelace|2022|AD|专为光线追踪和基于AI的神经图形设计，第四代 Tensor Core，第三代RT Core，提高GPU性能。|GeForce RTX 4090|\\n|Hopper|2022|GH|下一代加速计算平台，支持PCIe 5.0，专用的 Transformer引擎，适用于大型语言模型和对话 AI，提供企业级AI支持。|Telsa H100|\\n\\n\\n\\n按照应用领域划分：\\n\\n\\n**特点** **代表产品**\\n\\n|类型|系列|描述|应用领域|代表产品|\\n|---|---|---|---|---|\\n|游戏娱 乐|GeForce RTX™系 列|面向大众消费级游戏和创作者用户的图形 加速卡。在性能、功耗和成本之间达到最 佳平衡点,提供极致的游戏和创作体验。|游戏、娱乐、 内容创作|如RTX 3090、RTX 4090等|\\n|专业设 计和虚 拟化|NVIDIA RTX™系 列|面向专业可视化和创意工作负载的高性能 GPU,提供强大的计算性能、大容量视频 内存等。服务于工业设计、建筑设计、影 视特效渲染等专业用户。|工业设计、建 筑设计、影视 特效渲染|高端专业可视 化工作站级显 卡|\\n|深度学 习、人 工智能 和高性 能计算|A系列、 H系列、 L系列、 V系列、 T系列|不同系列针对不同AI和计算需求。A系列 是AI计算加速器; H系列是AI超算; L系列 是边缘AI推理; V系列是虚拟工作站; T系 列是AI推理优化解决方案。|数据中心AI训 练和推理、边 缘AI、虚拟桌 面、AI推理加 速|A100、 A30、A40、 H100、 L40、 DeepStream 加速器等|\\n\\n\\n像大模型领域这种生成式人工智能，需要强大的算力来生成文本、图像、视频等内容。在这个背景下，\\n\\nNVIDIA先后推出V100、A100和H100等多款用于AI训练的芯片，其中 A100 是 H100 的上一代产品，于2020\\n\\n年发布，使用7纳米工艺，支持AI推理和训练。而H100，该显卡是2022年3月发布，可谓是核弹级性能显\\n\\n卡，采用了台机电4纳米工艺，具备800亿个晶体管，采用最新 Neda Hopper架构，同时显存还支持\\n\\nhbm3，最高带宽可达 3TB每秒。第四代MNLINK的带宽，900G每秒。是PCIE5.0的7倍，比上一代的A100显\\n\\n\\n-----\\n\\n飞跃，各项基础性能是A100的三倍之多，H100的单片显卡售价24万元左右。\\n\\n但在2022年10月，漂亮国政府为了限制我国的人工智能发展，发布禁令：禁止NVIDIA向中国出售A100\\n\\n和H100显卡。数据显示，2022年中国市场的人工智能芯片规模高达70亿美元，而这70亿的市场，被NVIDIA\\n\\n垄断了90%，虽然NVIDIA的A100，H100这样的顶级芯片不能卖给中国，但NVIDIA作为商业公司，也是要做\\n\\n生意的，于是为了合规，NVIDIA针对传输速率进行了限制，提出了中国大陆特供版的A800和H800，即：\\n\\nH100、A100的阉割版。\\n\\n也就是说，由于漂亮国的禁令，我们现在使用的GPU都是中国特供版的，说白了就是阉割版的，像\\n\\nA100，到国内就成了A800，H100到国内就成了H800，那么 A ~ H的差距在哪里呢？\\n\\n直接用 SXM 版本的H800进行对比，只能说这个参数对比，对于不了解的人来说，还是比较出人意料\\n\\n的，除了 FP64 和 NVLink传输速率上的明显削弱，其他参数和H100都是一模一样的。FP64上的削弱主要影\\n\\n响是H800在科学计算、流体计算、有限元分析等超算领域的应用，受到影响最大的还是NVLINK上的削减，\\n\\n但因为架构上的升级，虽然比不上同为 Hoper架构的H100，但是比AMPERRE架构的A100还是要强上不少，\\n\\n说白了，老黄想要抓住国内市场，就算是阉割，也不会阉割的特别过分，漂亮国政府想限制国内的超算，那\\n\\n就算把超算性能砍掉，传输速率减小，换个名字，GPU照卖。只要保证H800在大部分场景下的性能不受影\\n\\n响，能满足大部分人的使用需求就足够了。毕竟也不会有人跟钱过不去，所以 其实H800和 H 100的性能差\\n\\n距并没有想象的那么夸张，就算是砍掉了FP64和 NVLINK的传输速率，性能依旧够用。最关键的是，它合法\\n\\n呀。所以如果不是追求极致性能的话，也没必要冒着风险去选择H100。\\n\\n而就在今年的10月份，漂亮国又玩起了变卦，10月份刚升级了芯片禁令，开启了新一轮的出口管制，先\\n\\n预留了30天的窗口期，随后又要求立即生效，连30天都没了，也就是说，从10月份开始，中国将无法再获得\\n\\nNVIDIA5类的GPU显卡 （A800、A800、H100、A100，L40S），其实早在8月份的时候，BAT的一些大厂不\\n\\n知道是收到风声还是控制风险，就向NVIDIA提前订购了10万个A800芯片，结果这次也是彻底泡汤。其实从\\n\\n\\n-----\\n\\n的尖端AI芯片了，漂亮国就是亮牌，高端AI芯片，必禁无疑。所以对于目前的 A100系列和H100系列，因为\\n\\n是漂亮国断供之前出的芯片，所以现在国内还有货，只不过市场渠道比较乱，需要甄别。\\n\\n同时需要说明的是，GeForce 系列显卡虽被官方定位为面向消费级市场，适合游戏爱好者。但这类显卡\\n\\n在深度学习领域同样展现出了出色的性能，很多人用来做推理、训练，单张卡的性能跟深度学习专业卡Tesla\\n\\n系列比起来其实差不太多，但是性价比却高很多。对于大模型来说，同样可以使用GeForce 系列显卡。\\n\\n那么个人使用或者实验室针对大模型的推理和微调需求配置服务器，高端显卡目前我们可选的就是\\n\\nA100、A800、H100和4090等，应该如何选呢？\\n\\n## 2.3 单卡4090 vs A100系列\\n\\n先说结论：没有双精度需求，追求性价比，选4090。有双精度需求，选A100，没有A100选A800。如果\\n\\n**是做大模型的训练，GeForce RTX 4090 是不行的。但在执行推理（inference/serving）任务时，使用**\\n\\n**RTX 4090 不仅可行，而且在性价比方面甚至略优于 A100。同时如果做微调，也勉强是可以的，但建议多**\\n\\n**卡。**\\n\\n|GPU 型号|Tensor FP16 算力|Tensor FP32 算力|内存 容量|内存 带宽|通信 带宽|通信 时延|售价（美元）|\\n|---|---|---|---|---|---|---|---|\\n|H100|989 Tflops|495 Tflops|80 GB|3.35 TB/s|900 GB/s|~1 us|30000~40000|\\n|A100|312 Tflops|156 Tflops|80 GB|2 TB/s|900 GB/s|~1 us|15000|\\n|4090|330 Tflops|83 Tflops|24 GB|1 TB/s|64 GB/s|~10 us|1600|\\n\\n\\n\\n**推理**\\n\\n从数据对比来看，A100 和 GeForce RTX 4090 显卡在通信能力和内存容量方面存在显著差异，但在算力\\n\\n上差距并不大。在 FP16 算力方面，两者几乎相当，4090 甚至略有优势。相较于 A100，其较高的性价比主\\n\\n要源于推理过程通常涉及单一模型，在这种场景下，显卡的算力才是关键因素，而 4090 在这方面表现出\\n\\n色。虽然内存带宽同样重要，但在推理任务中，4090 的内存带宽通常足以应对需求，不会成为显著的制约\\n\\n因素。\\n\\n[LambdaLabs 有个很好的 GPU 单机训练性能和成本对比：https://lambdalabs.com/gpu-benchmarks](https://lambdalabs.com/gpu-benchmarks)\\n\\n， 我们来看：\\n\\n\\n-----\\n\\n高的。\\n\\n**微调**\\n\\n反观训练需求下，4090在训练的时候表现不佳的原因主要是其有限的通信能力和内存容量。比如训练\\n\\nLLaMA-2 70B 时需要2400块 A100 ，同时据说训练ChatGPT用了上万块 A100，主要还是因为训练过程除了\\n\\n存储模型参数外，还需要处理大量数据以及各层之间的中间数据和参数。因此，大容量内存和高通信带宽会\\n\\n比较关键，以便高效地处理和协调这些信息。首先就是把n个T的数据，分发到不同的GPU上去，然后训练，\\n\\n这叫数据并行。第二个并行就是会把这个模型的数据在一块GPU里可能放不下，所以要按照每一层，把某几\\n\\n层放在不同的GPU上面，进行串联。这就叫流水线并行。第三个就是Tensor张量并行。主要是我们目前训练\\n\\n的Transform模型都是多头的，每一个头都是可以按照张量来进行并行训练。所以整个 LLaMA-2 70B他会通\\n\\n过张量，流水线、数据三种并行方式，从模型内层，到模型层之间，和训练数据三个维度进行计算空间的划\\n\\n分。\\n\\n2400块GPU之间要进行大量的协调和通讯计算，这种复杂的并行结构需要 GPU 之间进行大量的协调和\\n\\n通信。4090 的通信带宽仅为 64 GB/s，与 A100 的 900 GB/s 相比差距过大，导致在这类大规模训练任务中\\n\\n通信成为瓶颈，进而影响整体性价比。因此，尽管 4090 在某些方面表现优秀，但在大模型训练中的局限性\\n\\n仍然明显。尽管微调过程对硬件的要求相较于训练相对较低，但这个过程仍然需要足够的内存以存储模型参\\n\\n数，以及有效的通信带宽来处理数据和模型层之间的交互。所以对于需要高通信带宽和大内存容量的大模型\\n\\n微调任务，A100等高端GPU可能是更合适的选择。\\n\\n我们拿 GPT 3 来说，GPT 3的参数将近700亿，假设每个参数使用4字节（通常使用float 32）进行存\\n\\n储，训练运算储备需求是 4200 GB，完成一次GPT 3训练的总算力是：3.15 * 10 ^23 Flops,仅考虑算力的情\\n\\n况下，单块 A100 需要45741天，几乎是128年（假设有效算力是78Tflpos），单块4090 需要91146天，几\\n\\n乎是250年，（假设有效算力是40 Tflpos）。任何一张单卡训练一次都需要超过100年，对于参数量达到10\\n\\n亿级别的大模型，参数本身就大，而且大模型通常需要更高的显存来来存储参数、中间计算结果和梯度等，\\n\\n既然要多卡运行，数据的同步效率就会显得非常重要，那么内存带宽、通信带宽、通信延时等性能将非常非\\n\\n常重要。，4090 24g的显存小，而且内存带块、通信贷款、和通信延时都相对较弱，这就有点像短板理论。\\n\\n最弱的那一项就决定了显卡的能力。综上，4090在较大的大模型没有什么发挥的余地，但随着现在的大模型\\n\\n越来越小，对显存和算力需求相对较小的大模型，再加上推理的算力需求更低，如LLama 7B 13B模型，单\\n\\n卡的4090 都可以运行，至少对于学习和研究大模型的个人或实验室来说，4090还真是不错的选择。\\n\\n⼤模型的⼯业级实践要求，⼤模型全量微调需要⾄少4张A100 80G显卡；（ChatGLM6B模型 全量微调\\n\\n差不多也是需要这个配置）\\n\\n## 2.4 单卡4090 vs 双卡3090\\n\\n如果预算差不多的情况下，对于两张3090与一张4090的选择，推荐使用两张3090显卡。虽然从算力角\\n\\n度看，两张3090与一张4090大致持平，但两张3090显卡提供的总显存会更多，这对于处理大型模型尤为重\\n\\n要。目前，大多数深度学习计算框架都支持各种并行计算技术，如流水线并行、张量并行和CPU卸载。这些\\n\\n技术使得即使是显存较小的显卡也能处理大型模型。在这种情况下，双3090配置可以更有效地利用流水线并\\n\\n行，同时，与单4090配置相比，CPU卸载的需求会降低。此外，企业环境一般都是多卡并行，使用双3090配\\n\\n置还可以练习如何写多卡代码。现有技术如串行反向传播，进一步增强了多卡系统的效率，使其成为一个经\\n\\n济高效的选择，尤其是在需要处理大量数据和复杂模型的情况下。因此，从目前的技术和应用需求来看，选\\n\\n择两张3090显卡无疑是更优的选择。\\n\\n## 2.5 风扇卡与涡轮卡如何选择\\n\\n\\n-----\\n\\n风扇卡与涡轮卡的供电接口位置不同，涡轮卡的供电接口位置在接口尾部，供电线比风扇卡的线\\n\\n更短，这样是方便安装和理线，而风扇卡供电接口一般在显卡顶部，接线后线缆会高于机箱最高\\n\\n面，在服务器中使用风扇卡，服务器盖板盖不上。\\n\\n在散热方向上面，涡轮卡散热方向是朝尾部散热，并于服务器风向是一致的，而风扇卡的散热是朝四面\\n\\n八方来散热的，平常的PC机箱放一张是可以适应的，但用作服务器上（很多时候是多卡）就不适合了，\\n\\n很容易因为温度过热出现宕机。\\n\\n风扇卡与涡轮卡的尺寸大小不同\\n\\n涡轮卡与风扇卡的尺寸大小也是不一样的，风扇卡的尺寸一般是2.5-3倍宽设计，而涡轮卡的尺寸\\n\\n大小是双宽设计，因为涡轮卡为了方便放入服务器里，所以涡轮卡的尺寸和高度都远远低于风扇\\n\\n卡，从而服务器可以支持4卡或者8卡，如果用风扇卡代替涡轮卡装在服务器里，那位置够不够还\\n\\n是一回事儿呢。\\n\\n面对市场不同\\n\\n风扇卡无论是公版显卡还是非公版显卡，风扇卡都是面向个人的，是应用在个人游戏行业的，\\n\\n4090风扇卡的特点就是外观炫酷，而个人游戏行业就是为了风扇卡的外观和玩游戏的性能。而\\n\\n4090涡轮卡是定制版，是面向AI科技产业，因为做工精巧、支持多卡安装、性价比高等一系列优\\n\\n点，4090涡轮卡深受广大AI深度学习用户的喜爱。\\n\\n## 2.6 整机参考配置\\n\\n确定GPU后，根据GPU搭配合适的计算机组件，具体来说，计算机八大件：CPU、散热器、主板、内\\n\\n存、硬盘、显卡、电源、风扇、机箱。个人使用的计算机，典型的配置是单GPU或双GPU，一般不超过四个\\n\\nGPU，否则常规的机箱放不下，且运行时噪声很大，而且容易跳闸。\\n\\n目前国内实验室主流的还是4090和3090,10万+的预算配置4张4090是没问题的，20~30万的预算则可以\\n\\n考虑8张4090，或者两张A100 80G，如果预算不限，A100 8卡服务器一定是最佳选择。\\n\\n这里给出一个本地部署ChatGLM-6B，同时也适用于大多数消费级实验环境的配置：\\n\\nGPU：3090双卡，涡轮版；总共48G显存，能够适⽤于⼤多数试验和复现性质深度学习任务；同时双卡\\n\\n也便于模拟多卡运⾏的⼯业级环境；\\n\\nCPU：AMD 5900X；12核24线程，模拟普通服务器多线程设置；\\n\\n存储：64G内存+2T SSD数据盘；内存主要考虑机器学习任务需求；\\n\\n电源：1600W单电源；双卡GPU的电源在1200W-1600W均可；\\n\\n主板：华硕ROG X570-E；服务器级PCE，⽀持双卡PCIE；\\n\\n机箱：ROG太阳神601；atx全塔式⼤机箱，便于⾼功耗下散热；\\n\\nA800 工作站的典型配置信\\n\\n|配置项|规格|\\n|---|---|\\n|CPU|Intel 8358P 2.6G 11.2UFI 48M 32C 240W *2|\\n|内存|DDR4 3200 64G *32|\\n\\n\\n\\n数据盘 960G 2.5 SATA 6Gb R SSD *2\\n\\n\\n-----\\n\\n|配置项|规格|\\n|---|---|\\n|硬盘|3.84T 2.5-E4x4R SSD *2|\\n|网络|双口10G光纤网卡（含模块）*1|\\n||双口25G SFP28无模块光纤网卡（MCX512A-ADAT ）*1|\\n|GPU|HV HGX A800 8-GPU 8OGB *1|\\n|电源|3500W电源模块*4|\\n|其他|25G SFP28多模光模块 *2|\\n||单端口200G HDR HCA卡(型号:MCX653105A-HDAT) *4|\\n||2GB SAS 12Gb 8口 RAID卡 *1|\\n||16A电源线缆国标1.8m *4|\\n||托轨 *1|\\n||主板预留PCIE4.0x16接口 *4|\\n||支持2个M.2 *1|\\n|原厂质保|3年 *1|\\n\\n\\n总的来说：\\n\\n3090⽐4090综合性价⽐更⾼，不过4090计算速度⼏乎是3090的两倍，有需求亦可考虑升级，不过\\n\\n4090需要的机箱空间更⼤、电源配置也要求更⾼；\\n\\n双卡GPU升级路线：3090—>4090—>A100 40G （2.5w左右）—>A100 80G（6~7w左右）；\\n\\n⼤模型的⼯业级实践要求，⼤模型全量微调需要⾄少4张A100 80G显卡；（ChatGLM6B模型 全量微调\\n\\n差不多也是需要这个配置）\\n\\n## 2.7 显卡博弈的形式分析\\n\\n除此之外，在2023年11月13日老黄又放出来两个核弹。第一个核弹就是它推出了全新的超算GPU\\n\\nH200，直接说是当世最强，听起来很嚣张，但其实一点没有吹牛。在AI超算领域，对手只有看NVIDIA车尾\\n\\n灯的份。从数据层面看，H200强在大模型推理上，以700亿参数的Llama2 二代大模型为例，h200推理速度\\n\\n几乎比前代的h100快了一倍。而且能耗还降低了一半。显存从h100的80GB，直接拉到了141gb，带宽也从\\n\\n3.35TB/s，提升到了4.8TB/s，最新的GPU H200，跟前一代H100相比，最大的提升就是它的内存，达到了\\n\\n惊人的1.15TB/s，相当于在1s内传输了 230步FHD的高清电影。如果每一部的容量按5G来算的话。这个跟我\\n\\n们以前的计算机里的内存条就不一样了，它采用的最新技术是HBM3e，HBM就是高带宽内存，这个实现是\\n\\n把DRAM内存用3D封装的技术叠了起来，然后把它和GPU芯片放在同一个GPU的底板上，它们之间的通信就\\n\\n通过这块晶元直接做了，这样内存的容量就大大提高了，同时他们和GPU的通信速度也有显著的增长，。达\\n\\n到了每秒钟4.8个TB。然后又把所有的软件做了优化，这样就使得ChatGPT这样 大模型的推理速度大大的提\\n\\n升，跟A100相比提高了 18倍。第二个核弹就是CPU和GPU的合体，GH200， 就是把ARM的CPU和它的GPU\\n\\n封装在了同一块GPU晶圆板上，这样CPU和GPU之间的传输速度就非常快，而且可以共享内存。内存也达到\\n\\n\\n-----\\n\\n有1/2。\\n\\n炸一听好像是王炸升级，刚装满h100的企业要哭晕在厕所了。但实际上，它可能只是h100的一个中期\\n\\n改款，单论峰值算力，H100和H200其实是一模一样的。，真正提升的是显存和带宽，然而对于AI芯片的性\\n\\n能，讨论最多的是训练能力。，在GPT 3 175B大模型的训练中，H200相较于H100，只强了10%，提升并不\\n\\n明显，这操作，大概率是老黄有意为之，以前为了打造大模型，对GPU的首要要求是训练，但是到了现在，\\n\\n随着各种AI大预言模型的落地，大家开始卷的是推理速度。于是H200的升级，就忽略了算力升级，转向推理\\n\\n方面的发力，老黄的刀法依旧精准。哪怕只是小提升，依然当得起最强的称号。谁让NVIDIA的显卡，在AI芯\\n\\n片这块，这就是遥遥领先。\\n\\n但这因为是断供后的新卡，国内现在基本买不到。\\n\\n在H200没出现以前，H100是地表最强GPU，NVIDIA每一个层级的性能基本都是翻倍的，H100，其中\\n\\n微软采购了 15w片，mate 采购了 15w片，谷歌、亚马逊、甲骨文、腾讯都是5w片，那么谷歌的gemini发布\\n\\n晚，原来是因为缺少GPU哈。一共是 48w片，和外界传的 一年H100的产量50w基本吻合。在2024年预计出\\n\\n货量在200万张。中国采购的用户 H800要比H100量大，而且H800的售价比H100还要高，为什么性能不行\\n\\n价格还是高呢，主要原因还是有一份建议国企采购产品中的文件，里面只有A800和H800，没有A100和\\n\\nH100，这就导致国企采购更愿意采购A800的原因，\\n\\n同时需要说的是，在今年的10月份，漂亮国再次禁用 H800、A800芯片后，NVIDIA计算再次推出中国特\\n\\n供AI芯片，初步计划是3款，分别是h20，L20和 L2，这三款基于H100进行阉割。使以性能符合禁令的要\\n\\n求。其中最强的是H20，但与H100相比，性能被封印了80%，只有H100的20%左右的性能，对于NVIDIA而\\n\\n言，中国这笔70亿美元的大市场肯定不能丢，必须推出AI芯片来抢占，不过，近日有消息传出，这三款特供\\n\\n版芯片要跳票了，只有L20可能会按期推出，H20和L2都可能延期。特别是 H20这个最强的，什么时候推\\n\\n出，会不会推出都是未知数，最重要的原因，还是目前中国的市场已经发生了变化。没有NVIDIA想像的那么\\n\\n美好了。\\n\\n有一些朋友可能还不知道这回事，而知道的朋友有些也不清楚，为什么偏偏是显卡会成为国际博弈的工\\n\\n具，这些卡一张卖 十几 甚至几十万元，这么赚钱的生意，怎么就不让做了。在1999年之前的人类文明早\\n\\n期，世界上是没有显卡的，但已经有了电子游戏了，那时候的游戏画面是由CPU生成的，游戏玩家说，需要\\n\\n有高画质，于是就有了显卡。1999年，NVIDIA声称自己发明了GPU，也就是 GFFORCE 256，所谓的GPU，\\n\\n就是图形计算单元，他是显卡最最核心的部件，再给他配上其他一系列零件，就成为了一张显卡。GPU跟显\\n\\n卡，严格来说不是一个概念，只是大家平时很少特意区分。这玩意为啥能提升游戏画质？因为渲染游戏画面\\n\\n这件事，难就难在计算量太大了，比如游戏中任何一个3D物体，它的位置、方向、大小、光源、物体表面等\\n\\n变化，都需要电脑来计算。\\n\\n渲染画面这件事，就像再做10000道加减乘除，CPU的核心很强，但数量少。每个核心就像出于一个智\\n\\n力巅峰的高三学生，他能熟练的解出模拟卷上的最后一道大题，但让他算 1000道，他得累死。而显卡上面\\n\\n密密麻麻的分布着几千个小核心，每个核心都像是一个小学生，高考题肯定是不会，但他们能同时并排启\\n\\n动，在10秒只能，把10000道题做完。所以渲染特别快。显卡能提升画质，就是因为他有这种强大的并行计\\n\\n算能力。而显卡从此脱离游戏领域，被别的领域盯上，乃至成为国际博弈的筹码。也是因为这种强大的并行\\n\\n计算能力，。发布了没几年，斯坦福大学实验室就盯上了显卡，它们想要显卡解决其他领域的问题，但是并\\n\\n不简单，显卡算力虽强，但是它们都是小学生，需要合适的软件能驾驭才行。沿用刚才的比方，GPU就是一\\n\\n万个小学生在同时工作，计算能力很强，但前提是你得能把一道难解的大题，分解成无数个小学生能解决的\\n\\n简单问题才行。否则显卡再强又有什么用呢？转换到现实中，就是得让开发者能方便的写出代码。利用上显\\n\\n卡的并行计算能力才行。所以在2006年，带领团队出现了至今仍然在不断更新的 CUDA，CUDA就是更方便\\n\\n的让开发人员能够面向 编程 如果绝大多数 模型的训练 背后都离 开 的支持 除了\\n\\n\\n-----\\n\\n并行计算能力，在软件层面，配套的编程平台也成熟了，这就意味着，GPU可以完全离开游戏领域，走向更\\n\\n大的世界了。\\n\\n第一次感受到GPU，就是挖矿，也就是挖比特币，挖矿其实就是用计算机 来解决数学问题，比如任何数\\n\\n据，都可以通过哈希算法生成一串哈希值，原始数据不管发生多小的变化，最后生成的哈希值都完全不同。\\n\\n我们有时候下载大文件时，也会利用哈希算法的这种特性，来做一次校验。。看看下载的文件是否完整。很\\n\\n多挖矿，就是要生成一个符合要求的哈希值，这就需要计算机去反复的尝试，所以挖矿就跟游戏画面一样，\\n\\n属于那种不难，但是计算量非常大的事情。恰好能够利用显卡的算力。于是在加密价格持续攀升的日子里，\\n\\n显卡涨价，缺货。一路推动NVIDIA的市值从140亿美元暴涨到了 1750亿美元。但显卡跟加密货币之间只是一\\n\\n段露水情缘，随着专用矿机的出现，虚拟比特币的价格跳水，以太坊等调整了挖矿规则等原因，显卡跟虚拟\\n\\n货币的关系已经大不如前了。但显卡就跟上了更大、更革命的科技浪潮，就是AI。现在所有人都知道，AI是\\n\\n可能引起新一轮科技革命的巨大产业，而几乎所有的AI模型训练，都需要显卡。\\n\\n就拿现在正火的ChatGPT来说，它的模型训练中涉及到大量的矩阵运算，这些矩阵运算本身不难，但是\\n\\n量很大很大，所以就需要GPU来并行处理。AI是可能改变世界的，而AI的基础是 算法、算力和数据。而提到\\n\\n的A100和H100，售价高到十几万、甚至几十万的专业显卡，还供不应求。有报道说，训练ChatGPT需要相\\n\\n当于300块A100显卡的算力，光这一个项目就需要花几十个亿来购买显卡，这也是为什么 从2022年10月开\\n\\n始，NVIDIA的市值在半年时间内就飙升了34倍。\\n\\n## 2.8 国产AI超算芯片期待\\n\\n这么着急赶尽杀绝，不惜拉上自己企业垫背。答案就是我国在半导体自主研发上，已经触碰到了他们的\\n\\n痛处。很多人总以为，我们依赖国外的AI芯片，是自身技术上不过关，其实真相是我们的芯片都没有真正的\\n\\n上过牌桌，为什么？ AI芯片只有在实际应用中才能够发现问题，加快迭代，而我们的国产芯片，起步晚、性\\n\\n能差，所以国内的厂商大多不愿意使用，这也就造成了国产芯片无法获得正向反馈，发展速度只会越来越\\n\\n慢，这是一个矛盾的循环。在还有选择的时候，考虑到性能也好，成本也好，中国企业往往愿意选择像\\n\\nNVIDIA的芯片，所以在很长一段时间，美国对中国的高端芯片的制裁，也不是彻底封死，因为国产的AI芯片\\n\\n不是它的对手，都还不够好用，但现在，局面彻底改变了。出口禁令全面升级，中国企业没有了其他的任何\\n\\n选择，下一步只能规模化的采购国产芯片，并且培育出本土的产业链，逐步实现真正意义上的国产替代。那\\n\\n可能有人说，这件事这么简单，能实现岂不是早就实现了。不太可能。其实还真不一定，放眼全球算力芯片\\n\\n市场，不可否认，NVIDIA占据了九成多的份额，出于高度垄断的地位。但是，目前国产AI芯片的可替代方\\n\\n案，也不少。\\n\\n如果单看并行计算这个领域，有两家国产GPU公司值得关注：分别是摩尔线程和壁任科技。\\n\\n摩尔线程2020年10月成立，在2023年10月17日，第一款产品摩尔芯用了7纳米工艺，支持CUDA平台和\\n\\n算法模型，性能超过每秒20万亿次浮点计算，仅成立三年就上了白宫严选名单。成为老美的制裁对象之一。\\n\\n是现在唯一可能买到的实际产品，并且一直在更新驱动的公司。壁任科技一款产品 壁任一号，7纳米工艺，\\n\\n支持CUDA平台和算法模型，性能超过每秒30万亿次浮点计算。去年发布了一个GPU 叫BR100，性能就直逼\\n\\n英伟达的H100，但是这个芯片并没有使用，原因还是台积电不给生产，准确的说是美国政府不让台积电给我\\n\\n们生产，这就是一个不公平的竞争，华为的遭遇大家就更熟悉了，19年以后 芯片的生产、制造全都被摁的死\\n\\n死的，但是麒麟芯片出来了，说明我们大概率已经突破了先进芯片制造的生产流程了，顶多就是成本高一\\n\\n点。\\n\\n这些当中除了华为之外都面临相同的问题，那就是没有针对芯片专门优化的计算架构，换句话说，这些\\n\\n公司不具备与CUDA抗衡的能力，如果能成为芯片供应商，去识别其他的计算架构，理论上也是可以的。但\\n\\n是对于使用者来说这样做效率就太低了。一旦训练十几天，一旦出现BUG，不就前功尽弃了吗。所以这事还\\n\\n是得看华为 华为就厉害了 在人工智能领域的布局包括但不限于昇腾系列计算芯片 CANN异构计算架\\n\\n\\n-----\\n\\n可能布局出全栈式人工智能的公司，也是唯一一个全栈式国产化公司，困扰华为的最大问题还是美国的制\\n\\n裁，芯片没法生产，再怎么布局也是白搭，一旦能解决芯片问题，\\n\\n有句话说的好，天下苦英伟达久矣。有时候看似把我们逼入绝境，但其实是给我了我们绝地求生的机\\n\\n会，就看我们怎么把握了。尽管中国企业会迎来一段痛苦的过渡期，但从长远来看，这是一条不得不走的\\n\\n路。而面对当下的巨大差距，最关键的是终于有了攻坚克难的凝聚力。相信度过这段阵痛期，有的人，有的\\n\\n国家，只会后悔的拍大腿，眼睁睁的看着被逆袭、被超越，毕竟每一次我们都是这样过来的。'},\n",
       " {'ModuleID': '96fe82ff-62b5-4a2d-bbe3-817854f96b22',\n",
       "  'Course': ['Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'URL': ['data/Ch 1 开源大模型本地部署硬件指南.pdf'],\n",
       "  'ModuleName': '三、组装计算机硬件选型策略',\n",
       "  'Tags': '3.1 GPU选型策略, 3.2 CPU选型策略, 3.3 散热选型策略, 3.5 硬盘选型策略, 3.7 电源选型策略, 3.8 机箱选型策略',\n",
       "  'Content': '计算机八大件：CPU、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于一套\\n\\n需要部署大模型的个人计算机，如何搭配。\\n\\n## 3.1 GPU选型策略\\n\\n1. 选择厂商\\n\\n目前独立显卡主要有AMD和NVIDIA两家厂商。其中NVIDIA在深度学习布局较早，对深度学习框架支持\\n\\n更好。建议选择NVIDIA的GPU。\\n\\n[桌面显卡性能天梯图：https://www.mydrivers.com/zhuanti/tianti/gpu/index.html](https://www.mydrivers.com/zhuanti/tianti/gpu/index.html)\\n\\n2. 选择系列及品牌\\n\\n**对个人用户来说，就是从NVIDIA的RTX系列中，选择出合适的GPU。就部署大模型的需求来说，只需**\\n\\n考虑计算能力和显存大小就可以了。显存带宽通常相对固定，选择空间较小。目前各级别显卡的均价如下：\\n\\n\\n-----\\n\\n|品牌|华硕|微星|技嘉|\\n|---|---|---|---|\\n|顶级旗舰||||\\n|旗舰|ROG猛禽|超龙X|大雕|\\n|次旗舰|TUF|魔龙|超级雕/小雕|\\n|中端|巨齿鲨|/|雪鹰/魔鹰|\\n|丐版|DUAL|万图师|猎鹰|\\n\\n\\n华硕显卡品控优做工好，高品牌信仰足，但溢价严重，这个牌子最会宣传，卡不错但性价比不是很高。\\n\\n高端优先推荐华硕ROG猛禽，当然缺点就是：贵，另外主流用户个人更推荐TUF，更低端的巨齿鲨和\\n\\nDUAL不太推荐\\n\\n微星显卡30系列之前更推荐魔龙，30系列更推荐超龙\\n\\n**准一线**\\n\\n|品牌|七彩虹|\\n|---|---|\\n|顶级旗舰|九段|\\n|旗舰|火神/水神|\\n|次旗舰|adoc|\\n|中端|ultra|\\n|丐版|战斧|\\n\\n\\n\\n推荐七彩虹，用料良心，保修出名的好，深受矿老板们的喜爱，支持个人送保，ultra，三风扇，散热性\\n\\n能极好，噪音小，白色颜值高，不带rgb灯效，喜欢rgb灯效的可以选择adoc。\\n\\n**二线**\\n\\n|品牌|影驰|索泰|映众|耕升|铭瑄|\\n|---|---|---|---|---|---|\\n|顶级旗舰||||||\\n|旗舰|名人堂|PGF/AMP|冰龙寒霜||MGG 大玩家|\\n|此旗舰|GAMER/星耀|天启|超级冰龙||电竞之心|\\n|中端|金属大师|/|冰龙|炫光/星极||\\n|丐版|黑将/大将|X-gaming|黑金至尊|追风|turbo/终结者|\\n\\n\\n\\n二线品牌中，影驰和索泰属于二线中实力比较强的两个，可以说是强二线，索泰重点推荐PGF（排骨\\n\\n\\n-----\\n\\n级别的产品，颜值高，性能强，次旗舰GAMER和星耀一个主打DIY一个主打RGB，都是非常有特点的产品\\n\\n企业级显卡\\n\\n参考第二部分的GPU推荐。\\n\\n服务器推断卡\\n\\n除了用于训练，还有一类卡是用于推断的（只预测，不训练），如：\\n\\n这些卡全部都是不带风扇的，但它们也需要散热，需要借助服务器强大的风扇被动散热，所以只能在专\\n\\n门设计的服务器上运行，性价比首选 Tesla T4，但是发挥全部性能需要使用 TensorRT 深度优化，目前仍然\\n\\n存在许多坑，比如当你的网络使用了不支持的运算符时，需要自己实现。\\n\\n**避免踩坑**\\n\\n如果选择配置单机多卡，采购显卡的时候，一定要注意买涡轮版的，不要买两个或者三个风扇的版本，\\n\\n除非只打算买一张卡。因为涡轮风扇的热是往外机箱外部吹的，可以很好地带走热量，如果买三个风扇的版\\n\\n本，插多卡的时候，上面的卡会把热量吹向第二张卡，导致第二张卡温度过高，影响性能。\\n\\n## 3.2 CPU选型策略\\n\\nCPU在大模型使用中起到什么作用？当在GPU上运行大模型时，CPU几乎不会进行任何计算。最有用的\\n\\n应用是数据预处理。CPU负责将数据从系统内存传输到GPU的显存中，同时也处理GPU完成计算后的数据。\\n\\n有两种不同的通用数据处理策略，具有不同的CPU需求。\\n\\n训练时处理数据：高性能的多核CPU能显著提高效率。建议每个GPU至少有4个线程，即为每个GPU分\\n\\n配两个CPU核心。每为GPU增加一个核心 ，应该获得大约0-5％的额外性能提升。\\n\\n训练前处理数据：不需要非常好的CPU。建议每个GPU至少有2个线程，即为每个GPU分配一个CPU核\\n\\n心。用这种策略，更多内核也不会让性能显著提升。\\n\\n在这种情况下，GPU通常承担大部分计算负担，CPU的作用更多是管理和协调，因此需要高核心数，同\\n\\n时也需要快速的数据预处理，同样需要高频率，所以高核心 + 高频率，虽然不是必须，但推我们推荐还是能\\n\\n**高即高，标准是：要与选择的GPU和CPU的性能水平相匹配，避免将一款高端显卡与低端CPU或一款高性能**\\n\\nCPU与低端显卡匹配，因为这可能导致性能瓶颈。比如：\\n\\nNVIDIA GeForce RTX 3090 * 2 搭配 Intel Core i7-13700K/KF 或 AMD 5900X CPU；\\n\\nNVIDIA GeForce RTX 3080 搭配 Intel Core i9-11900K CPU。\\n\\n但相对来说，瓶颈没有那么大，一般以一个GPU对应 2~4 个CPU核数就满足基本需求，比如单卡机器买\\n\\n四核CPU，四卡机器买十核CPU。在训练的时候，只要数据生成器（DataLoader）的产出速度比 GPU 的消\\n\\n耗速度快，那么 CPU 就不会成为瓶颈，也就不会拖慢训练速度。\\n\\n\\n-----\\n\\n去很长的一段时间了里，英特尔一直是绝对霸主的存在，代表最先进的生产力，时至今日，AMD在产品性能\\n\\n层面已经完全可以和Intel正面硬刚了。\\n\\n[CPU性能天梯图：https://www.mydrivers.com/zhuanti/tianti/cpu/index.html](https://www.mydrivers.com/zhuanti/tianti/cpu/index.html)\\n\\n**Intel 系列命名规范**\\n\\n可以通过CPU名称得到一些信息，如i7-10700K，代表产品型号是i7，后面的10代表是第10代，然后700\\n\\n代表性能等级高低，K代表这个CPU可以超频，当然后缀字母还有T、X、F等，X后缀代表高性能处理器，而T\\n\\n代表超低电压，/F代表无CPU无内置显卡版本。\\n\\n1. 系列：由低到高 Celeron（赛扬） / Pentium（奔腾） /酷睿系列的i3 / i5 / i7 / i9\\n\\n2. 世代：第1组数字代表是第几代\\n\\n例如这三个CPU：I7-8700、I7-9700，i7-10700第1个是第八代，第2个是第九代、第3个是第十代，还\\n\\n是比较容易理解的。\\n\\n3. 性能：第2组(3个数)是表示性能等级\\n\\n例如：I5-12400、I5-12500，数字越大表示越好。\\n\\n4. 后缀：K→可超频，F→没有核显\\n\\n可超频K版CPU要搭配可超频的Z系列主板才行，才可以超频，搭载不能超频的主板也可以用，只是不能\\n\\n超频了而已。\\n\\n没有核显的F版CPU要搭配独立显卡才能开机点亮屏幕\\n\\n超频简单理解就是可以提升处理器性能，核显就是把一张入门级的显卡塞进处理器里。\\n\\n**i3是家用级别，i5是游戏级别，i7是生产力和游戏发烧友级别，i9是最顶级的。后缀带K可以超频，带F**\\n\\n**表示没有核显。**\\n\\n**AMD系列命名规范**\\n\\n和Intel类似：\\n\\n1. 系列：由低到高 APU / Althlon（速龙） / Ryzen（锐龙）系列 R3 / R5 / R7 / R9\\n\\n2. 世代：第1个数字代表第几代\\n\\n3. 例如这两个CPU：R7-2700X、R7-3700X，第1个是第二代，第2个是第三代。\\n\\n4. 性能：第2组数字（3个数字）表示性能等级\\n\\n数字越大性能越好，例如 R7 3800X的性能大于R7 3700X。\\n\\n5 后缀：字母G表示有核显 字母X没有明确意思 一般性能强一点 如R5 3600X比R5 3600性能高一\\n\\n\\n-----\\n\\n**要选Intel还是AMD，其实都可以。如果追求性价比，AMD性价比高一些，如果主要玩游戏，且对价格**\\n\\n不敏感，建议选择英特尔Intel，英特尔Intel一般主频较高，一些游戏主要依赖主频，所以高主频的Intel玩游\\n\\n戏更推荐一些。除了品牌维度的分析，目前主流的大模型训练硬件通常采用 Intel + NVIDIA GPU。但具体\\n\\n情况具体分析，只能简单说：一分钱一分货，一般来说贵的好。\\n\\n**选购CPU误区**\\n\\n电子产品有一个说法是，“买新不买旧”，一般新产品，会用更新的工艺架构，性能更强，功耗更低，比\\n\\n较值得购买。当然有些时候，老一代的性价比很高，也可以考虑，但如果是好几代前的老产品，就不要考虑\\n\\n了，有些商家会卖几年前的i7电脑主机，它的性能可能还不如最新的i3，主要是忽悠小白的，要注意辨别。\\n\\n目前消费级市场，我们最常听到的i3 i5 i7 等，是英特尔的酷睿系列产品，主要面向一般消费市场，数字\\n\\n大的性能更强，（注意这里只在同代产品中成立）。AMD与之对应的是R3 R5 R7。这里值得注意是，同代产\\n\\n品i7比i5强，如果拿老一代的i7和新一代的i5比，就未必成立，部分商家经常会营销i5免费升级i7，其实是把\\n\\n最新一代的i5换成立了老一代的i7，性能方面可能还不如没升级呢？比如i5-8400的性能就高于i7-7700.\\n\\n## 3.3 散热选型策略\\n\\nCPU 不断地更新换代和性能提升，其功耗和发热量也越来越大，如果温度过高，就会出现自动关机或者\\n\\n是蓝屏死机等情况，所以需要单独的散热器来压制，目前CPU散热器分两种：水冷和风冷。\\n\\n风冷和水冷系统都是用于GPU的散热解决方案。它们各有优势和不足：通常，水冷系统在散热效率方面\\n\\n**优于风冷系统。以Intel的i9-13900KF为例，这款CPU性能目前位于CPU性能天梯榜第二位，很多用户认为使**\\n\\n用水冷系统是必要的。但如果这款CPU没有超频需求，使用高质量的风冷系统其实也能够有效地散热。只有\\n\\n在超频的情况下，水冷系统由于其更出色的冷却效果，才成为更佳的选择。\\n\\n但需要注意，风冷和水冷与GPU无关。在计算机硬件中，CPU和GPU（显卡）的散热策略和要求各有不\\n\\n同。CPU通常需要配单独的散热器，我们可以根据需要选择并购买不同类型的散热器，例如水冷或风冷系\\n\\n统，并且可以根据性能要求进行升级。由于CPU的高主频和较少的核心数（通常是几个到二十几个核心），\\n\\n高性能的CPU在运行时会产生较多热量，因此需要更有效的散热解决方案来维持合适的运行温度。与此相\\n\\n对，显卡通常采用一体化的散热设计，其散热器是显卡的重要组成部分。显卡散热器的设计已经由各厂商经\\n\\n过测试和优化，因此用户一般不需要担心显卡的散热问题。显卡采用的是多核心、低频率的策略，即使是高\\n\\n端显卡如Nvidia的4090，其频率也相对较低，通常在3000MHz左右，而同代的高端CPU（如Intel i9）的频\\n\\n率可能是其两倍。显卡的散热器可以直接接触GPU核心和显存，从而高效散热。因此，在正常满载情况下，\\n\\n显卡的温度达到70多或80多度是正常现象，通常不会成为性能瓶颈。\\n\\n对于大模型部署来说，首要原则还是CPU的等级要和GPU相匹配。对于中低端处理器，如Intel的i5系\\n\\n列，以及AMD的R5和R7系列，一般推荐使用风冷系统。这些处理器的热设计功耗（TDP）通常较低，风冷系\\n\\n统足以提供有效的散热。而对于更高性能的处理器，如Intel的i7 13700KF及更高级别的i7和i9系列，建议至\\n\\n少使用240mm规格的水冷系统。考虑到这些处理器较高的性能和热输出，水冷系统能提供更为高效和稳定\\n\\n的冷却效果。因此，尽管风冷在某些情况下依然可行，但为了确保最佳性能和稳定性，对于高性能处理器，\\n\\n\\n-----\\n\\n在构建大模型的系统时，低端主板通常不适用。根据所选的CPU和GPU规格，应从中端或高端主板中选\\n\\n择出合适的。\\n\\n|制造 商|系列|定 位|支持CPU超 频|支持内存超 频|适用用户|\\n|---|---|---|---|---|---|\\n|Intel|Z系 列|高 端|是|是|追求高性能和定制化设置的用户|\\n|Intel|B系 列|中 端|否|是|需要一定性能但预算有限的用户|\\n|Intel|H系 列|低 端|否|否|预算有限或对性能要求不高的基本用 途|\\n|AMD|X系 列|高 端|是|是|追求最高性能和高度定制化的用户|\\n|AMD|B系 列|中 端|否|是|寻求性价比的用户|\\n|AMD|A系 列|低 端|否|否|有限预算或基本计算需求的用户|\\n\\n\\n\\n选择主板时，核心因素是确保它与CPU的性能和超频能力相匹配。以Intel处理器为例：对于中高端CPU\\n\\n（如i5系列及以上），更适合选择B660到Z690系列的主板。对于如13600KF这样的高性能CPU，至少应选择\\n\\nB660系列的主板作为起点。需要考虑的是CPU是否支持超频（如带有“K”后缀）。可超频的CPU更适合搭配\\n\\n支持超频的高端主板，如Z系列\\n\\n其次，需要检查CPU和主板型号是否匹配及合理。\\n\\n通常情况下，每一种型号的CPU都需要搭配对应型号的主板，每代CPU和主板都有自己的针角及接口类\\n\\n型，Intel cpu不能用于AMD系列主板，某些主板可能会通用几代cpu，但有的主板只能兼容某一代，例如\\n\\nintel 十代 的i510400f，不能用于早期的四代 b85系列主板，而是否匹配，指的是高性能CPU搭配低性能主\\n\\n板，h610是入门主板，虽然可以点亮，但是低端主板因为供电有限，无法发挥出cpu的全部性能，以及无法\\n\\n超频，这样就失去了cpu本身的性能和意义。\\n\\n最后，考虑PCIe通道。\\n\\nPCIe通道是一种高速接口，用于将GPU连接到计算机的主板。通过这些通道，GPU可以与CPU以及系统\\n\\n内存快速交换数据。每个PCIe通道（或称为“通道”）都提供一定的数据传输带宽。更多的通道意味着更高的\\n\\n总体带宽。例如，PCIe 3.0 x16接口意味着有16个通道，每个通道的速度是PCIe 3.0标准的速度。\\n\\nGPU的性能部分取决于它与主板之间的通信速度，这是由PCIe通道的数量和版本（如PCIe 3.0、4.0或\\n\\n5.0）决定的。更高版本的PCIe提供更高的传输速率，从而可能提高GPU的性能。以下是需要考虑的几个关键\\n\\n点：\\n\\n1. PCIe版本：\\n\\n\\n-----\\n\\n4.0和5.0）提供更高的数据传输速率，这对于高性能GPU和其他高速设备非常重要。\\n\\n2. PCIe槽数量和布局：\\n\\n主板上的PCIe槽数量决定了可以安装多少个扩展卡。如果计划安装多个GPU或其他PCIe设备，需\\n\\n要确保主板有足够的槽位。\\n\\n槽位布局也很重要，尤其是在安装大型GPU时，需要确保它们之间有足够的空间，避免过热或物\\n\\n理干扰。\\n\\n3. PCIe通道分配：\\n\\n主板上的PCIe通道是从CPU和芯片组分配的。不同的主板可能有不同的通道分配方式，这可能会\\n\\n影响到扩展卡的性能，特别是在多GPU配置中。\\n\\n确认主板是否支持您所需的PCIe配置，例如双向或四向GPU设置。\\n\\n4. 与GPU的兼容性：\\n\\n虽然大多数现代GPU兼容大多数主板的PCIe槽，但是为了最佳性能，最好确认GPU与主板的PCIe\\n\\n版本相匹配。\\n\\n综上所述，因为需要通过PCIe通道连接和使用GPU，因此在选择主板时考虑PCIe通道的版本、数量、布\\n\\n局和通道分配非常重要。\\n\\n## 3.5 硬盘选型策略\\n\\n**首先考虑接口类型。主流固态硬盘主要有两种接口：SATA和M.2。**\\n\\n**SATA接口的固态硬盘体积较大，形状类似于传统的机械硬盘，主要用于升级老式电脑，因为这些电脑**\\n\\n通常不具备M.2接口。SATA接口硬盘的最高速度为600MB/s。\\n\\n**M.2接口的硬盘则较小，可以直接安装在主板上的专用接口。它们采用新的硬盘协议，速度上可以达到**\\n\\n4GB/s。\\n\\n**推荐选择 M.2接口的硬盘。**\\n\\n**然后考虑协议。M.2接口的固态硬盘分为SATA协议和NVMe协议两种。**\\n\\nM.2接口的SATA协议硬盘速度较慢，实际上就是标准SATA硬盘的形状变化，速度仍然是最高\\n\\n600MB/s，这类硬盘多用于旧电脑。\\n\\n**NVMe协议硬盘则速度更快，适合对速度有较高要求的应用。**\\n\\n在面对大量小文件的时候，使用 NVMe 硬盘可以一分钟扫完 1000万文件，如果使用普通硬盘，那么就\\n\\n需要一天时间。推荐选择 NVME协议的M.2接口的硬盘。\\n\\n**最后考虑 PCIe等级。当前市面上最新的是PCIe 5.0，但更常见的是PCIe 3.0和PCIe 4.0。PCIe等级越**\\n\\n高，硬盘的速度潜力越大。但重要的是检查主板是否支持相应的PCIe等级。例如，一些主板可能最高只支持\\n\\n到PCIe 4.0。一般来说，选择PCIe 4.0的即可。\\n\\n硬盘不会限制深度学习任务的运行，但如果小看了硬盘的作用，可能会让你追、悔、莫、及。想象一\\n\\n下，如果你从硬盘中读取的数据的速度只有100MB/s，那么加载一个32张ImageNet图片构成的mini\\n\\n-----\\n\\n**建议内存容量应大于GPU的显存。例如，对于搭载单卡GPU的系统，建议配置至少16GB内存。如果是**\\n\\n四卡GPU系统，则建议至少配置64GB内存。由于数据生成器（DataLoader）的存在，数据不需要全部加载\\n\\n到内存中，因此内存通常不会成为性能瓶颈。\\n\\n内存不用太纠结，是GPU显存的一到两倍。目前，128G 就可以，64G 也凑合。而且内存没那么贵，可\\n\\n以配满。\\n\\n内存大小不会影响深度学习性能，但是它可能会影响你执行GPU代码的效率。内存容量大一点，CPU就\\n\\n可以不通过磁盘，直接和GPU交换数据。所以应该配备与GPU显存匹配的内存容量。\\n\\n在选择的时候，注意检查主板是否支持内存的数量及型号。目前常见的 ddr3 ~ 5，每一代内存都需要对\\n\\n应主板的插槽类，ddr 5代内存 是无法混插在 ddr 4代内存上的。另外需要确定主板的内存插槽数量，如果只\\n\\n有两个插槽，买了四个，那么根本插不进去。\\n\\n其次检查cpu主板是否支持内存频率。内存条的频率上限 受到 cpu 和主板的控制， 例如 i5的 10400f +\\n\\nb460主板 = 2666，如果你买的内存是 3600频率的，无疑发挥不出内存本身的优势。\\n\\n## 3.7 电源选型策略\\n\\n在选择电脑电源时，需要检查电源的瓦数是否足以支持整机的功耗。并非越高瓦数越好，但瓦数过低可\\n\\n能会导致关机或黑屏等问题。在确定合适的电源瓦数之前，应综合评估整机硬件的功耗，尤其要考虑CPU和\\n\\n显卡这两个功耗大户。通常，将CPU和显卡的TDP功耗相加后乘以2可以得到一个合适的电源瓦数估计。例\\n\\n如，对于一个65W的CPU和125W的显卡，合适的电源瓦数应该在400W或450W左右。\\n\\n双卡最好1000W以上，四卡最好买1600W的电源\\n\\n## 3.8 机箱选型策略\\n\\n最后，选择完所有上述所有配件之后，选择机箱就相对简单，只需要确保这个机箱足够宽敞，能够容纳\\n\\n所选的所有配件。我们需要检查以下几项内容：\\n\\n1. 核对主板与机箱尺寸匹配性：\\n\\n确保所选主板的大小与机箱兼容。例如，ITX主板应与ITX机箱相匹配。这就像选择合适大小的鞋子\\n\\n一样重要。\\n\\n2. 确认机箱支持显卡尺寸：\\n\\n对比显卡的长度与机箱对显卡长度的限制。建议选择机箱的显卡限长至少比显卡长度长出30毫米\\n\\n以上，以确保有足够空间进行安装和通风。\\n\\n3. 检查散热器与机箱的兼容性：\\n\\n非常重要的一点是比较散热器的尺寸与机箱的散热限高。如果散热器太高，可能无法正常安装侧\\n\\n盖。\\n\\n考虑到许多内存条配备较高的散热马甲，需要确认散热风扇是否会受到内存条的干扰。\\n\\n如果选择水冷散热系统，确保机箱的水冷位能够容纳水冷冷排的尺寸。例如，360mm的水冷冷排\\n\\n无法安装在仅适用于240mm的位置上。\\n\\n\\n-----\\n\\n常见的电源类型包括SFX、ATX和TFX。由于不同规格的电源在形状和大小上有所不同，必须确认\\n\\n机箱的电源仓是否适合所选电源的尺寸。\\n\\n\\n-----'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63198bff-a70c-46e8-a271-be3b93715876",
   "metadata": {},
   "source": [
    "&emsp;&emsp;保存为本地的.csv文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02b0c666-2d2f-4e8d-8a72-311179853a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录: /root/project/yanshi\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 打印当前工作目录\n",
    "print(\"当前工作目录:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98ae08ff-1cb4-46c9-9840-afc1a19ccb80",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from pandas) (2024.1)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m72.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hUsing cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 tzdata-2024.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9c34558-e822-448b-b16b-f297baf43185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "csv_file_path = os.path.join('.', 'output.csv')\n",
    "\n",
    "# 保存 DataFrame 到 CSV 文件\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f8511f-48b4-4395-827c-2d1ffcb763e9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们要做两件事：\n",
    "1. 依次遍历这个data数据的 Content 字段，调用一个大模型，对这段文本生成摘要，然后填充一个新的字段：Abstract\n",
    "2. 同时判断 Tags 是否为空，如何为空的话，提取 URL 相同的 Tags中的内容作为提示，让大模型生成Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4db4799c-6bf6-479e-87ad-1b9a1a569d14",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zhipuai\n",
      "  Downloading zhipuai-2.1.5.20230904-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting cachetools>=4.2.2 (from zhipuai)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from zhipuai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.9.0 in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from zhipuai) (2.9.1)\n",
      "Requirement already satisfied: pydantic-core>=2.14.6 in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from zhipuai) (2.23.3)\n",
      "Collecting pyjwt<2.9.0,>=2.8.0 (from zhipuai)\n",
      "  Using cached PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: anyio in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from httpx>=0.23.0->zhipuai) (4.4.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from httpx>=0.23.0->zhipuai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from httpx>=0.23.0->zhipuai) (1.0.5)\n",
      "Requirement already satisfied: idna in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from httpx>=0.23.0->zhipuai) (3.7)\n",
      "Requirement already satisfied: sniffio in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from httpx>=0.23.0->zhipuai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from httpcore==1.*->httpx>=0.23.0->zhipuai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from pydantic<3.0,>=1.9.0->zhipuai) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from pydantic<3.0,>=1.9.0->zhipuai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/py38/lib/python3.8/site-packages (from anyio->httpx>=0.23.0->zhipuai) (1.2.1)\n",
      "Downloading zhipuai-2.1.5.20230904-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m626.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: pyjwt, cachetools, zhipuai\n",
      "Successfully installed cachetools-5.5.0 pyjwt-2.8.0 zhipuai-2.1.5.20230904\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install zhipuai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b80b2b7f-8922-48c7-bbf9-87ed6dc6463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"智谱未来，开放共赢 —— 携手创造无限可能！\"\n"
     ]
    }
   ],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "client = ZhipuAI(api_key=\"2c397e8089af28761792c52d334e92c3.fZQHXOprjyAR3Xs7\") # 填写您自己的APIKey\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",  # 填写需要调用的模型名称\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"作为一名营销专家，请为智谱开放平台创作一个吸引人的slogan\"},\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message.content)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5657b95-cb29-4eb1-ad54-582802736130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from zhipuai import ZhipuAI\n",
    "\n",
    "\n",
    "client = ZhipuAI(api_key=\"2c397e8089af28761792c52d334e92c3.fZQHXOprjyAR3Xs7\")\n",
    "\n",
    "# 假设用于生成摘要的函数\n",
    "def generate_summary(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",  # 填写需要调用的模型名称\n",
    "        messages=[{\"role\": \"user\", \"content\": \"作为一名语言学专家，请根据如下的输入文本:\\n{} \\n生成一段摘要\".format(text)}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# 假设这是你用于生成 Tags 的函数\n",
    "def generate_tags(prompt):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"glm-4\",  # 填写需要调用的模型名称\n",
    "        messages=[{\"role\": \"user\", \"content\": \"作为一名语言学专家，请根据类似的示例:\\n{} \\n生成这段文本的关键性标签，注意：一定要简短且能抓住重点\".format(prompt)}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d52e39db-2386-46fc-af1a-19cad0759238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 CSV 文件\n",
    "df = pd.read_csv(\"output.csv\")\n",
    "\n",
    "# 遍历 DataFrame，生成摘要\n",
    "df['Abstract'] = df['Content'].apply(generate_summary)\n",
    "\n",
    "# 检查并填充空的 Tags\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['Tags']) or row['Tags'].strip() == \"\":\n",
    "        # 查找具有相同 URL 的其他记录的 Tags\n",
    "        similar_tags = df[df['URL'] == row['URL']]['Tags'].dropna()\n",
    "        if not similar_tags.empty:\n",
    "            tag_prompt = similar_tags.iloc[0]  # 使用第一个非空标签作为提示\n",
    "            generated_tags = generate_tags(tag_prompt)\n",
    "            df.at[index, 'Tags'] = generated_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "057a7ab5-5324-4722-bbfb-beda8c22786d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ModuleID</th>\n",
       "      <th>Course</th>\n",
       "      <th>URL</th>\n",
       "      <th>ModuleName</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Content</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3fb9c842-26df-4c66-a865-030ffbc05ba3</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['data/Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>本地部署开源大模型</td>\n",
       "      <td>Ch.1 如何选择合适的硬件配置</td>\n",
       "      <td>## Ch.1 如何选择合适的硬件配置\\n\\n为了在本地有效部署和使用开源大模型，深入理解硬...</td>\n",
       "      <td>本段文本主要介绍了为了有效部署和使用开源大模型，理解和掌握硬件与软件需求的重要性。建议使用高...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a4d419bf-68cd-47a7-9efb-bbdef5b8756e</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['data/Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>一、大模型应用需求分析</td>\n",
       "      <td>\"硬件选择指南\" 或 \"配置优化要点\"</td>\n",
       "      <td>大模型的本地部署主要应用于三个方面：训练（train）、高效微调（fine-tune）和推理...</td>\n",
       "      <td>本地部署大型模型主要涉及训练、高效微调和推理三个环节，其中算力消耗依次递减。由于从头训练大模...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ad5870ff-83e3-4858-b9c5-613e7f2ad695</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['data/Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>二、硬件配置的选择标准</td>\n",
       "      <td>2.1 选择满足显存需求的 GPU, 2.2 主流显卡性能分析, 2.3 单卡4090 vs...</td>\n",
       "      <td>无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\\...</td>\n",
       "      <td>本文介绍了选择GPU时需要考虑的因素，包括显存容量、算力、内存带宽等。NVIDIA的GPU在...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96fe82ff-62b5-4a2d-bbe3-817854f96b22</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['data/Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>三、组装计算机硬件选型策略</td>\n",
       "      <td>3.1 GPU选型策略, 3.2 CPU选型策略, 3.3 散热选型策略, 3.5 硬盘选型...</td>\n",
       "      <td>计算机八大件：CPU、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于一套...</td>\n",
       "      <td>本文详细介绍了如何配置一台用于部署大模型的个人计算机，包括GPU、CPU、散热器、主板、内存...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ModuleID                      Course  \\\n",
       "0  3fb9c842-26df-4c66-a865-030ffbc05ba3  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "1  a4d419bf-68cd-47a7-9efb-bbdef5b8756e  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "2  ad5870ff-83e3-4858-b9c5-613e7f2ad695  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "3  96fe82ff-62b5-4a2d-bbe3-817854f96b22  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "\n",
       "                               URL     ModuleName  \\\n",
       "0  ['data/Ch 1 开源大模型本地部署硬件指南.pdf']      本地部署开源大模型   \n",
       "1  ['data/Ch 1 开源大模型本地部署硬件指南.pdf']    一、大模型应用需求分析   \n",
       "2  ['data/Ch 1 开源大模型本地部署硬件指南.pdf']    二、硬件配置的选择标准   \n",
       "3  ['data/Ch 1 开源大模型本地部署硬件指南.pdf']  三、组装计算机硬件选型策略   \n",
       "\n",
       "                                                Tags  \\\n",
       "0                                   Ch.1 如何选择合适的硬件配置   \n",
       "1                                \"硬件选择指南\" 或 \"配置优化要点\"   \n",
       "2  2.1 选择满足显存需求的 GPU, 2.2 主流显卡性能分析, 2.3 单卡4090 vs...   \n",
       "3  3.1 GPU选型策略, 3.2 CPU选型策略, 3.3 散热选型策略, 3.5 硬盘选型...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  ## Ch.1 如何选择合适的硬件配置\\n\\n为了在本地有效部署和使用开源大模型，深入理解硬...   \n",
       "1  大模型的本地部署主要应用于三个方面：训练（train）、高效微调（fine-tune）和推理...   \n",
       "2  无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\\...   \n",
       "3  计算机八大件：CPU、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于一套...   \n",
       "\n",
       "                                            Abstract  \n",
       "0  本段文本主要介绍了为了有效部署和使用开源大模型，理解和掌握硬件与软件需求的重要性。建议使用高...  \n",
       "1  本地部署大型模型主要涉及训练、高效微调和推理三个环节，其中算力消耗依次递减。由于从头训练大模...  \n",
       "2  本文介绍了选择GPU时需要考虑的因素，包括显存容量、算力、内存带宽等。NVIDIA的GPU在...  \n",
       "3  本文详细介绍了如何配置一台用于部署大模型的个人计算机，包括GPU、CPU、散热器、主板、内存...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ba39d54-affa-49c5-8e2e-8b244d721efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存更新后的 DataFrame 到新的 CSV 文件\n",
    "df.to_csv(\"updated_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29e225eb-d50e-45a9-945d-8fc190d7097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_course = pd.read_csv(\n",
    "    'updated_output.csv',\n",
    "    sep=',',\n",
    "    header=0)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a72c57-8ed8-45a7-9a1b-28494b51de07",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后，新增embedding_info列，用于构建Embedding向量表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b008398a-13a3-458a-9877-2d0482cbe7f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ModuleID</th>\n",
       "      <th>Course</th>\n",
       "      <th>URL</th>\n",
       "      <th>ModuleName</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Content</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>embedding_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3fb9c842-26df-4c66-a865-030ffbc05ba3</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['data/Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>本地部署开源大模型</td>\n",
       "      <td>Ch.1 如何选择合适的硬件配置</td>\n",
       "      <td>## Ch.1 如何选择合适的硬件配置\\n\\n为了在本地有效部署和使用开源大模型，深入理解硬...</td>\n",
       "      <td>本段文本主要介绍了为了有效部署和使用开源大模型，理解和掌握硬件与软件需求的重要性。建议使用高...</td>\n",
       "      <td>{ \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a4d419bf-68cd-47a7-9efb-bbdef5b8756e</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['data/Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>一、大模型应用需求分析</td>\n",
       "      <td>\"硬件选择指南\" 或 \"配置优化要点\"</td>\n",
       "      <td>大模型的本地部署主要应用于三个方面：训练（train）、高效微调（fine-tune）和推理...</td>\n",
       "      <td>本地部署大型模型主要涉及训练、高效微调和推理三个环节，其中算力消耗依次递减。由于从头训练大模...</td>\n",
       "      <td>{ \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ad5870ff-83e3-4858-b9c5-613e7f2ad695</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['data/Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>二、硬件配置的选择标准</td>\n",
       "      <td>2.1 选择满足显存需求的 GPU, 2.2 主流显卡性能分析, 2.3 单卡4090 vs...</td>\n",
       "      <td>无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\\...</td>\n",
       "      <td>本文介绍了选择GPU时需要考虑的因素，包括显存容量、算力、内存带宽等。NVIDIA的GPU在...</td>\n",
       "      <td>{ \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96fe82ff-62b5-4a2d-bbe3-817854f96b22</td>\n",
       "      <td>['Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>['data/Ch 1 开源大模型本地部署硬件指南.pdf']</td>\n",
       "      <td>三、组装计算机硬件选型策略</td>\n",
       "      <td>3.1 GPU选型策略, 3.2 CPU选型策略, 3.3 散热选型策略, 3.5 硬盘选型...</td>\n",
       "      <td>计算机八大件：CPU、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于一套...</td>\n",
       "      <td>本文详细介绍了如何配置一台用于部署大模型的个人计算机，包括GPU、CPU、散热器、主板、内存...</td>\n",
       "      <td>{ \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ModuleID                      Course  \\\n",
       "0  3fb9c842-26df-4c66-a865-030ffbc05ba3  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "1  a4d419bf-68cd-47a7-9efb-bbdef5b8756e  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "2  ad5870ff-83e3-4858-b9c5-613e7f2ad695  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "3  96fe82ff-62b5-4a2d-bbe3-817854f96b22  ['Ch 1 开源大模型本地部署硬件指南.pdf']   \n",
       "\n",
       "                               URL     ModuleName  \\\n",
       "0  ['data/Ch 1 开源大模型本地部署硬件指南.pdf']      本地部署开源大模型   \n",
       "1  ['data/Ch 1 开源大模型本地部署硬件指南.pdf']    一、大模型应用需求分析   \n",
       "2  ['data/Ch 1 开源大模型本地部署硬件指南.pdf']    二、硬件配置的选择标准   \n",
       "3  ['data/Ch 1 开源大模型本地部署硬件指南.pdf']  三、组装计算机硬件选型策略   \n",
       "\n",
       "                                                Tags  \\\n",
       "0                                   Ch.1 如何选择合适的硬件配置   \n",
       "1                                \"硬件选择指南\" 或 \"配置优化要点\"   \n",
       "2  2.1 选择满足显存需求的 GPU, 2.2 主流显卡性能分析, 2.3 单卡4090 vs...   \n",
       "3  3.1 GPU选型策略, 3.2 CPU选型策略, 3.3 散热选型策略, 3.5 硬盘选型...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  ## Ch.1 如何选择合适的硬件配置\\n\\n为了在本地有效部署和使用开源大模型，深入理解硬...   \n",
       "1  大模型的本地部署主要应用于三个方面：训练（train）、高效微调（fine-tune）和推理...   \n",
       "2  无论是个人使用、科研团队进行项目研究，还是企业寻求商业应用的落地，不同的应用场景和目标任务\\...   \n",
       "3  计算机八大件：CPU、散热器、主板、内存、硬盘、显卡、电源、风扇、机箱，我们依次来看对于一套...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  本段文本主要介绍了为了有效部署和使用开源大模型，理解和掌握硬件与软件需求的重要性。建议使用高...   \n",
       "1  本地部署大型模型主要涉及训练、高效微调和推理三个环节，其中算力消耗依次递减。由于从头训练大模...   \n",
       "2  本文介绍了选择GPU时需要考虑的因素，包括显存容量、算力、内存带宽等。NVIDIA的GPU在...   \n",
       "3  本文详细介绍了如何配置一台用于部署大模型的个人计算机，包括GPU、CPU、散热器、主板、内存...   \n",
       "\n",
       "                                      embedding_info  \n",
       "0  { \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...  \n",
       "1  { \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...  \n",
       "2  { \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...  \n",
       "3  { \"Course\": \"['Ch 1 开源大模型本地部署硬件指南.pdf']\", \"Mod...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新增 embedding_info_info 列，合并`类别、子类别、标题、摘要`字符串\n",
    "concatenation_order = [\"Course\", \"ModuleName\", \"Abstract\"]\n",
    "\n",
    "embedding_info_list = df_course['embedding_info'] = df_course.apply(lambda row: '{ ' + ', '.join(\n",
    "    f'\"{col}\": \"{row[col]}\"' for col in concatenation_order) + ' }', axis=1)\n",
    "\n",
    "df_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c7478-51d3-4b07-b0d2-de0c076a27ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
